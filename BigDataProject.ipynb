{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tweepy in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages\n",
      "Requirement already satisfied: PySocks>=1.5.7 in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages (from tweepy)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages (from tweepy)\n",
      "Requirement already satisfied: requests>=2.11.1 in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages (from tweepy)\n",
      "Requirement already satisfied: six>=1.10.0 in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages (from tweepy)\n",
      "Requirement already satisfied: oauthlib>=0.6.2 in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages (from requests-oauthlib>=0.7.0->tweepy)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages (from requests>=2.11.1->tweepy)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages (from requests>=2.11.1->tweepy)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages (from requests>=2.11.1->tweepy)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages (from requests>=2.11.1->tweepy)\n",
      "\u001b[33mYou are using pip version 9.0.3, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: textblob in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages\n",
      "Requirement already satisfied: nltk>=3.1 in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages (from textblob)\n",
      "Requirement already satisfied: six in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages (from nltk>=3.1->textblob)\n",
      "\u001b[33mYou are using pip version 9.0.3, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tweepy\n",
    "!pip install textblob\n",
    "!python -m textblob.download_corpora\n",
    "!pip install nltk\n",
    "!pip install numpy\n",
    "!pip install sklearn\n",
    "!pip install scipy\n",
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import re\n",
    "import csv\n",
    "import tweepy\n",
    "import unicodedata\n",
    "import json\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.misc import imread\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class Definiton for the keywords that will be used to fetch the tweets\n",
    "class keywords:\n",
    "    def __init__(self):\n",
    "        self.keywords = ['AlaskaAir','Allegiant','AmericanAir','Delta','FlyFrontier','HawaiianAir','@united','JetBlue','SouthwestAir','SpiritAirlines','VirginAmerica','SunCountryAir']\n",
    "        \n",
    "    def getKeyWords(self):\n",
    "        return self.keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class Definition to pull the twitter tweets\n",
    "class pullData:      \n",
    "    def __init__(self,key,secret,maxTweets,tweetsPerQry,keywords):\n",
    "        self.key = key   \n",
    "        self.secret = secret\n",
    "        self.maxTweets = maxTweets\n",
    "        self.tweetsPerQry = tweetsPerQry          \n",
    "        self.keywords = keywords\n",
    "        self.api = ''\n",
    "        self.auth = ''\n",
    "        \n",
    "    def printParams(self):\n",
    "        print('Parameters set to...')\n",
    "        print('key...',self.key)\n",
    "        print('secret...',self.secret)\n",
    "        print('maxTweets...',self.maxTweets)\n",
    "        print('tweetsPerQry...',self.tweetsPerQry)\n",
    "        print('keywords...',self.keywords)\n",
    "        \n",
    "    def connect(self):\n",
    "        self.auth = tweepy.AppAuthHandler(self.key,self.secret )  \n",
    "        self.api = tweepy.API(self.auth, wait_on_rate_limit=True,wait_on_rate_limit_notify=True)\n",
    "        return True\n",
    "        \n",
    "        if (not self.api):\n",
    "            print (\"Can't Authenticate\")\n",
    "            return False\n",
    "        \n",
    "    def downloadData(self):\n",
    "        for word in self.keywords:\n",
    "            print( \"Downloading Tweets for the keyword: \", word )\n",
    "            fName = 'tweets_' + word + '.txt'\n",
    "            sinceId = None\n",
    "            max_id = -1\n",
    "            tweetCount = 0\n",
    "            tweet_dict = []\n",
    "    \n",
    "            print(\"Downloading max {0} tweets\".format(maxTweets))\n",
    "\n",
    "            while tweetCount < self.maxTweets:\n",
    "                try:\n",
    "                    if (max_id <= 0):\n",
    "                        if (not sinceId):   \n",
    "                            new_tweets = self.api.search(q=word, count=self.tweetsPerQry)\n",
    "                        else:\n",
    "                            new_tweets = self.api.search(q=word, count=self.tweetsPerQry,since_id=sinceId)\n",
    "                    else:\n",
    "                        if (not sinceId):\n",
    "                            new_tweets = self.api.search(q=word, count=self.tweetsPerQry,\n",
    "                                    max_id=str(max_id - 1))\n",
    "                        else:\n",
    "                            new_tweets = self.api.search(q=word, count=self.tweetsPerQry,\n",
    "                                    max_id=str(max_id - 1),since_id=sinceId)\n",
    "                    if not new_tweets:\n",
    "                        print(\"No more tweets found\")\n",
    "                        break            \n",
    "                        \n",
    "                    for tweet in new_tweets:\n",
    "                        tweet_dict.append(tweet._json)\n",
    " \n",
    "                    tweetCount += len(new_tweets)\n",
    "                    print(\"Downloaded {0} tweets\".format(tweetCount))\n",
    "                    max_id = new_tweets[-1].id\n",
    "                \n",
    "                except tweepy.TweepError as e:\n",
    "                    # Just exit if any error\n",
    "                    print(\"some error : \" + str(e))\n",
    "                    break\n",
    "            \n",
    "            with open(fName, 'w', encoding='utf8', errors='replace') as f:   \n",
    "                json.dump(tweet_dict, f, ensure_ascii=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "consumer_key = 'bppIV0LkQUIARDug4b8Sij8pm'\n",
    "consumer_secret = '5ofpZtxL1CsDmfbF93Qh0EcPOHUVP3ZSF6CGhbkk2ki2fFWNFL'\n",
    "maxTweets = 100000 # Some arbitrary large number\n",
    "tweetsPerQry = 100  # this is the max the API permits\n",
    "\n",
    "keys = keywords()\n",
    "words = keys.getKeyWords()  # this is what we're searching for\n",
    "\n",
    "pulldata = pullData(consumer_key,consumer_secret,maxTweets,tweetsPerQry,words)\n",
    "\n",
    "if ( pulldata.connect() == False ):\n",
    "    print ( \"Connecting to the twitter API Failed\")\n",
    "    sys.exit(1)\n",
    "pulldata.printParams()\n",
    "    \n",
    "pulldata.downloadData()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class definition for JsonParser\n",
    "\n",
    "class JsonParser:\n",
    "    def loadData(self,fname):\n",
    "        with open(fname, encoding='utf8', errors='replace') as json_data:\n",
    "            d = json.load(json_data)\n",
    "        return d\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class definition to Clean the tweets and get the sentiment\n",
    "class TweetCleaner: \n",
    "    def __init__(self,stopwords_fname):\n",
    "        self.stopwords_fname = stopwords_fname\n",
    "        self.negation_cues = self.get_negation_cues('negation_cues.txt')\n",
    "        self.sentiment_fnames = [\"EffectWordNet.tff\", \"subjclueslen1-HLTEMNLP05.tff\"]\n",
    "        self.emoji_pattern = re.compile(r'[\\U00010000-\\U0010ffff]', flags=re.UNICODE)\n",
    "        self.url_pattern = re.compile(r'https?:\\/\\/.*\\b')\n",
    "        self.handle_pattern = re.compile(r'@\\w+')\n",
    "        self.hashtag_pattern = re.compile(r'#\\w+')\n",
    "        \n",
    "        #Populate the Stop Words\n",
    "        self.stopwords = set()\n",
    "        self.populateStopWords(self.stopwords_fname)\n",
    "        \n",
    "        #populate the wordtoEffectMap\n",
    "        self.wordToEffectMap = {}\n",
    "        self.buildWordToEffectMap(self.sentiment_fnames)\n",
    "        \n",
    "        #Initialize nltk classes\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.tokenizer = TweetTokenizer()\n",
    "    \n",
    "    #---------Create the Stop words Set---------#\n",
    "    def populateStopWords(self,fname):\n",
    "        stop_file = open(fname)\n",
    "        \n",
    "        for line in stop_file:\n",
    "            self.stopwords.add(line.strip())\n",
    "    \n",
    "    def get_negation_cues(self, fname):\n",
    "        cues_fname = open(fname, 'r')\n",
    "        neg = []\n",
    "        for line in cues_fname:\n",
    "            if not line:\n",
    "                continue\n",
    "            line = line.strip()\n",
    "            neg.append(line)\n",
    "        return neg\n",
    "    \n",
    "    #---------Build the word to +/- Effect Map----------#\n",
    "    def buildWordToEffectMap(self, sentiment_files):\n",
    "        #read sentiment files into dictionary of words and positive or negative sentiment\n",
    "        for file in sentiment_files:\n",
    "            with open(file, \"r\", encoding='utf8', errors='replace') as f:\n",
    "                if(file == \"EffectWordNet.tff\"):\n",
    "                    for line in f:\n",
    "                        #02279615\t+Effect\tprofiteer\t make an unreasonable profit, as on the sale of difficult to obtain goods \n",
    "                        words = line.split('\\t')\n",
    "                        effect = words[1]\n",
    "                        effect_val = 0\n",
    "                        if '+' in effect:\n",
    "                            effect_val = 1\n",
    "                        elif '-' in effect:\n",
    "                            effect_val = -1\n",
    "                        else:\n",
    "                            effect_val = 0\n",
    "                        list_of_words = []\n",
    "                        if (',' in words[2]):\n",
    "                            list_of_words = words[2].split(',')\n",
    "                        else:\n",
    "                            list_of_words.append(words[2])\n",
    "                        for word in list_of_words:\n",
    "                            self.wordToEffectMap[word] = effect_val\n",
    "                elif(file == \"subjclueslen1-HLTEMNLP05.tff\"):\n",
    "                    for line in f:\n",
    "                        #type=weaksubj len=1 word1=abandoned pos1=adj stemmed1=n priorpolarity=negative\n",
    "                        line = line.strip()\n",
    "                        if not line:\n",
    "                            continue\n",
    "                        words = line.split(' ')\n",
    "                        effect_val = words[5]\n",
    "                        #print(effect_val)\n",
    "                        word_val = words[2]\n",
    "                        word = word_val.split(\"=\")[1]\n",
    "                        effect = effect_val.split(\"=\")[1]\n",
    "                        eff = 0\n",
    "                        if (effect == \"positive\"):\n",
    "                            eff = 1\n",
    "                        elif (effect == \"negative\"):\n",
    "                            eff = -1\n",
    "                        elif (effect == \"neutral\"):\n",
    "                            eff = 0\n",
    "                        self.wordToEffectMap[word] = eff\n",
    "        \n",
    "    #---------Clean the tweets---------#\n",
    "    def cleanTweet(self,tweet):\n",
    "        tweet = tweet.lower()\n",
    "        \n",
    "        #1. remove emojis\n",
    "        tweet = re.sub(self.emoji_pattern, r'', tweet)\n",
    "        \n",
    "        #remove URLS\n",
    "        tweet = re.sub(self.url_pattern, r'', tweet)\n",
    "\n",
    "        #convert @mentions to AT_USER\n",
    "        tweet = re.sub(self.handle_pattern, r'AT_USER', tweet)\n",
    "        \n",
    "        #convert #tags to HASH_TAG\n",
    "        tweet = re.sub(self.hashtag_pattern, r'HASH_TAG', tweet)\n",
    "        \n",
    "        tweet_list = self.tokenizer.tokenize(tweet)\n",
    "        \n",
    "        #2. remove the stop words\n",
    "        words_filtered = []\n",
    "        for word in tweet_list:\n",
    "            if (word not in self.stopwords ):\n",
    "                words_filtered.append( word )\n",
    "              \n",
    "        #3. Stem the words\n",
    "        #words_stemmed = [self.stemmer.stem(word) for word in words_filtered]\n",
    "        \n",
    "        _tweet = \"\"\n",
    "        for word in words_filtered:\n",
    "            _tweet += \" \" + word\n",
    "        \n",
    "        #4. return the fitered tweet\n",
    "        return _tweet\n",
    "             \n",
    "    #--------- negative: 0 , neutral : 1 , positive : 2 ---------#\n",
    "    def getSentiment(self,tweet):\n",
    "        score = 0\n",
    "        tweet = tweet.lower()\n",
    "        tweet_list = self.tokenizer.tokenize(tweet)\n",
    "        for word in tweet_list:\n",
    "            if word in self.wordToEffectMap:\n",
    "                score += self.wordToEffectMap[word]\n",
    "        if (score < 0):\n",
    "            return 0\n",
    "        elif (score > 0):\n",
    "            return 2\n",
    "        else:\n",
    "            return 1\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleans all the tweets , builds the tweet set and adds the sentiment to the tweets\n",
    "fw = open('flyfrontier_tweets_with_sentiment.txt', 'w', encoding='utf8', errors='replace')\n",
    "class BuildFeatureSet: \n",
    "    \n",
    "    def __init__(self,n,stopwords_fname,max_features_count=5000): # n : ngram for the tweets\n",
    "        self.ngrams = n\n",
    "\n",
    "        #Lexicon related variables\n",
    "        self.lexicon = {}\n",
    "        self.inverse_lexicon = { }\n",
    "        self.ngram_count = 0\n",
    "        self.max_features_count = max_features_count\n",
    "        self.tweetCleaner = TweetCleaner(stopwords_fname)\n",
    "        self.tweet_count = 0\n",
    "        self.tweet_map = {}\n",
    "        \n",
    "        #Training Dataset\n",
    "        self.data = {}\n",
    "\n",
    "    def addToLexicon(self,words):\n",
    "        for word in words:\n",
    "            if ( word not in self.lexicon and ( self.ngram_count <  self.max_features_count ) ):\n",
    "                self.lexicon[word] = self.ngram_count #Assign a unique number for the word seen\n",
    "                self.inverse_lexicon[self.ngram_count] = word\n",
    "                self.ngram_count = self.ngram_count + 1\n",
    "                #print( 'Lexicon: ',self.ngram_count,word)\n",
    "    \n",
    "    def isTweetReply(self,tweet):\n",
    "        if ( tweet[0].lower() == 'r' and  tweet[1].lower() == 't' ):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def get_tweet_map(self):\n",
    "        return self.tweet_map\n",
    "        \n",
    "    def isTweetFromAirline(self, tweet, airline_handle):\n",
    "        if( tweet['user']['screen_name'] == airline_handle):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    #Add the tweet to the lexicon set\n",
    "    def addTweet(self,tweet):\n",
    "        \n",
    "        self.tweet_count += 1\n",
    "        self.tweet_map[self.tweet_count] = tweet\n",
    "        \n",
    "        #1. get the sentiment for the tweet\n",
    "        sentiment = self.tweetCleaner.getSentiment(tweet)\n",
    "        \n",
    "        #2. Clean the Tweet\n",
    "        tweet = self.tweetCleaner.cleanTweet(tweet)\n",
    "        \n",
    "        #3. get the ngrams for the tweet\n",
    "        _ngrams = nltk.ngrams(tweet.split(), self.ngrams)\n",
    "            \n",
    "        #4. Add the ngrams to the lexicon dictionary\n",
    "        words = list(_ngrams)\n",
    "        self.addToLexicon( words )\n",
    "        \n",
    "        #5. Add this tweet row to the training set\n",
    "        self.addToTrainingData(words,sentiment)\n",
    "    \n",
    "    #Build the Feature set for all the tweets\n",
    "    def addToTrainingData(self,ngrams,sentiment):\n",
    "        row = np.zeros(self.max_features_count + 1 ) # last feature is the label \n",
    "        \n",
    "        for word in ngrams:\n",
    "            if ( word in self.lexicon ):\n",
    "                row[ self.lexicon[word] ] = row[ self.lexicon[word] ] + 1 #Increase the count of the word \n",
    "\n",
    "        row [ self.max_features_count ] = sentiment\n",
    "        \n",
    "        self.data[ len(self.data)  ]  = row \n",
    "    \n",
    "    \n",
    "    def getFeatures(self):\n",
    "        features = np.zeros(( len(self.data), self.max_features_count ))\n",
    "        \n",
    "        for index in self.data:\n",
    "            features[index][:] =  self.data[index][:-1]\n",
    "            \n",
    "        return features\n",
    "    \n",
    "    def getLabels(self):\n",
    "        labels = np.zeros( len(self.data) )\n",
    "        \n",
    "        for index in self.data:\n",
    "            labels[index] = ( int(self.data[index][self.max_features_count]) )\n",
    "            \n",
    "        return labels\n",
    "    \n",
    "    def getHeaders(self):\n",
    "        headers = []\n",
    "        for i in range(self.max_features_count):\n",
    "            headers.append( self.inverse_lexicon[i] )\n",
    "        return headers\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parse the Tweets\n",
    "jsonParser = JsonParser()\n",
    "features = BuildFeatureSet(2,'stopwords.txt') \n",
    "\n",
    "files = ['tweets_Allegiant.txt']#,'tweets_AlaskaAir.txt','tweets_Delta.txt','tweets_AmericanAir.txt','tweets_FlyFrontier.txt','tweets_HawaiianAir.txt'] \n",
    "\n",
    "\n",
    "for file in files:\n",
    "    print ('Building Feature set for the file:',file)\n",
    "    tweets =  jsonParser.loadData(file)\n",
    "    airline_handle = file.split(\"_\")[1]\n",
    "    for tweet in tweets:\n",
    "        #1. Skip if the tweet the is a reply to an existing tweet\n",
    "        if ( features.isTweetReply(tweet['text']) == False and features.isTweetFromAirline(tweet, airline_handle) == False ):\n",
    "            features.addTweet ( tweet['text']  ) \n",
    "\n",
    "# print ( features.getFeatures().shape )\n",
    "\n",
    "# print ( features.getLabels().shape )\n",
    "\n",
    "# print ( features.getHeaders() )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic regression classifier\n",
    "\n",
    "X = features.getFeatures()\n",
    "y = features.getLabels()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8)\n",
    "\n",
    "param_grid = {'C':[0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1.0, 3.0, 10, 30, 100, 300]}\n",
    "logreg = LogisticRegression(C=3.0)\n",
    "\n",
    "#clf = GridSearchCV(logreg, param_grid)\n",
    "#clf.fit(X_train, y_train)\n",
    "\n",
    "#print(clf.best_params_)\n",
    "#print(clf.best_score_)\n",
    "\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(acc)\n",
    "\n",
    "m,n = X_train.shape\n",
    "i,j = X_test.shape\n",
    "\n",
    "twtmap = features.get_tweet_map()\n",
    "\n",
    "#for val in range(i):\n",
    "#    print ( twtmap[m+val], y_pred[val], y_test[val] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Visualize:\n",
    "    def __init__(self,debug,removeWordsList=[]):\n",
    "        self.debug = debug\n",
    "        \n",
    "        #map[word]->count for the word cloud\n",
    "        self.poscount = {} \n",
    "        self.negcount = {}  \n",
    "        self.neutralcount = {} \n",
    "        self.jsonParser = JsonParser()\n",
    "        self.tweetCleaner = TweetCleaner('stopwords.txt')\n",
    "        self.removeWordsList = removeWordsList #List of keywords to remove for the wordcount. ex: AT_USER\n",
    "    \n",
    "    def log(self,msg):\n",
    "        if ( self.debug ):\n",
    "            print (msg)\n",
    "    \n",
    "    #Function to return the top n words \n",
    "    \n",
    "    def getTopN(self,n,mode):# mode: 0 | 1 | 2 \n",
    "        if( mode == 0):\n",
    "            items = sorted(self.negcount.items(), key=lambda x: x[1],reverse = True)\n",
    "        elif( mode == 1 ):\n",
    "            items = sorted(self.neutralcount.items(), key=lambda x: x[1],reverse = True)\n",
    "        elif( mode == 2 ):\n",
    "            items = sorted(self.poscount.items(), key=lambda x: x[1],reverse = True)\n",
    "        items = items[0:n]\n",
    "        result = [i for i in items if i[0] not in self.removeWordsList]\n",
    "        return result\n",
    "    \n",
    "    def writeToCSV(self,fname,n,mode):\n",
    "        if(mode == 0):\n",
    "            fname=fname+'_neg' +'.csv'\n",
    "        elif ( mode == 1):\n",
    "            fname=fname+'_neut'+'.csv'\n",
    "        else:\n",
    "            fname=fname+'_pos'+'.csv'\n",
    "            \n",
    "        with open(fname,'w') as out:\n",
    "            csv_out=csv.writer(out)\n",
    "            csv_out.writerow(['word','count'])\n",
    "            rows = self.getTopN(n,mode)\n",
    "            for row in rows:\n",
    "                csv_out.writerow(row)\n",
    "        \n",
    "    def setWordCount(self,fname):\n",
    "        self.log('File name is,'+fname)\n",
    "        tweets =  jsonParser.loadData(fname)\n",
    "        \n",
    "        self.log('Reading all the tweets')\n",
    "        \n",
    "        for tweet in tweets:\n",
    "            #1.Skip for retweets\n",
    "            if ( tweet['text'][0].lower() == 'r' and  tweet['text'][1].lower() == 't' ):\n",
    "                continue\n",
    "                \n",
    "            text = self.tweetCleaner.cleanTweet( tweet['text'] )\n",
    "            sentiment = self.tweetCleaner.getSentiment( tweet['text'] )\n",
    "            if ( sentiment == 2):\n",
    "                for  word in text.split(' '):\n",
    "                    if ( word not in self.poscount ):\n",
    "                        self.poscount[word] = 1\n",
    "                    else:\n",
    "                        self.poscount[word] = self.poscount[word] + 1\n",
    "            elif ( sentiment == 1 ):\n",
    "                for  word in text.split(' '):\n",
    "                    if ( word not in self.neutralcount ):\n",
    "                        self.neutralcount[word] = 1\n",
    "                    else:\n",
    "                        self.neutralcount[word] = self.neutralcount[word] + 1\n",
    "            else:\n",
    "                for  word in text.split(' '):\n",
    "                    if ( word not in self.negcount ):\n",
    "                        self.negcount[word] = 1\n",
    "                    else:\n",
    "                        self.negcount[word] = self.negcount[word] + 1\n",
    "                    \n",
    "        self.log('Word count is set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removeWordsList = ['AT_USER','HASH_TAG','...',',','’','rt','','…']\n",
    "visualize = Visualize(False,removeWordsList)\n",
    "visualize.setWordCount('tweets_AmericanAir.txt')\n",
    "\n",
    "print ( 'negative', visualize.getTopN(30,0) )\n",
    "print ( 'neutral',visualize.getTopN(30,1) )\n",
    "print ( 'positive',visualize.getTopN(30,2) )\n",
    "\n",
    "\n",
    "#Write Data to a sav file for visualisation in tableau\n",
    "\n",
    "#visualize.writeToCSV('wc_AmericanAir',200,0)\n",
    "#visualize.writeToCSV('wc_AmericanAir',200,1)\n",
    "#visualize.writeToCSV('wc_AmericanAir',200,2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
