{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tweepy in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages\n",
      "Requirement already satisfied: PySocks>=1.5.7 in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages (from tweepy)\n",
      "Requirement already satisfied: six>=1.10.0 in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages (from tweepy)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages (from tweepy)\n",
      "Requirement already satisfied: requests>=2.11.1 in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages (from tweepy)\n",
      "Requirement already satisfied: oauthlib>=0.6.2 in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages (from requests-oauthlib>=0.7.0->tweepy)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages (from requests>=2.11.1->tweepy)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages (from requests>=2.11.1->tweepy)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages (from requests>=2.11.1->tweepy)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages (from requests>=2.11.1->tweepy)\n",
      "\u001b[33mYou are using pip version 9.0.3, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: textblob in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages\n",
      "Requirement already satisfied: nltk>=3.1 in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages (from textblob)\n",
      "Requirement already satisfied: six in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages (from nltk>=3.1->textblob)\n",
      "\u001b[33mYou are using pip version 9.0.3, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     /Users/srinivassuri/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/srinivassuri/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/srinivassuri/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/srinivassuri/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package conll2000 to\n",
      "[nltk_data]     /Users/srinivassuri/nltk_data...\n",
      "[nltk_data]   Package conll2000 is already up-to-date!\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /Users/srinivassuri/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "Finished.\n",
      "Requirement already satisfied: nltk in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages\n",
      "Requirement already satisfied: six in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages (from nltk)\n",
      "\u001b[33mYou are using pip version 9.0.3, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: numpy in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages\n",
      "\u001b[33mYou are using pip version 9.0.3, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: sklearn in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages\n",
      "Requirement already satisfied: scikit-learn in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages (from sklearn)\n",
      "\u001b[33mYou are using pip version 9.0.3, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: scipy in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages\n",
      "Requirement already satisfied: numpy>=1.8.2 in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages (from scipy)\n",
      "\u001b[33mYou are using pip version 9.0.3, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: wordcloud in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages\n",
      "Requirement already satisfied: pillow in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages (from wordcloud)\n",
      "Requirement already satisfied: numpy>=1.6.1 in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages (from wordcloud)\n",
      "Requirement already satisfied: matplotlib in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages (from wordcloud)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages (from matplotlib->wordcloud)\n",
      "Requirement already satisfied: six>=1.10 in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages (from matplotlib->wordcloud)\n",
      "Requirement already satisfied: pytz in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages (from matplotlib->wordcloud)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages (from matplotlib->wordcloud)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages (from matplotlib->wordcloud)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages (from matplotlib->wordcloud)\n",
      "Requirement already satisfied: setuptools in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib->wordcloud)\n",
      "\u001b[33mYou are using pip version 9.0.3, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: keras in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages\n",
      "Requirement already satisfied: h5py in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages (from keras)\n",
      "Requirement already satisfied: scipy>=0.14 in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages (from keras)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages (from keras)\n",
      "Requirement already satisfied: pyyaml in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages (from keras)\n",
      "Requirement already satisfied: six>=1.9.0 in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages (from keras)\n",
      "\u001b[33mYou are using pip version 9.0.3, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting tensorflow\n",
      "  Downloading https://files.pythonhosted.org/packages/64/6a/c0681f99098edc2ac32e7485ca5046cd47461b6ac379f65932c817913b34/tensorflow-1.7.0-cp36-cp36m-macosx_10_11_x86_64.whl (45.3MB)\n",
      "\u001b[K    100% |████████████████████████████████| 45.3MB 37kB/s eta 0:00:011   27% |████████▊                       | 12.3MB 2.8MB/s eta 0:00:12    69% |██████████████████████▎         | 31.5MB 2.1MB/s eta 0:00:07    92% |█████████████████████████████▊  | 42.1MB 2.0MB/s eta 0:00:02\n",
      "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages (from tensorflow)\n",
      "Collecting astor>=0.6.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/b2/91/cc9805f1ff7b49f620136b3a7ca26f6a1be2ed424606804b0fbcf499f712/astor-0.6.2-py2.py3-none-any.whl\n",
      "Requirement already satisfied: numpy>=1.13.3 in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages (from tensorflow)\n",
      "Collecting absl-py>=0.1.6 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/90/6b/ba04a9fe6aefa56adafa6b9e0557b959e423c49950527139cb8651b0480b/absl-py-0.2.0.tar.gz (82kB)\n",
      "\u001b[K    100% |████████████████████████████████| 92kB 2.9MB/s ta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.4.0 in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages (from tensorflow)\n",
      "Collecting gast>=0.2.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/5c/78/ff794fcae2ce8aa6323e789d1f8b3b7765f601e7702726f430e814822b96/gast-0.2.0.tar.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting grpcio>=1.8.6 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/b9/43/4914451df9fe1acd8e75ecc395bce7fbcf09f87689b2bc4ae3b78b0ddc0f/grpcio-1.11.0-cp36-cp36m-macosx_10_11_x86_64.whl (1.6MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.6MB 613kB/s ta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard<1.8.0,>=1.7.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/0b/ec/65d4e8410038ca2a78c09034094403d231228d0ddcae7d470b223456e55d/tensorboard-1.7.0-py3-none-any.whl (3.1MB)\n",
      "\u001b[K    100% |████████████████████████████████| 3.1MB 529kB/s ta 0:00:01\n",
      "\u001b[?25hCollecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz\n",
      "Requirement already satisfied: wheel>=0.26 in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages (from tensorflow)\n",
      "Requirement already satisfied: setuptools in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages (from protobuf>=3.4.0->tensorflow)\n",
      "Collecting bleach==1.5.0 (from tensorboard<1.8.0,>=1.7.0->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/33/70/86c5fec937ea4964184d4d6c4f0b9551564f821e1c3575907639036d9b90/bleach-1.5.0-py2.py3-none-any.whl\n",
      "Collecting markdown>=2.6.8 (from tensorboard<1.8.0,>=1.7.0->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/6d/7d/488b90f470b96531a3f5788cf12a93332f543dbab13c423a5e7ce96a0493/Markdown-2.6.11-py2.py3-none-any.whl (78kB)\n",
      "\u001b[K    100% |████████████████████████████████| 81kB 3.5MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting werkzeug>=0.11.10 (from tensorboard<1.8.0,>=1.7.0->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/20/c4/12e3e56473e52375aa29c4764e70d1b8f3efa6682bef8d0aae04fe335243/Werkzeug-0.14.1-py2.py3-none-any.whl (322kB)\n",
      "\u001b[K    100% |████████████████████████████████| 327kB 2.1MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting html5lib==0.9999999 (from tensorboard<1.8.0,>=1.7.0->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/ae/ae/bcb60402c60932b32dfaf19bb53870b29eda2cd17551ba5639219fb5ebf9/html5lib-0.9999999.tar.gz (889kB)\n",
      "\u001b[K    100% |████████████████████████████████| 890kB 1.1MB/s ta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: absl-py, gast, termcolor, html5lib\n",
      "  Running setup.py bdist_wheel for absl-py ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/srinivassuri/Library/Caches/pip/wheels/23/35/1d/48c0a173ca38690dd8dfccfa47ffc750db48f8989ed898455c\n",
      "  Running setup.py bdist_wheel for gast ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/srinivassuri/Library/Caches/pip/wheels/9a/1f/0e/3cde98113222b853e98fc0a8e9924480a3e25f1b4008cedb4f\n",
      "  Running setup.py bdist_wheel for termcolor ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/srinivassuri/Library/Caches/pip/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6\n",
      "  Running setup.py bdist_wheel for html5lib ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/srinivassuri/Library/Caches/pip/wheels/50/ae/f9/d2b189788efcf61d1ee0e36045476735c838898eef1cad6e29\n",
      "Successfully built absl-py gast termcolor html5lib\n",
      "Installing collected packages: astor, absl-py, gast, grpcio, html5lib, bleach, markdown, werkzeug, tensorboard, termcolor, tensorflow\n",
      "  Found existing installation: html5lib 1.0.1\n",
      "    Uninstalling html5lib-1.0.1:\n",
      "      Successfully uninstalled html5lib-1.0.1\n",
      "  Found existing installation: bleach 2.1.3\n",
      "    Uninstalling bleach-2.1.3:\n",
      "      Successfully uninstalled bleach-2.1.3\n",
      "Successfully installed absl-py-0.2.0 astor-0.6.2 bleach-1.5.0 gast-0.2.0 grpcio-1.11.0 html5lib-0.9999999 markdown-2.6.11 tensorboard-1.7.0 tensorflow-1.7.0 termcolor-1.1.0 werkzeug-0.14.1\n",
      "\u001b[33mYou are using pip version 9.0.3, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tweepy\n",
    "!pip install textblob\n",
    "!python -m textblob.download_corpora\n",
    "!pip install nltk\n",
    "!pip install numpy\n",
    "!pip install sklearn\n",
    "!pip install scipy\n",
    "!pip install wordcloud\n",
    "!pip install keras\n",
    "!pip install tensorflow\n",
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import re\n",
    "import csv\n",
    "import tweepy\n",
    "import unicodedata\n",
    "import json\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.misc import imread\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM, Convolution1D, Flatten, Dropout, LSTM, MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "from textblob import TextBlob\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "#download this file from - https://nlp.stanford.edu/projects/glove/\n",
    "glove_file = \"glove.6B.300d.txt\"\n",
    "tmp_file = \"glove_w2vec.txt\"\n",
    "#remove these comments when you want word embeddings\n",
    "#glove2word2vec(glove_file, tmp_file)\n",
    "#vec = KeyedVectors.load_word2vec_format(tmp_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Class Definiton for the keywords that will be used to fetch the tweets\n",
    "class keywords:\n",
    "    def __init__(self):\n",
    "        self.keywords = ['AlaskaAir','Allegiant','AmericanAir','Delta','FlyFrontier','HawaiianAir','@united','JetBlue','SouthwestAir','SpiritAirlines','VirginAmerica','SunCountryAir']\n",
    "        \n",
    "    def getKeyWords(self):\n",
    "        return self.keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Class Definition to pull the twitter tweets\n",
    "class pullData:      \n",
    "    def __init__(self,key,secret,maxTweets,tweetsPerQry,keywords):\n",
    "        self.key = key   \n",
    "        self.secret = secret\n",
    "        self.maxTweets = maxTweets\n",
    "        self.tweetsPerQry = tweetsPerQry          \n",
    "        self.keywords = keywords\n",
    "        self.api = ''\n",
    "        self.auth = ''\n",
    "        \n",
    "    def printParams(self):\n",
    "        print('Parameters set to...')\n",
    "        print('key...',self.key)\n",
    "        print('secret...',self.secret)\n",
    "        print('maxTweets...',self.maxTweets)\n",
    "        print('tweetsPerQry...',self.tweetsPerQry)\n",
    "        print('keywords...',self.keywords)\n",
    "        \n",
    "    def connect(self):\n",
    "        self.auth = tweepy.AppAuthHandler(self.key,self.secret )  \n",
    "        self.api = tweepy.API(self.auth, wait_on_rate_limit=True,wait_on_rate_limit_notify=True)\n",
    "        return True\n",
    "        \n",
    "        if (not self.api):\n",
    "            print (\"Can't Authenticate\")\n",
    "            return False\n",
    "        \n",
    "    def downloadData(self):\n",
    "        for word in self.keywords:\n",
    "            print( \"Downloading Tweets for the keyword: \", word )\n",
    "            fName = 'tweets_' + word + '.txt'\n",
    "            sinceId = None\n",
    "            max_id = -1\n",
    "            tweetCount = 0\n",
    "            tweet_dict = []\n",
    "    \n",
    "            print(\"Downloading max {0} tweets\".format(maxTweets))\n",
    "\n",
    "            while tweetCount < self.maxTweets:\n",
    "                try:\n",
    "                    if (max_id <= 0):\n",
    "                        if (not sinceId):   \n",
    "                            new_tweets = self.api.search(q=word, count=self.tweetsPerQry)\n",
    "                        else:\n",
    "                            new_tweets = self.api.search(q=word, count=self.tweetsPerQry,since_id=sinceId)\n",
    "                    else:\n",
    "                        if (not sinceId):\n",
    "                            new_tweets = self.api.search(q=word, count=self.tweetsPerQry,\n",
    "                                    max_id=str(max_id - 1))\n",
    "                        else:\n",
    "                            new_tweets = self.api.search(q=word, count=self.tweetsPerQry,\n",
    "                                    max_id=str(max_id - 1),since_id=sinceId)\n",
    "                    if not new_tweets:\n",
    "                        print(\"No more tweets found\")\n",
    "                        break            \n",
    "                        \n",
    "                    for tweet in new_tweets:\n",
    "                        tweet_dict.append(tweet._json)\n",
    " \n",
    "                    tweetCount += len(new_tweets)\n",
    "                    print(\"Downloaded {0} tweets\".format(tweetCount))\n",
    "                    max_id = new_tweets[-1].id\n",
    "                \n",
    "                except tweepy.TweepError as e:\n",
    "                    # Just exit if any error\n",
    "                    print(\"some error : \" + str(e))\n",
    "                    break\n",
    "            \n",
    "            with open(fName, 'w', encoding='utf8', errors='replace') as f:   \n",
    "                json.dump(tweet_dict, f, ensure_ascii=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "consumer_key = 'bppIV0LkQUIARDug4b8Sij8pm'\n",
    "consumer_secret = '5ofpZtxL1CsDmfbF93Qh0EcPOHUVP3ZSF6CGhbkk2ki2fFWNFL'\n",
    "maxTweets = 100000 # Some arbitrary large number\n",
    "tweetsPerQry = 100  # this is the max the API permits\n",
    "\n",
    "keys = keywords()\n",
    "words = keys.getKeyWords()  # this is what we're searching for\n",
    "\n",
    "pulldata = pullData(consumer_key,consumer_secret,maxTweets,tweetsPerQry,words)\n",
    "\n",
    "if ( pulldata.connect() == False ):\n",
    "    print ( \"Connecting to the twitter API Failed\")\n",
    "    sys.exit(1)\n",
    "pulldata.printParams()\n",
    "    \n",
    "pulldata.downloadData()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Class definition for JsonParser\n",
    "\n",
    "class JsonParser:\n",
    "    def loadData(self,fname):\n",
    "        with open(fname, encoding='utf8', errors='replace') as json_data:\n",
    "            d = json.load(json_data)\n",
    "        return d\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Class definition to Clean the tweets and get the sentiment\n",
    "class TweetCleaner: \n",
    "    def __init__(self,stopwords_fname):\n",
    "        self.stopwords_fname = stopwords_fname\n",
    "        self.negation_cues = self.get_negation_cues('negation_cues.txt')\n",
    "        self.sentiment_fnames = [\"EffectWordNet.tff\", \"subjclueslen1-HLTEMNLP05.tff\"]\n",
    "        self.emoji_pattern = re.compile(r'[\\U00010000-\\U0010ffff]', flags=re.UNICODE)\n",
    "        self.url_pattern = re.compile(r'https?:\\/\\/.*\\b')\n",
    "        self.handle_pattern = re.compile(r'@\\w+')\n",
    "        self.hashtag_pattern = re.compile(r'#\\w+')\n",
    "        \n",
    "        #Populate the Stop Words\n",
    "        self.stopwords = set()\n",
    "        self.populateStopWords(self.stopwords_fname)\n",
    "        \n",
    "        #populate the wordtoEffectMap\n",
    "        self.wordToEffectMap = {}\n",
    "        self.buildWordToEffectMap(self.sentiment_fnames)\n",
    "        \n",
    "        #Initialize nltk classes\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.tokenizer = TweetTokenizer()\n",
    "    \n",
    "    #---------Create the Stop words Set---------#\n",
    "    def populateStopWords(self,fname):\n",
    "        stop_file = open(fname)\n",
    "        \n",
    "        for line in stop_file:\n",
    "            self.stopwords.add(line.strip())\n",
    "    \n",
    "    def get_negation_cues(self, fname):\n",
    "        cues_fname = open(fname, 'r')\n",
    "        neg = []\n",
    "        for line in cues_fname:\n",
    "            if not line:\n",
    "                continue\n",
    "            line = line.strip()\n",
    "            neg.append(line)\n",
    "        return neg\n",
    "    \n",
    "    #---------Build the word to +/- Effect Map----------#\n",
    "    def buildWordToEffectMap(self, sentiment_files):\n",
    "        #read sentiment files into dictionary of words and positive or negative sentiment\n",
    "        for file in sentiment_files:\n",
    "            with open(file, \"r\", encoding='utf8', errors='replace') as f:\n",
    "                if(file == \"EffectWordNet.tff\"):\n",
    "                    for line in f:\n",
    "                        #02279615\t+Effect\tprofiteer\t make an unreasonable profit, as on the sale of difficult to obtain goods \n",
    "                        words = line.split('\\t')\n",
    "                        effect = words[1]\n",
    "                        effect_val = 0\n",
    "                        if '+' in effect:\n",
    "                            effect_val = 1\n",
    "                        elif '-' in effect:\n",
    "                            effect_val = -1\n",
    "                        else:\n",
    "                            effect_val = 0\n",
    "                        list_of_words = []\n",
    "                        if (',' in words[2]):\n",
    "                            list_of_words = words[2].split(',')\n",
    "                        else:\n",
    "                            list_of_words.append(words[2])\n",
    "                        for word in list_of_words:\n",
    "                            self.wordToEffectMap[word] = effect_val\n",
    "                elif(file == \"subjclueslen1-HLTEMNLP05.tff\"):\n",
    "                    for line in f:\n",
    "                        #type=weaksubj len=1 word1=abandoned pos1=adj stemmed1=n priorpolarity=negative\n",
    "                        line = line.strip()\n",
    "                        if not line:\n",
    "                            continue\n",
    "                        words = line.split(' ')\n",
    "                        effect_val = words[5]\n",
    "                        #print(effect_val)\n",
    "                        word_val = words[2]\n",
    "                        word = word_val.split(\"=\")[1]\n",
    "                        effect = effect_val.split(\"=\")[1]\n",
    "                        eff = 0\n",
    "                        if (effect == \"positive\"):\n",
    "                            eff = 1\n",
    "                        elif (effect == \"negative\"):\n",
    "                            eff = -1\n",
    "                        elif (effect == \"neutral\"):\n",
    "                            eff = 0\n",
    "                        self.wordToEffectMap[word] = eff\n",
    "        \n",
    "    #---------Clean the tweets---------#\n",
    "    def cleanTweet(self,tweet):\n",
    "        tweet = tweet.lower()\n",
    "        \n",
    "        #1. remove emojis\n",
    "        tweet = re.sub(self.emoji_pattern, r'', tweet)\n",
    "        \n",
    "        #remove URLS\n",
    "        tweet = re.sub(self.url_pattern, r'', tweet)\n",
    "\n",
    "        #convert @mentions to AT_USER\n",
    "        tweet = re.sub(self.handle_pattern, r'AT_USER', tweet)\n",
    "        \n",
    "        #convert #tags to HASH_TAG\n",
    "        tweet = re.sub(self.hashtag_pattern, r'HASH_TAG', tweet)\n",
    "        \n",
    "        tweet_list = self.tokenizer.tokenize(tweet)\n",
    "        \n",
    "        #2. remove the stop words\n",
    "        words_filtered = []\n",
    "        for word in tweet_list:\n",
    "            if (word not in self.stopwords ):\n",
    "                words_filtered.append( word )\n",
    "              \n",
    "        #3. Stem the words\n",
    "        #words_stemmed = [self.stemmer.stem(word) for word in words_filtered]\n",
    "        \n",
    "        _tweet = \"\"\n",
    "        for word in words_filtered:\n",
    "            _tweet += \" \" + word\n",
    "        \n",
    "        #4. return the fitered tweet\n",
    "        return _tweet\n",
    "             \n",
    "    #--------- negative: 0 , neutral : 1 , positive : 2 ---------#\n",
    "    def getSentiment(self,tweet):\n",
    "        score = 0\n",
    "        tweet = tweet.lower()\n",
    "        tweet_list = self.tokenizer.tokenize(tweet)\n",
    "        for word in tweet_list:\n",
    "            if word in self.wordToEffectMap:\n",
    "                score += self.wordToEffectMap[word]\n",
    "        if (score < 0):\n",
    "            return 0\n",
    "        elif (score > 0):\n",
    "            return 2\n",
    "        else:\n",
    "            return 1\n",
    "        \n",
    "    def getSentimentWithTextBlob(self, tweet):\n",
    "        tweet = tweet.lower()\n",
    "        tweetBlob = TextBlob(tweet)\n",
    "        if (tweetBlob.sentiment.polarity > 0 ):\n",
    "            return 2\n",
    "        elif ( tweetBlob.sentiment.polarity < 0):\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Cleans all the tweets , builds the tweet set and adds the sentiment to the tweets\n",
    "class BuildFeatureSet: \n",
    "    \n",
    "    def __init__(self,n,stopwords_fname,max_features_count=5000): # n : ngram for the tweets\n",
    "        self.ngrams = n\n",
    "\n",
    "        #Lexicon related variables\n",
    "        self.lexicon = {}\n",
    "        self.inverse_lexicon = { }\n",
    "        self.ngram_count = 0\n",
    "        self.max_features_count = max_features_count\n",
    "        self.tweetCleaner = TweetCleaner(stopwords_fname)\n",
    "        self.tweet_count = 0\n",
    "        self.tweet_map = {}\n",
    "        \n",
    "        #Training Dataset\n",
    "        self.data = {}\n",
    "\n",
    "    def addToLexicon(self,words):\n",
    "        for word in words:\n",
    "            if ( word not in self.lexicon and ( self.ngram_count <  self.max_features_count ) ):\n",
    "                self.lexicon[word] = self.ngram_count #Assign a unique number for the word seen\n",
    "                self.inverse_lexicon[self.ngram_count] = word\n",
    "                self.ngram_count = self.ngram_count + 1\n",
    "                #print( 'Lexicon: ',self.ngram_count,word)\n",
    "    \n",
    "    def isTweetReply(self,tweet):\n",
    "        if ( tweet[0].lower() == 'r' and  tweet[1].lower() == 't' ):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def get_tweet_map(self):\n",
    "        return self.tweet_map\n",
    "        \n",
    "    def isTweetFromAirline(self, tweet, airline_handle):\n",
    "        if( tweet['user']['screen_name'] == airline_handle):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    #Add the tweet to the lexicon set\n",
    "    def addTweet(self,tweet, wordvec=False):\n",
    "        \n",
    "        self.tweet_count += 1\n",
    "        self.tweet_map[self.tweet_count] = tweet\n",
    "        \n",
    "        #1. get the sentiment for the tweet\n",
    "        sentiment = self.tweetCleaner.getSentiment(tweet)\n",
    "        \n",
    "        #2. Clean the Tweet\n",
    "        tweet = self.tweetCleaner.cleanTweet(tweet)\n",
    "        \n",
    "        #3. get the ngrams for the tweet\n",
    "        _ngrams = nltk.ngrams(tweet.split(), self.ngrams)\n",
    "            \n",
    "        #4. Add the ngrams to the lexicon dictionary\n",
    "        words = list(_ngrams)\n",
    "        if not wordvec:\n",
    "            self.addToLexicon( words )\n",
    "        \n",
    "        #5. Add this tweet row to the training set\n",
    "        if not wordvec:\n",
    "            self.addToTrainingData(words,sentiment)\n",
    "        else:\n",
    "            self.addVecToTrainingData(words, sentiment)\n",
    "    \n",
    "    #Build the Feature set for all the tweets\n",
    "    def addToTrainingData(self,ngrams,sentiment):\n",
    "        row = np.zeros(self.max_features_count + 1 ) # last feature is the label \n",
    "        \n",
    "        for word in ngrams:\n",
    "            if ( word in self.lexicon ):\n",
    "                row[ self.lexicon[word] ] = row[ self.lexicon[word] ] + 1 #Increase the count of the word \n",
    "\n",
    "        row [ self.max_features_count ] = sentiment\n",
    "        \n",
    "        self.data[ len(self.data)  ]  = row \n",
    "        \n",
    "    def addVecToTrainingData(self, ngrams, sentiment):\n",
    "        sentence = []\n",
    "        #print(\"Inside addVec\")\n",
    "        for word in ngrams:\n",
    "            if( word[0] in vec):\n",
    "                #print (\"found word vec\")\n",
    "                row  = vec[word[0]]\n",
    "            else: \n",
    "                row = [0.0 for i in range(300)]\n",
    "            sentence.append(row)\n",
    "        if(np.sum(sentence) == 0):\n",
    "            average_row = [0.0 for i in range(300)]\n",
    "        else:\n",
    "            average_row = np.mean(sentence, axis=0)\n",
    "        average_row = np.append(average_row, [sentiment])\n",
    "        self.data[ len(self.data) ] = average_row\n",
    "    \n",
    "    \n",
    "    def getFeatures(self):\n",
    "        features = np.zeros(( len(self.data), self.max_features_count ))\n",
    "        \n",
    "        for index in self.data:\n",
    "            features[index][:] =  self.data[index][:-1]\n",
    "            \n",
    "        return features\n",
    "    \n",
    "    def getLabels(self):\n",
    "        labels = np.zeros( len(self.data) )\n",
    "        \n",
    "        for index in self.data:\n",
    "            labels[index] = ( int(self.data[index][self.max_features_count]) )\n",
    "            \n",
    "        return labels\n",
    "    \n",
    "    def getHeaders(self):\n",
    "        headers = []\n",
    "        for i in range(self.max_features_count):\n",
    "            headers.append( self.inverse_lexicon[i] )\n",
    "        return headers\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Train Classifiers\n",
    "class Classifiers:\n",
    "    def __init__(self,ngrams,stopwords_fname,f_airlines,split,noOfFeatures):\n",
    "        self.ngrams = ngrams\n",
    "        self.f_airlines = f_airlines\n",
    "        self.split  = split\n",
    "        self.noOfFeatures=noOfFeatures\n",
    "\n",
    "        #1.Define Custom Classes required for the class\n",
    "        self.features = BuildFeatureSet(self.ngrams, stopwords_fname, max_features_count = self.noOfFeatures)\n",
    "        self.jsonParser = JsonParser()\n",
    "\n",
    "        #2. Build the feature set\n",
    "        print ('Building the feature set')\n",
    "        \n",
    "        for file in f_airlines:\n",
    "            print ('Parsing tweets from the file:',file)\n",
    "            tweets =  self.jsonParser.loadData(file)\n",
    "            airline_handle = file.split(\"_\")[1]\n",
    "            for tweet in tweets:\n",
    "                #1. Skip if the tweet the is a reply to an existing tweet\n",
    "                if ( self.features.isTweetReply(tweet['text']) == False and self.features.isTweetFromAirline(tweet, airline_handle) == False ):\n",
    "                    self.features.addTweet ( tweet['text']  )  #use wordvec=True if word embeddings needed\n",
    "        \n",
    "        #3. Prepare the train and test datasets\n",
    "        print ( 'Preparing the training and the test set...')\n",
    "        self.X = self.features.getFeatures()\n",
    "        self.y = self.features.getLabels()\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.X, self.y, train_size= self.split,test_size=1- self.split)\n",
    "        print ( 'Done preapring the train and test set...Ready to train classifiers')\n",
    "        \n",
    "        #4. Define Variables for classifiers performance\n",
    "        #4a. Logistic Regression\n",
    "        self.logreg_train_acc = 0\n",
    "        self.logreg_test_acc = 0\n",
    "        \n",
    "        #4b. Gaussian Naive Bayes\n",
    "        self.nb_train_acc = 0\n",
    "        self.nb_test_acc = 0\n",
    "        \n",
    "        #4c. LibLinearSVC\n",
    "        self.svc_train_acc = 0\n",
    "        self.svc_test_acc = 0\n",
    "        \n",
    "        #4d. Decision Tree\n",
    "        self.dt_train_acc = 0\n",
    "        self.dt_test_acc = 0\n",
    "\n",
    "        #4e.  Ada Boost\n",
    "        self.adaboost_train_acc = 0\n",
    "        self.adaboost_test_acc = 0\n",
    "        \n",
    "        #4f.\n",
    "        self.rf_train_acc = 0\n",
    "        self.rf_test_acc = 0\n",
    "        \n",
    "        #4g.\n",
    "        self.ltsm_train_acc = 0\n",
    "        self.ltsm_test_acc = 0\n",
    "    \n",
    "    def applyPCA(self,n):\n",
    "        print ( 'Applying PCA on X' )\n",
    "        pca = PCA(n_components=n)\n",
    "        self.X = pca.fit_transform(self.X)\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.X, self.y, train_size= self.split,test_size=1- self.split)\n",
    "        \n",
    "    def printResults(self):\n",
    "        print ( 'Results for Logistic Regression')\n",
    "        print( '(train,test):(',self.logreg_train_acc,self.logreg_test_acc,')' )\n",
    "        print ( 'Results for Gaussian Naive Bayes')\n",
    "        print( '(train,test):(',self.nb_train_acc,self.nb_test_acc,')' )\n",
    "        print ( 'Results for LibLinear SVM')\n",
    "        print( '(train,test):(',self.svc_train_acc,self.svc_test_acc,')' )\n",
    "        print ( 'Results for Decision Tree')\n",
    "        print( '(train,test):(',self.dt_train_acc,self.dt_test_acc,')' )\n",
    "        print ( 'Results for Adaboost')\n",
    "        print( '(train,test):(',self.adaboost_train_acc,self.adaboost_test_acc,')' )\n",
    "        print ( 'Results for Random Forests')\n",
    "        print( '(train,test):(',self.rf_train_acc,self.rf_test_acc,')' )\n",
    "        print ( 'Results for LTSM')\n",
    "        print( '(train,test):(',self.ltsm_train_acc,self.ltsm_test_acc,')' )\n",
    "    \n",
    "    def getResults(self):\n",
    "        return [self.logreg_train_acc,self.logreg_test_acc, \n",
    "                self.nb_train_acc,self.nb_test_acc, \n",
    "                self.svc_train_acc,self.svc_test_acc,  \n",
    "                self.dt_train_acc,self.dt_test_acc, \n",
    "                self.adaboost_train_acc,self.adaboost_test_acc, \n",
    "                self.rf_train_acc,self.rf_test_acc,\n",
    "                self.ltsm_train_acc,self.ltsm_test_acc]\n",
    "    \n",
    "    def runClassifiers(self):\n",
    "            #1. Logistic regression classifier\n",
    "            print('Training Logistic Regression Classifiers')\n",
    "\n",
    "            #C = [0.001,  0.01,  0.1,  1.0,  10,  100]\n",
    "            C = [0.1,10]\n",
    "\n",
    "            logreg = GridSearchCV(LogisticRegression(max_iter=100),cv = 10, param_grid= {\"C\" : C},verbose=1,n_jobs=60)\n",
    "            logreg.fit(self.X_train, self.y_train)\n",
    "\n",
    "            #print(clf.best_params_)\n",
    "            self.logreg_train_acc = accuracy_score( self.y_train,logreg.predict(self.X_train) )\n",
    "            self.logreg_test_acc = accuracy_score( self.y_test,logreg.predict(self.X_test) )\n",
    "            \n",
    "            #2. Naive Bayes\n",
    "            print('Training Gaussian Naive Bayes Classifier')\n",
    "            nb = GaussianNB()\n",
    "            nb.fit(self.X_train, self.y_train)\n",
    "\n",
    "            self.nb_train_acc = accuracy_score( self.y_train,nb.predict(self.X_train) )\n",
    "            self.nb_test_acc = accuracy_score( self.y_test,nb.predict(self.X_test) )\n",
    "            \n",
    "            #3.LibLinear SVM\n",
    "            print( 'Training SVM Lib Linear Classifier')\n",
    "            svc = GridSearchCV(LinearSVC(max_iter=1000),cv = 10, param_grid= {\"C\" : C},verbose=1,n_jobs=60)\n",
    "            svc.fit(self.X_train, self.y_train)\n",
    "\n",
    "            self.svc_train_acc =  accuracy_score(self.y_train,svc.predict(self.X_train)) \n",
    "            self.svc_test_acc = accuracy_score(self.y_test,svc.predict(self.X_test)) \n",
    "            \n",
    "            #4.Decision Tree\n",
    "            print( 'Training Decision Tree Classifier' )\n",
    "            max_depth = [1,3,5,7]\n",
    "            dt = GridSearchCV(DecisionTreeClassifier(),cv = 10, param_grid= {\"max_depth\" : max_depth},verbose=1,n_jobs=60)\n",
    "            dt.fit(self.X_train, self.y_train)\n",
    "            \n",
    "            self.dt_train_acc =  accuracy_score(self.y_train,dt.predict(self.X_train)) \n",
    "            self.dt_test_acc = accuracy_score(self.y_test,dt.predict(self.X_test)) \n",
    "            \n",
    "            #5. AdaBoost\n",
    "            print( 'Training AdaBoost' )\n",
    "            grd = GradientBoostingClassifier(n_estimators=500,max_depth=5,random_state=42)\n",
    "            grd.fit(self.X_train, self.y_train)\n",
    "            self.adaboost_train_acc =  accuracy_score(self.y_train,grd.predict(self.X_train)) \n",
    "            self.adaboost_test_acc = accuracy_score(self.y_test,grd.predict(self.X_test)) \n",
    "            \n",
    "            #6. Random Forests\n",
    "            print( 'Training Random Forests' )\n",
    "            rf = RandomForestClassifier(n_estimators=500,max_depth=5,random_state=42)\n",
    "            rf.fit(self.X_train, self.y_train)\n",
    "            self.rf_train_acc =  accuracy_score(self.y_train,rf.predict(self.X_train)) \n",
    "            self.rf_test_acc = accuracy_score(self.y_test,rf.predict(self.X_test)) \n",
    "            \n",
    "            #7.Deep net LTSM\n",
    "            print( 'Training LTSM Deep Net' )\n",
    "            embedding_vecor_length = 300\n",
    "            model = Sequential()\n",
    "            top_words = self.X_train.shape[0]\n",
    "            max_review_length = self.X_train.shape[1]\n",
    "            model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "\n",
    "            # Convolutional model (3x conv, flatten, 2x dense)\n",
    "            model.add(Dropout(0.2))\n",
    "            model.add(Convolution1D(64, 5, activation='relu'))\n",
    "            model.add(MaxPooling1D(pool_size=4))\n",
    "            model.add(LSTM(128))\n",
    "            model.add(Dense(3,activation='softmax'))\n",
    "\n",
    "            # Log to tensorboard\n",
    "            tensorBoardCallback = TensorBoard(log_dir='./logs', write_graph=True)\n",
    "            model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "            model.fit(self.X_train, self.y_train, epochs=3, batch_size=64)\n",
    "\n",
    "            # Evaluation on the test set\n",
    "            self.ltsm_train_acc =  model.evaluate(self.X_train, self.y_train, verbose=0)[1]\n",
    "            self.ltsm_test_acc =  model.evaluate(self.X_test, self.y_test, verbose=0)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate results for  0 -gram Classifiers\n",
      "Building the feature set\n",
      "Parsing tweets from the file: tweets_HawaiianAir.txt\n",
      "Preparing the training and the test set...\n",
      "Done preapring the train and test set...Ready to train classifiers\n",
      "Training Logistic Regression Classifiers\n",
      "Fitting 10 folds for each of 2 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=60)]: Done   6 out of  20 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=60)]: Done  20 out of  20 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Gaussian Naive Bayes Classifier\n",
      "Training SVM Lib Linear Classifier\n",
      "Fitting 10 folds for each of 2 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=60)]: Done   6 out of  20 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=60)]: Done  20 out of  20 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Decision Tree Classifier\n",
      "Fitting 10 folds for each of 4 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=60)]: Done   3 out of  40 | elapsed:    0.0s remaining:    0.3s\n",
      "[Parallel(n_jobs=60)]: Done  40 out of  40 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AdaBoost\n",
      "Training Random Forests\n",
      "Training LTSM Deep Net\n",
      "Epoch 1/3\n",
      "345/345 [==============================] - 1s 3ms/step - loss: 1.0575 - acc: 0.4551\n",
      "Epoch 2/3\n",
      "345/345 [==============================] - 0s 445us/step - loss: 1.0352 - acc: 0.4899\n",
      "Epoch 3/3\n",
      "345/345 [==============================] - 0s 447us/step - loss: 1.0265 - acc: 0.4899\n",
      "Generate results for  1 -gram Classifiers\n",
      "Building the feature set\n",
      "Parsing tweets from the file: tweets_HawaiianAir.txt\n",
      "Preparing the training and the test set...\n",
      "Done preapring the train and test set...Ready to train classifiers\n",
      "Training Logistic Regression Classifiers\n",
      "Fitting 10 folds for each of 2 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=60)]: Done   6 out of  20 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=60)]: Done  20 out of  20 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Gaussian Naive Bayes Classifier\n",
      "Training SVM Lib Linear Classifier\n",
      "Fitting 10 folds for each of 2 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=60)]: Done   6 out of  20 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=60)]: Done  20 out of  20 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Decision Tree Classifier\n",
      "Fitting 10 folds for each of 4 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=60)]: Done   3 out of  40 | elapsed:    0.0s remaining:    0.2s\n",
      "[Parallel(n_jobs=60)]: Done  40 out of  40 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AdaBoost\n",
      "Training Random Forests\n",
      "Training LTSM Deep Net\n",
      "Epoch 1/3\n",
      "345/345 [==============================] - 1s 3ms/step - loss: 1.0644 - acc: 0.4580\n",
      "Epoch 2/3\n",
      "345/345 [==============================] - 0s 439us/step - loss: 1.0309 - acc: 0.4870\n",
      "Epoch 3/3\n",
      "345/345 [==============================] - 0s 458us/step - loss: 1.0393 - acc: 0.4870\n",
      "Generate results for  2 -gram Classifiers\n",
      "Building the feature set\n",
      "Parsing tweets from the file: tweets_HawaiianAir.txt\n",
      "Preparing the training and the test set...\n",
      "Done preapring the train and test set...Ready to train classifiers\n",
      "Training Logistic Regression Classifiers\n",
      "Fitting 10 folds for each of 2 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=60)]: Done   6 out of  20 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=60)]: Done  20 out of  20 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Gaussian Naive Bayes Classifier\n",
      "Training SVM Lib Linear Classifier\n",
      "Fitting 10 folds for each of 2 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=60)]: Done   6 out of  20 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=60)]: Done  20 out of  20 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Decision Tree Classifier\n",
      "Fitting 10 folds for each of 4 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=60)]: Done   3 out of  40 | elapsed:    0.0s remaining:    0.3s\n",
      "[Parallel(n_jobs=60)]: Done  40 out of  40 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AdaBoost\n",
      "Training Random Forests\n",
      "Training LTSM Deep Net\n",
      "Epoch 1/3\n",
      "345/345 [==============================] - 1s 3ms/step - loss: 1.0576 - acc: 0.4551\n",
      "Epoch 2/3\n",
      "345/345 [==============================] - 0s 620us/step - loss: 1.0279 - acc: 0.4928\n",
      "Epoch 3/3\n",
      "345/345 [==============================] - 0s 520us/step - loss: 1.0102 - acc: 0.4928\n",
      "Generate results for  3 -gram Classifiers\n",
      "Building the feature set\n",
      "Parsing tweets from the file: tweets_HawaiianAir.txt\n",
      "Preparing the training and the test set...\n",
      "Done preapring the train and test set...Ready to train classifiers\n",
      "Training Logistic Regression Classifiers\n",
      "Fitting 10 folds for each of 2 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/srinivassuri/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:58: DeprecationWarning: generator 'ngrams' raised StopIteration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Gaussian Naive Bayes Classifier\n",
      "Training SVM Lib Linear Classifier\n",
      "Fitting 10 folds for each of 2 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=60)]: Done   6 out of  20 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=60)]: Done  20 out of  20 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=60)]: Done   6 out of  20 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=60)]: Done  20 out of  20 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Decision Tree Classifier\n",
      "Fitting 10 folds for each of 4 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=60)]: Done   3 out of  40 | elapsed:    0.0s remaining:    0.3s\n",
      "[Parallel(n_jobs=60)]: Done  40 out of  40 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AdaBoost\n",
      "Training Random Forests\n",
      "Training LTSM Deep Net\n",
      "Epoch 1/3\n",
      "345/345 [==============================] - 1s 3ms/step - loss: 1.0571 - acc: 0.4319\n",
      "Epoch 2/3\n",
      "345/345 [==============================] - 0s 504us/step - loss: 1.0073 - acc: 0.5072\n",
      "Epoch 3/3\n",
      "345/345 [==============================] - 0s 492us/step - loss: 1.0078 - acc: 0.5072\n",
      "Generate results for  4 -gram Classifiers\n",
      "Building the feature set\n",
      "Parsing tweets from the file: tweets_HawaiianAir.txt\n",
      "Preparing the training and the test set...\n",
      "Done preapring the train and test set...Ready to train classifiers\n",
      "Training Logistic Regression Classifiers\n",
      "Fitting 10 folds for each of 2 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/srinivassuri/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:58: DeprecationWarning: generator 'ngrams' raised StopIteration\n",
      "[Parallel(n_jobs=60)]: Done   6 out of  20 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=60)]: Done  20 out of  20 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Gaussian Naive Bayes Classifier\n",
      "Training SVM Lib Linear Classifier\n",
      "Fitting 10 folds for each of 2 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=60)]: Done   6 out of  20 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=60)]: Done  20 out of  20 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Decision Tree Classifier\n",
      "Fitting 10 folds for each of 4 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=60)]: Done   3 out of  40 | elapsed:    0.0s remaining:    0.4s\n",
      "[Parallel(n_jobs=60)]: Done  40 out of  40 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AdaBoost\n",
      "Training Random Forests\n",
      "Training LTSM Deep Net\n",
      "Epoch 1/3\n",
      "345/345 [==============================] - 1s 3ms/step - loss: 1.0420 - acc: 0.4812\n",
      "Epoch 2/3\n",
      "345/345 [==============================] - 0s 467us/step - loss: 1.0457 - acc: 0.4812\n",
      "Epoch 3/3\n",
      "345/345 [==============================] - 0s 626us/step - loss: 1.0303 - acc: 0.4812\n",
      "Results (features): ( 25 ):\n",
      "   logreg_train  logreg_test  nb_train   nb_test  svm_train  svm_test  \\\n",
      "0      0.576812     0.586207  0.226087  0.198276   0.582609  0.577586   \n",
      "1      0.597101     0.534483  0.257971  0.155172   0.588406  0.543103   \n",
      "2      0.550725     0.465517  0.179710  0.232759   0.553623  0.482759   \n",
      "3      0.524638     0.396552  0.524638  0.396552   0.524638  0.396552   \n",
      "4      0.484058     0.491379  0.179710  0.198276   0.486957  0.482759   \n",
      "\n",
      "   dt_train   dt_test  adaboost_train  adaboost_test  rf_train   rf_test  \\\n",
      "0  0.657971  0.577586        0.704348       0.577586  0.597101  0.568966   \n",
      "1  0.588406  0.534483        0.713043       0.517241  0.614493  0.500000   \n",
      "2  0.582609  0.517241        0.582609       0.500000  0.582609  0.517241   \n",
      "3  0.524638  0.396552        0.524638       0.396552  0.524638  0.396552   \n",
      "4  0.486957  0.482759        0.486957       0.482759  0.486957  0.482759   \n",
      "\n",
      "                                  ltsm_train  \\\n",
      "0   [1.0252210886582085, 0.4898550710816314]   \n",
      "1  [1.0281713133272918, 0.48695652070252793]   \n",
      "2   [1.0170595497324846, 0.4927536218062691]   \n",
      "3   [1.0100797623827837, 0.5072463760341424]   \n",
      "4  [1.0324398933977321, 0.48115941925325256]   \n",
      "\n",
      "                                   ltsm_test  \n",
      "0  [1.0163637440780113, 0.46551723932397776]  \n",
      "1  [1.0005298688493927, 0.47413793206214905]  \n",
      "2    [1.05672169964889, 0.45689655172413796]  \n",
      "3    [1.0739886514071761, 0.413793107558941]  \n",
      "4  [1.0469824733405277, 0.49137931240016014]  \n",
      "Generate results for  0 -gram Classifiers\n",
      "Building the feature set\n",
      "Parsing tweets from the file: tweets_HawaiianAir.txt\n",
      "Preparing the training and the test set...\n",
      "Done preapring the train and test set...Ready to train classifiers\n",
      "Training Logistic Regression Classifiers\n",
      "Fitting 10 folds for each of 2 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=60)]: Done   6 out of  20 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=60)]: Done  20 out of  20 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Gaussian Naive Bayes Classifier\n",
      "Training SVM Lib Linear Classifier\n",
      "Fitting 10 folds for each of 2 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=60)]: Done   6 out of  20 | elapsed:    0.1s remaining:    0.2s\n",
      "[Parallel(n_jobs=60)]: Done  20 out of  20 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Decision Tree Classifier\n",
      "Fitting 10 folds for each of 4 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=60)]: Done   3 out of  40 | elapsed:    0.0s remaining:    0.3s\n",
      "[Parallel(n_jobs=60)]: Done  40 out of  40 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AdaBoost\n",
      "Training Random Forests\n",
      "Training LTSM Deep Net\n",
      "Epoch 1/3\n",
      "345/345 [==============================] - 1s 4ms/step - loss: 1.0404 - acc: 0.4406\n",
      "Epoch 2/3\n",
      "345/345 [==============================] - 0s 949us/step - loss: 1.0006 - acc: 0.5101\n",
      "Epoch 3/3\n",
      "345/345 [==============================] - 0s 938us/step - loss: 0.9990 - acc: 0.5101\n",
      "Generate results for  1 -gram Classifiers\n",
      "Building the feature set\n",
      "Parsing tweets from the file: tweets_HawaiianAir.txt\n",
      "Preparing the training and the test set...\n",
      "Done preapring the train and test set...Ready to train classifiers\n",
      "Training Logistic Regression Classifiers\n",
      "Fitting 10 folds for each of 2 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=60)]: Done   6 out of  20 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=60)]: Done  20 out of  20 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Gaussian Naive Bayes Classifier\n",
      "Training SVM Lib Linear Classifier\n",
      "Fitting 10 folds for each of 2 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=60)]: Done   6 out of  20 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=60)]: Done  20 out of  20 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Decision Tree Classifier\n",
      "Fitting 10 folds for each of 4 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=60)]: Done   3 out of  40 | elapsed:    0.0s remaining:    0.3s\n",
      "[Parallel(n_jobs=60)]: Done  40 out of  40 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AdaBoost\n",
      "Training Random Forests\n",
      "Training LTSM Deep Net\n",
      "Epoch 1/3\n",
      "345/345 [==============================] - 2s 5ms/step - loss: 1.0667 - acc: 0.4406\n",
      "Epoch 2/3\n",
      "345/345 [==============================] - 0s 997us/step - loss: 1.0242 - acc: 0.4841\n",
      "Epoch 3/3\n",
      "345/345 [==============================] - 0s 884us/step - loss: 1.0233 - acc: 0.4841\n",
      "Generate results for  2 -gram Classifiers\n",
      "Building the feature set\n",
      "Parsing tweets from the file: tweets_HawaiianAir.txt\n",
      "Preparing the training and the test set...\n",
      "Done preapring the train and test set...Ready to train classifiers\n",
      "Training Logistic Regression Classifiers\n",
      "Fitting 10 folds for each of 2 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=60)]: Done   6 out of  20 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=60)]: Done  20 out of  20 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Gaussian Naive Bayes Classifier\n",
      "Training SVM Lib Linear Classifier\n",
      "Fitting 10 folds for each of 2 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=60)]: Done   6 out of  20 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=60)]: Done  20 out of  20 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Decision Tree Classifier\n",
      "Fitting 10 folds for each of 4 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=60)]: Done   3 out of  40 | elapsed:    0.0s remaining:    0.3s\n",
      "[Parallel(n_jobs=60)]: Done  40 out of  40 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AdaBoost\n",
      "Training Random Forests\n",
      "Training LTSM Deep Net\n",
      "Epoch 1/3\n",
      "345/345 [==============================] - 2s 5ms/step - loss: 1.0436 - acc: 0.4261\n",
      "Epoch 2/3\n",
      "345/345 [==============================] - 0s 940us/step - loss: 1.0305 - acc: 0.4957\n",
      "Epoch 3/3\n",
      "345/345 [==============================] - 0s 879us/step - loss: 1.0154 - acc: 0.4957\n",
      "Generate results for  3 -gram Classifiers\n",
      "Building the feature set\n",
      "Parsing tweets from the file: tweets_HawaiianAir.txt\n",
      "Preparing the training and the test set...\n",
      "Done preapring the train and test set...Ready to train classifiers\n",
      "Training Logistic Regression Classifiers\n",
      "Fitting 10 folds for each of 2 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/srinivassuri/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:58: DeprecationWarning: generator 'ngrams' raised StopIteration\n",
      "[Parallel(n_jobs=60)]: Done   6 out of  20 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=60)]: Done  20 out of  20 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Gaussian Naive Bayes Classifier\n",
      "Training SVM Lib Linear Classifier\n",
      "Fitting 10 folds for each of 2 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=60)]: Done   6 out of  20 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=60)]: Done  20 out of  20 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Decision Tree Classifier\n",
      "Fitting 10 folds for each of 4 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=60)]: Done   3 out of  40 | elapsed:    0.0s remaining:    0.4s\n",
      "[Parallel(n_jobs=60)]: Done  40 out of  40 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AdaBoost\n",
      "Training Random Forests\n",
      "Training LTSM Deep Net\n",
      "Epoch 1/3\n",
      "345/345 [==============================] - 2s 5ms/step - loss: 1.0395 - acc: 0.4986\n",
      "Epoch 2/3\n",
      "345/345 [==============================] - 0s 926us/step - loss: 1.0132 - acc: 0.4986\n",
      "Epoch 3/3\n",
      "345/345 [==============================] - 0s 889us/step - loss: 1.0079 - acc: 0.4986\n",
      "Generate results for  4 -gram Classifiers\n",
      "Building the feature set\n",
      "Parsing tweets from the file: tweets_HawaiianAir.txt\n",
      "Preparing the training and the test set...\n",
      "Done preapring the train and test set...Ready to train classifiers\n",
      "Training Logistic Regression Classifiers\n",
      "Fitting 10 folds for each of 2 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/srinivassuri/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:58: DeprecationWarning: generator 'ngrams' raised StopIteration\n",
      "[Parallel(n_jobs=60)]: Done   6 out of  20 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=60)]: Done  20 out of  20 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Gaussian Naive Bayes Classifier\n",
      "Training SVM Lib Linear Classifier\n",
      "Fitting 10 folds for each of 2 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=60)]: Done   6 out of  20 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=60)]: Done  20 out of  20 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Decision Tree Classifier\n",
      "Fitting 10 folds for each of 4 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=60)]: Done   3 out of  40 | elapsed:    0.0s remaining:    0.3s\n",
      "[Parallel(n_jobs=60)]: Done  40 out of  40 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AdaBoost\n",
      "Training Random Forests\n",
      "Training LTSM Deep Net\n",
      "Epoch 1/3\n",
      "345/345 [==============================] - 2s 5ms/step - loss: 1.0673 - acc: 0.4000\n",
      "Epoch 2/3\n",
      "345/345 [==============================] - 0s 917us/step - loss: 1.0344 - acc: 0.4899\n",
      "Epoch 3/3\n",
      "345/345 [==============================] - 0s 1ms/step - loss: 1.0243 - acc: 0.4899\n",
      "Results (features): ( 50 ):\n",
      "   logreg_train  logreg_test  nb_train   nb_test  svm_train  svm_test  \\\n",
      "0      0.602899     0.568966  0.260870  0.370690   0.600000  0.525862   \n",
      "1      0.628986     0.543103  0.272464  0.224138   0.634783  0.525862   \n",
      "2      0.573913     0.525862  0.226087  0.250000   0.573913  0.525862   \n",
      "3      0.501449     0.439655  0.182609  0.250000   0.507246  0.431034   \n",
      "4      0.489855     0.465517  0.350725  0.336207   0.495652  0.465517   \n",
      "\n",
      "   dt_train   dt_test  adaboost_train  adaboost_test  rf_train   rf_test  \\\n",
      "0  0.597101  0.517241        0.779710       0.551724  0.611594  0.500000   \n",
      "1  0.585507  0.517241        0.785507       0.448276  0.620290  0.534483   \n",
      "2  0.579710  0.465517        0.605797       0.500000  0.582609  0.465517   \n",
      "3  0.515942  0.431034        0.515942       0.431034  0.515942  0.431034   \n",
      "4  0.492754  0.465517        0.498551       0.465517  0.498551  0.465517   \n",
      "\n",
      "                                  ltsm_train  \\\n",
      "0    [0.9997558331144029, 0.510144927708999]   \n",
      "1    [1.019540030023326, 0.4840579708417257]   \n",
      "2    [1.0167257578476616, 0.495652174949646]   \n",
      "3  [1.0065492636915567, 0.49855072506959885]   \n",
      "4    [1.0221391836802165, 0.489855072291001]   \n",
      "\n",
      "                                   ltsm_test  \n",
      "0  [1.0977263738369119, 0.40517241430693657]  \n",
      "1   [1.0355241298675537, 0.4827586217173215]  \n",
      "2    [1.049311115823943, 0.4482758641242981]  \n",
      "3   [1.076662174586592, 0.43965517652445824]  \n",
      "4  [1.0337749637406448, 0.46551724137931033]  \n",
      "Results After Performing PCA \n",
      "Generate results for ith gram Classifiers\n",
      "Building the feature set\n",
      "Parsing tweets from the file: tweets_HawaiianAir.txt\n",
      "Preparing the training and the test set...\n",
      "Done preapring the train and test set...Ready to train classifiers\n",
      "Applying PCA on X\n",
      "Training Logistic Regression Classifiers\n",
      "Fitting 10 folds for each of 2 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=60)]: Done   6 out of  20 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=60)]: Done  20 out of  20 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Gaussian Naive Bayes Classifier\n",
      "Training SVM Lib Linear Classifier\n",
      "Fitting 10 folds for each of 2 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=60)]: Done   6 out of  20 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=60)]: Done  20 out of  20 | elapsed:    0.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Decision Tree Classifier\n",
      "Fitting 10 folds for each of 4 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=60)]: Done   3 out of  40 | elapsed:    0.0s remaining:    0.3s\n",
      "[Parallel(n_jobs=60)]: Done  40 out of  40 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AdaBoost\n",
      "Training Random Forests\n",
      "Training LTSM Deep Net\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "indices[25,0] = -1 is not in [0, 345)\n\t [[Node: embedding_12/Gather = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, validate_indices=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](embedding_12/embeddings/read, embedding_12/Cast)]]\n\nCaused by op 'embedding_12/Gather', defined at:\n  File \"/Users/srinivassuri/miniconda3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Users/srinivassuri/miniconda3/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/srinivassuri/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/Users/srinivassuri/miniconda3/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/srinivassuri/miniconda3/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/Users/srinivassuri/miniconda3/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 127, in start\n    self.asyncio_loop.run_forever()\n  File \"/Users/srinivassuri/miniconda3/lib/python3.6/asyncio/base_events.py\", line 421, in run_forever\n    self._run_once()\n  File \"/Users/srinivassuri/miniconda3/lib/python3.6/asyncio/base_events.py\", line 1431, in _run_once\n    handle._run()\n  File \"/Users/srinivassuri/miniconda3/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/Users/srinivassuri/miniconda3/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 117, in _handle_events\n    handler_func(fileobj, events)\n  File \"/Users/srinivassuri/miniconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/srinivassuri/miniconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/Users/srinivassuri/miniconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/srinivassuri/miniconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/srinivassuri/miniconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/srinivassuri/miniconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/srinivassuri/miniconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/srinivassuri/miniconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/srinivassuri/miniconda3/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/srinivassuri/miniconda3/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/srinivassuri/miniconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/Users/srinivassuri/miniconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/srinivassuri/miniconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2903, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/srinivassuri/miniconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-29-bc7d3be241f5>\", line 29, in <module>\n    clf.runClassifiers()\n  File \"<ipython-input-28-6c3e451a230b>\", line 151, in runClassifiers\n    model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n  File \"/Users/srinivassuri/miniconda3/lib/python3.6/site-packages/keras/models.py\", line 497, in add\n    layer(x)\n  File \"/Users/srinivassuri/miniconda3/lib/python3.6/site-packages/keras/engine/topology.py\", line 619, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"/Users/srinivassuri/miniconda3/lib/python3.6/site-packages/keras/layers/embeddings.py\", line 138, in call\n    out = K.gather(self.embeddings, inputs)\n  File \"/Users/srinivassuri/miniconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 1215, in gather\n    return tf.gather(reference, indices)\n  File \"/Users/srinivassuri/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 2698, in gather\n    params, indices, validate_indices=validate_indices, name=name)\n  File \"/Users/srinivassuri/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 2672, in gather\n    validate_indices=validate_indices, name=name)\n  File \"/Users/srinivassuri/miniconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/Users/srinivassuri/miniconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3290, in create_op\n    op_def=op_def)\n  File \"/Users/srinivassuri/miniconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1654, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): indices[25,0] = -1 is not in [0, 345)\n\t [[Node: embedding_12/Gather = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, validate_indices=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](embedding_12/embeddings/read, embedding_12/Cast)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1312\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m             status, run_metadata)\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    517\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: indices[25,0] = -1 is not in [0, 345)\n\t [[Node: embedding_12/Gather = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, validate_indices=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](embedding_12/embeddings/read, embedding_12/Cast)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-bc7d3be241f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClassifiers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'stopwords.txt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.75\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeatureLen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplyPCA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Keeping the top 20 important features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunClassifiers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetResults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-6c3e451a230b>\u001b[0m in \u001b[0;36mrunClassifiers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0mtensorBoardCallback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensorBoard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./logs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrite_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sparse_categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0;31m# Evaluation on the test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1000\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1002\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1003\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1004\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1234\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1236\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1237\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2480\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2481\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2482\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2483\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1140\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1321\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1338\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1340\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: indices[25,0] = -1 is not in [0, 345)\n\t [[Node: embedding_12/Gather = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, validate_indices=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](embedding_12/embeddings/read, embedding_12/Cast)]]\n\nCaused by op 'embedding_12/Gather', defined at:\n  File \"/Users/srinivassuri/miniconda3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Users/srinivassuri/miniconda3/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/srinivassuri/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/Users/srinivassuri/miniconda3/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/srinivassuri/miniconda3/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/Users/srinivassuri/miniconda3/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 127, in start\n    self.asyncio_loop.run_forever()\n  File \"/Users/srinivassuri/miniconda3/lib/python3.6/asyncio/base_events.py\", line 421, in run_forever\n    self._run_once()\n  File \"/Users/srinivassuri/miniconda3/lib/python3.6/asyncio/base_events.py\", line 1431, in _run_once\n    handle._run()\n  File \"/Users/srinivassuri/miniconda3/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/Users/srinivassuri/miniconda3/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 117, in _handle_events\n    handler_func(fileobj, events)\n  File \"/Users/srinivassuri/miniconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/srinivassuri/miniconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/Users/srinivassuri/miniconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/srinivassuri/miniconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/srinivassuri/miniconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/srinivassuri/miniconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/srinivassuri/miniconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/srinivassuri/miniconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/srinivassuri/miniconda3/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/srinivassuri/miniconda3/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/srinivassuri/miniconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/Users/srinivassuri/miniconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/srinivassuri/miniconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2903, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/srinivassuri/miniconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-29-bc7d3be241f5>\", line 29, in <module>\n    clf.runClassifiers()\n  File \"<ipython-input-28-6c3e451a230b>\", line 151, in runClassifiers\n    model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n  File \"/Users/srinivassuri/miniconda3/lib/python3.6/site-packages/keras/models.py\", line 497, in add\n    layer(x)\n  File \"/Users/srinivassuri/miniconda3/lib/python3.6/site-packages/keras/engine/topology.py\", line 619, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"/Users/srinivassuri/miniconda3/lib/python3.6/site-packages/keras/layers/embeddings.py\", line 138, in call\n    out = K.gather(self.embeddings, inputs)\n  File \"/Users/srinivassuri/miniconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 1215, in gather\n    return tf.gather(reference, indices)\n  File \"/Users/srinivassuri/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 2698, in gather\n    params, indices, validate_indices=validate_indices, name=name)\n  File \"/Users/srinivassuri/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 2672, in gather\n    validate_indices=validate_indices, name=name)\n  File \"/Users/srinivassuri/miniconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/Users/srinivassuri/miniconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3290, in create_op\n    op_def=op_def)\n  File \"/Users/srinivassuri/miniconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1654, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): indices[25,0] = -1 is not in [0, 345)\n\t [[Node: embedding_12/Gather = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, validate_indices=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](embedding_12/embeddings/read, embedding_12/Cast)]]\n"
     ]
    }
   ],
   "source": [
    "#Generate the Classifier Results\n",
    "#files = ['tweets_Allegiant.txt','tweets_AlaskaAir.txt','tweets_Delta.txt','tweets_AmericanAir.txt','tweets_FlyFrontier.txt','tweets_HawaiianAir.txt']  \n",
    "files = ['tweets_HawaiianAir.txt']\n",
    "noOfFeatures = [25,50]\n",
    "\n",
    "headers=['logreg_train','logreg_test','nb_train','nb_test','svm_train','svm_test','dt_train','dt_test','adaboost_train','adaboost_test'\n",
    "        ,'rf_train','rf_test','ltsm_train','ltsm_test']\n",
    "results=[]\n",
    "\n",
    "for featureLen in noOfFeatures:\n",
    "    results = []\n",
    "    for i in range(5):\n",
    "        print( 'Generate results for ', i, '-gram Classifiers')\n",
    "        clf = Classifiers(i,'stopwords.txt',files,0.75,featureLen)\n",
    "        clf.runClassifiers()\n",
    "        results.append( clf.getResults() )\n",
    "    \n",
    "    print( 'Results (features): (',featureLen,'):' )\n",
    "    df = pd.DataFrame(results, columns=headers)\n",
    "    print( df)\n",
    "    \n",
    "print ( 'Results After Performing PCA ')\n",
    "for featureLen in noOfFeatures:\n",
    "    results = []\n",
    "    for i in range(5):\n",
    "        print( 'Generate results for ith gram Classifiers')\n",
    "        clf = Classifiers(i,'stopwords.txt',files,0.75,featureLen)\n",
    "        clf.applyPCA(20) #Keeping the top 20 important features\n",
    "        clf.runClassifiers()\n",
    "        results.append( clf.getResults() )\n",
    "    \n",
    "    print( 'Results (features): (',featureLen,'):' )\n",
    "    df = pd.DataFrame(results, columns=headers)\n",
    "    print( df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class Visualize:\n",
    "    def __init__(self,debug,removeWordsList=[]):\n",
    "        self.debug = debug\n",
    "        \n",
    "        self.jsonParser = JsonParser()\n",
    "        self.tweetCleaner = TweetCleaner('stopwords.txt')\n",
    "        self.removeWordsList = removeWordsList #List of keywords to remove for the wordcount. ex: AT_USER\n",
    "        \n",
    "        #map[word]->count for the word cloud\n",
    "        self.poscount = {} \n",
    "        self.negcount = {}  \n",
    "        self.neutralcount = {} \n",
    "\n",
    "        #Location of tweets\n",
    "        self.posloc = {}\n",
    "        self.negloc = {}\n",
    "        self.neutloc = {}\n",
    "        \n",
    "        #month_day of tweets\n",
    "        self.posdate = {}\n",
    "        self.negdate = {}\n",
    "        self.neutraldate = {}\n",
    "        \n",
    "    \n",
    "    def log(self,msg):\n",
    "        if ( self.debug ):\n",
    "            print (msg)\n",
    "    \n",
    "    def clearCount(self,mode):\n",
    "        if (mode == 0 ):\n",
    "            self.negcount = {}  \n",
    "        elif(mode == 1):\n",
    "            self.neutralcount = {} \n",
    "        else:\n",
    "            self.poscount = {} \n",
    "            \n",
    "    def clearLocations(self,mode):\n",
    "        if (mode == 0 ):\n",
    "            self.negloc = {}  \n",
    "        elif(mode == 1):\n",
    "            self.neutloc = {} \n",
    "        else:\n",
    "            self.posloc = {} \n",
    "    \n",
    "    #Function to return the top n words \n",
    "    def getTopNWords(self,n,mode):# mode: 0 | 1 | 2 \n",
    "        if( mode == 0):\n",
    "            items = sorted(self.negcount.items(), key=lambda x: x[1],reverse = True)\n",
    "        elif( mode == 1 ):\n",
    "            items = sorted(self.neutralcount.items(), key=lambda x: x[1],reverse = True)\n",
    "        elif( mode == 2 ):\n",
    "            items = sorted(self.poscount.items(), key=lambda x: x[1],reverse = True)\n",
    "        items = items[0:n]\n",
    "        result = [i for i in items if i[0] not in self.removeWordsList]\n",
    "        return result\n",
    "    \n",
    "    #Function to return the top n Locations\n",
    "    def getTopNLocations(self,n,mode):# mode: 0 | 1 | 2 \n",
    "        if( mode == 0):\n",
    "            items = sorted(self.negloc.items(), key=lambda x: x[1],reverse = True)\n",
    "        elif( mode == 1 ):\n",
    "            items = sorted(self.neutloc.items(), key=lambda x: x[1],reverse = True)\n",
    "        elif( mode == 2 ):\n",
    "            items = sorted(self.posloc.items(), key=lambda x: x[1],reverse = True)\n",
    "        items = items[0:n]\n",
    "        result = [i for i in items if i[0] not in self.removeWordsList]\n",
    "        return result\n",
    "    \n",
    "    #Function to return the top n dates\n",
    "    def getTopNDates(self,n,mode):# mode: 0 | 1 | 2 \n",
    "        if( mode == 0):\n",
    "            items = sorted(self.negdate.items(), key=lambda x: x[1],reverse = True)\n",
    "        elif( mode == 1 ):\n",
    "            items = sorted(self.neutraldate.items(), key=lambda x: x[1],reverse = True)\n",
    "        elif( mode == 2 ):\n",
    "            items = sorted(self.posdate.items(), key=lambda x: x[1],reverse = True)\n",
    "        items = items[0:n]\n",
    "        result = [i for i in items if i[0] not in self.removeWordsList]\n",
    "        return result\n",
    "    \n",
    "    def writeToCSV(self,fname,n,mode):\n",
    "        if(mode == 0):\n",
    "            fname=fname+'_neg' +'.csv'\n",
    "        elif ( mode == 1):\n",
    "            fname=fname+'_neut'+'.csv'\n",
    "        else:\n",
    "            fname=fname+'_pos'+'.csv'\n",
    "            \n",
    "        with open(fname,'w') as out:\n",
    "            csv_out=csv.writer(out)\n",
    "            csv_out.writerow(['word','count'])\n",
    "            rows = self.getTopN(n,mode)\n",
    "            for row in rows:\n",
    "                csv_out.writerow(row)\n",
    "    \n",
    "    def populateCountList(self,text,sentiment):\n",
    "            #Populate word count\n",
    "            if ( sentiment == 2):\n",
    "                for  word in text.split(' '):\n",
    "                    if ( word not in self.poscount ):\n",
    "                        self.poscount[word] = 1\n",
    "                    else:\n",
    "                        self.poscount[word] = self.poscount[word] + 1\n",
    "            elif ( sentiment == 1 ):\n",
    "                for  word in text.split(' '):\n",
    "                    if ( word not in self.neutralcount ):\n",
    "                        self.neutralcount[word] = 1\n",
    "                    else:\n",
    "                        self.neutralcount[word] = self.neutralcount[word] + 1\n",
    "            else:\n",
    "                for  word in text.split(' '):\n",
    "                    if ( word not in self.negcount ):\n",
    "                        self.negcount[word] = 1\n",
    "                    else:\n",
    "                        self.negcount[word] = self.negcount[word] + 1\n",
    "                        \n",
    "    def populateLocationList(self,location,sentiment):\n",
    "            #Populate Location count\n",
    "            if ( sentiment == 2):\n",
    "                    if ( location not in self.posloc ):\n",
    "                        self.posloc[location] = 1\n",
    "                    else:\n",
    "                        self.posloc[location] = self.posloc[location] + 1\n",
    "            elif ( sentiment == 1 ):\n",
    "                    if ( location not in self.neutloc ):\n",
    "                        self.neutloc[location] = 1\n",
    "                    else:\n",
    "                        self.neutloc[location] = self.neutloc[location] + 1\n",
    "            else:\n",
    "                    if ( location not in self.negloc ):\n",
    "                        self.negloc[location] = 1\n",
    "                    else:\n",
    "                        self.negloc[location] = self.negloc[location] + 1\n",
    "    \n",
    "    def populateTimeTweetList(self, date, sentiment):\n",
    "        #first extract the date\n",
    "        #Sat Apr 14 04:46:23 +0000 2018\n",
    "        date_words = date.split(\" \")\n",
    "        _date = date_words[1] + \" \" + date_words[2]\n",
    "        \n",
    "        #populate the date count\n",
    "        if ( sentiment == 2):\n",
    "                    if ( _date not in self.posdate ):\n",
    "                        self.posdate[_date] = 1\n",
    "                    else:\n",
    "                        self.posdate[_date] += 1\n",
    "        elif ( sentiment == 1 ):\n",
    "                    if ( _date not in self.neutraldate ):\n",
    "                        self.neutraldate[_date] = 1\n",
    "                    else:\n",
    "                        self.neutraldate[_date] += 1\n",
    "        else:\n",
    "                    if ( _date not in self.negdate ):\n",
    "                        self.negdate[_date] = 1\n",
    "                    else:\n",
    "                        self.negdate[_date] += 1\n",
    "\n",
    "             \n",
    "    def setWordCount(self,fname):\n",
    "        self.log('File name is,'+fname)\n",
    "        tweets =  jsonParser.loadData(fname)\n",
    "        \n",
    "        self.log('Reading all the tweets')\n",
    "        \n",
    "        for tweet in tweets:\n",
    "            #1.Skip for retweets\n",
    "            if ( tweet['text'][0].lower() == 'r' and  tweet['text'][1].lower() == 't' ):\n",
    "                continue\n",
    "                \n",
    "            text = self.tweetCleaner.cleanTweet( tweet['text'] )\n",
    "            sentiment = self.tweetCleaner.getSentiment( tweet['text'] )\n",
    "            self.populateCountList(text,sentiment)\n",
    "            self.populateLocationList(tweet['user']['location'],sentiment)\n",
    "            self.populateTimeTweetList(tweet['created_at'], sentiment)\n",
    "\n",
    "                    \n",
    "        self.log('Word count is set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "removeWordsList = ['AT_USER','HASH_TAG','...',',','’','rt','','…','flight','?','!','can','t','s','m']\n",
    "visualize = Visualize(False,removeWordsList)\n",
    "visualize.setWordCount('tweets_JetBlue.txt')\n",
    "\n",
    "print ( 'negative', visualize.getTopNWords(30,0) )\n",
    "print ('')\n",
    "print ( 'neutral',visualize.getTopNWords(30,1) )\n",
    "print ('')\n",
    "print ( 'positive',visualize.getTopNWords(30,2) )\n",
    "print ('')\n",
    "print ( 'negative locations', visualize.getTopNLocations(30,0) )\n",
    "print ('')\n",
    "print ( 'neutral locations',visualize.getTopNLocations(30,1) )\n",
    "print ('')\n",
    "print ( 'positive locations',visualize.getTopNLocations(30,2) )\n",
    "print ('')\n",
    "print ( 'negative dates', visualize.getTopNDates(5,0) )\n",
    "print ('')\n",
    "print ( 'neutral dates',visualize.getTopNDates(5,1) )\n",
    "print ('')\n",
    "print ( 'positive dates',visualize.getTopNDates(5,2) )\n",
    "\n",
    "\n",
    "#Write Data to a sav file for visualisation in tableau\n",
    "\n",
    "#visualize.writeToCSV('wc_AmericanAir',200,0)\n",
    "#visualize.writeToCSV('wc_AmericanAir',200,1)\n",
    "#visualize.writeToCSV('wc_AmericanAir',200,2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
