{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tweepy in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages\n",
      "Requirement already satisfied: PySocks>=1.5.7 in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages (from tweepy)\n",
      "Requirement already satisfied: six>=1.10.0 in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages (from tweepy)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages (from tweepy)\n",
      "Requirement already satisfied: requests>=2.11.1 in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages (from tweepy)\n",
      "Requirement already satisfied: oauthlib>=0.6.2 in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages (from requests-oauthlib>=0.7.0->tweepy)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages (from requests>=2.11.1->tweepy)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages (from requests>=2.11.1->tweepy)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages (from requests>=2.11.1->tweepy)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages (from requests>=2.11.1->tweepy)\n",
      "\u001b[33mYou are using pip version 9.0.3, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: textblob in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages\n",
      "Requirement already satisfied: nltk>=3.1 in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages (from textblob)\n",
      "Requirement already satisfied: six in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages (from nltk>=3.1->textblob)\n",
      "\u001b[33mYou are using pip version 9.0.3, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     /Users/srinivassuri/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/srinivassuri/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/srinivassuri/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/srinivassuri/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package conll2000 to\n",
      "[nltk_data]     /Users/srinivassuri/nltk_data...\n",
      "[nltk_data]   Package conll2000 is already up-to-date!\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /Users/srinivassuri/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "Finished.\n",
      "Requirement already satisfied: nltk in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages\n",
      "Requirement already satisfied: six in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages (from nltk)\n",
      "\u001b[33mYou are using pip version 9.0.3, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: numpy in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages\n",
      "\u001b[33mYou are using pip version 9.0.3, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: sklearn in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages\n",
      "Requirement already satisfied: scikit-learn in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages (from sklearn)\n",
      "\u001b[33mYou are using pip version 9.0.3, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: scipy in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages\n",
      "Requirement already satisfied: numpy>=1.8.2 in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages (from scipy)\n",
      "\u001b[33mYou are using pip version 9.0.3, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: wordcloud in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages\n",
      "Requirement already satisfied: pillow in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages (from wordcloud)\n",
      "Requirement already satisfied: numpy>=1.6.1 in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages (from wordcloud)\n",
      "Requirement already satisfied: matplotlib in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages (from wordcloud)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages (from matplotlib->wordcloud)\n",
      "Requirement already satisfied: six>=1.10 in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages (from matplotlib->wordcloud)\n",
      "Requirement already satisfied: pytz in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages (from matplotlib->wordcloud)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages (from matplotlib->wordcloud)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages (from matplotlib->wordcloud)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages (from matplotlib->wordcloud)\n",
      "Requirement already satisfied: setuptools in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib->wordcloud)\n",
      "\u001b[33mYou are using pip version 9.0.3, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: keras in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages\n",
      "Requirement already satisfied: h5py in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages (from keras)\n",
      "Requirement already satisfied: scipy>=0.14 in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages (from keras)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages (from keras)\n",
      "Requirement already satisfied: pyyaml in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages (from keras)\n",
      "Requirement already satisfied: six>=1.9.0 in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages (from keras)\n",
      "\u001b[33mYou are using pip version 9.0.3, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting tensorflow\n",
      "  Downloading https://files.pythonhosted.org/packages/64/6a/c0681f99098edc2ac32e7485ca5046cd47461b6ac379f65932c817913b34/tensorflow-1.7.0-cp36-cp36m-macosx_10_11_x86_64.whl (45.3MB)\n",
      "\u001b[K    100% |████████████████████████████████| 45.3MB 37kB/s eta 0:00:011   27% |████████▊                       | 12.3MB 2.8MB/s eta 0:00:12    69% |██████████████████████▎         | 31.5MB 2.1MB/s eta 0:00:07    92% |█████████████████████████████▊  | 42.1MB 2.0MB/s eta 0:00:02\n",
      "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages (from tensorflow)\n",
      "Collecting astor>=0.6.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/b2/91/cc9805f1ff7b49f620136b3a7ca26f6a1be2ed424606804b0fbcf499f712/astor-0.6.2-py2.py3-none-any.whl\n",
      "Requirement already satisfied: numpy>=1.13.3 in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages (from tensorflow)\n",
      "Collecting absl-py>=0.1.6 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/90/6b/ba04a9fe6aefa56adafa6b9e0557b959e423c49950527139cb8651b0480b/absl-py-0.2.0.tar.gz (82kB)\n",
      "\u001b[K    100% |████████████████████████████████| 92kB 2.9MB/s ta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.4.0 in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages (from tensorflow)\n",
      "Collecting gast>=0.2.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/5c/78/ff794fcae2ce8aa6323e789d1f8b3b7765f601e7702726f430e814822b96/gast-0.2.0.tar.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting grpcio>=1.8.6 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/b9/43/4914451df9fe1acd8e75ecc395bce7fbcf09f87689b2bc4ae3b78b0ddc0f/grpcio-1.11.0-cp36-cp36m-macosx_10_11_x86_64.whl (1.6MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.6MB 613kB/s ta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard<1.8.0,>=1.7.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/0b/ec/65d4e8410038ca2a78c09034094403d231228d0ddcae7d470b223456e55d/tensorboard-1.7.0-py3-none-any.whl (3.1MB)\n",
      "\u001b[K    100% |████████████████████████████████| 3.1MB 529kB/s ta 0:00:01\n",
      "\u001b[?25hCollecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz\n",
      "Requirement already satisfied: wheel>=0.26 in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages (from tensorflow)\n",
      "Requirement already satisfied: setuptools in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages (from protobuf>=3.4.0->tensorflow)\n",
      "Collecting bleach==1.5.0 (from tensorboard<1.8.0,>=1.7.0->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/33/70/86c5fec937ea4964184d4d6c4f0b9551564f821e1c3575907639036d9b90/bleach-1.5.0-py2.py3-none-any.whl\n",
      "Collecting markdown>=2.6.8 (from tensorboard<1.8.0,>=1.7.0->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/6d/7d/488b90f470b96531a3f5788cf12a93332f543dbab13c423a5e7ce96a0493/Markdown-2.6.11-py2.py3-none-any.whl (78kB)\n",
      "\u001b[K    100% |████████████████████████████████| 81kB 3.5MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting werkzeug>=0.11.10 (from tensorboard<1.8.0,>=1.7.0->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/20/c4/12e3e56473e52375aa29c4764e70d1b8f3efa6682bef8d0aae04fe335243/Werkzeug-0.14.1-py2.py3-none-any.whl (322kB)\n",
      "\u001b[K    100% |████████████████████████████████| 327kB 2.1MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting html5lib==0.9999999 (from tensorboard<1.8.0,>=1.7.0->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/ae/ae/bcb60402c60932b32dfaf19bb53870b29eda2cd17551ba5639219fb5ebf9/html5lib-0.9999999.tar.gz (889kB)\n",
      "\u001b[K    100% |████████████████████████████████| 890kB 1.1MB/s ta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: absl-py, gast, termcolor, html5lib\n",
      "  Running setup.py bdist_wheel for absl-py ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/srinivassuri/Library/Caches/pip/wheels/23/35/1d/48c0a173ca38690dd8dfccfa47ffc750db48f8989ed898455c\n",
      "  Running setup.py bdist_wheel for gast ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/srinivassuri/Library/Caches/pip/wheels/9a/1f/0e/3cde98113222b853e98fc0a8e9924480a3e25f1b4008cedb4f\n",
      "  Running setup.py bdist_wheel for termcolor ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/srinivassuri/Library/Caches/pip/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6\n",
      "  Running setup.py bdist_wheel for html5lib ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/srinivassuri/Library/Caches/pip/wheels/50/ae/f9/d2b189788efcf61d1ee0e36045476735c838898eef1cad6e29\n",
      "Successfully built absl-py gast termcolor html5lib\n",
      "Installing collected packages: astor, absl-py, gast, grpcio, html5lib, bleach, markdown, werkzeug, tensorboard, termcolor, tensorflow\n",
      "  Found existing installation: html5lib 1.0.1\n",
      "    Uninstalling html5lib-1.0.1:\n",
      "      Successfully uninstalled html5lib-1.0.1\n",
      "  Found existing installation: bleach 2.1.3\n",
      "    Uninstalling bleach-2.1.3:\n",
      "      Successfully uninstalled bleach-2.1.3\n",
      "Successfully installed absl-py-0.2.0 astor-0.6.2 bleach-1.5.0 gast-0.2.0 grpcio-1.11.0 html5lib-0.9999999 markdown-2.6.11 tensorboard-1.7.0 tensorflow-1.7.0 termcolor-1.1.0 werkzeug-0.14.1\n",
      "\u001b[33mYou are using pip version 9.0.3, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tweepy\n",
    "!pip install textblob\n",
    "!python -m textblob.download_corpora\n",
    "!pip install nltk\n",
    "!pip install numpy\n",
    "!pip install sklearn\n",
    "!pip install scipy\n",
    "!pip install wordcloud\n",
    "!pip install keras\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import re\n",
    "import csv\n",
    "import tweepy\n",
    "import unicodedata\n",
    "import json\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.misc import imread\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM, Convolution1D, Flatten, Dropout, LSTM, MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import TensorBoard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class Definiton for the keywords that will be used to fetch the tweets\n",
    "class keywords:\n",
    "    def __init__(self):\n",
    "        self.keywords = ['AlaskaAir','Allegiant','AmericanAir','Delta','FlyFrontier','HawaiianAir','@united','JetBlue','SouthwestAir','SpiritAirlines','VirginAmerica','SunCountryAir']\n",
    "        \n",
    "    def getKeyWords(self):\n",
    "        return self.keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class Definition to pull the twitter tweets\n",
    "class pullData:      \n",
    "    def __init__(self,key,secret,maxTweets,tweetsPerQry,keywords):\n",
    "        self.key = key   \n",
    "        self.secret = secret\n",
    "        self.maxTweets = maxTweets\n",
    "        self.tweetsPerQry = tweetsPerQry          \n",
    "        self.keywords = keywords\n",
    "        self.api = ''\n",
    "        self.auth = ''\n",
    "        \n",
    "    def printParams(self):\n",
    "        print('Parameters set to...')\n",
    "        print('key...',self.key)\n",
    "        print('secret...',self.secret)\n",
    "        print('maxTweets...',self.maxTweets)\n",
    "        print('tweetsPerQry...',self.tweetsPerQry)\n",
    "        print('keywords...',self.keywords)\n",
    "        \n",
    "    def connect(self):\n",
    "        self.auth = tweepy.AppAuthHandler(self.key,self.secret )  \n",
    "        self.api = tweepy.API(self.auth, wait_on_rate_limit=True,wait_on_rate_limit_notify=True)\n",
    "        return True\n",
    "        \n",
    "        if (not self.api):\n",
    "            print (\"Can't Authenticate\")\n",
    "            return False\n",
    "        \n",
    "    def downloadData(self):\n",
    "        for word in self.keywords:\n",
    "            print( \"Downloading Tweets for the keyword: \", word )\n",
    "            fName = 'tweets_' + word + '.txt'\n",
    "            sinceId = None\n",
    "            max_id = -1\n",
    "            tweetCount = 0\n",
    "            tweet_dict = []\n",
    "    \n",
    "            print(\"Downloading max {0} tweets\".format(maxTweets))\n",
    "\n",
    "            while tweetCount < self.maxTweets:\n",
    "                try:\n",
    "                    if (max_id <= 0):\n",
    "                        if (not sinceId):   \n",
    "                            new_tweets = self.api.search(q=word, count=self.tweetsPerQry)\n",
    "                        else:\n",
    "                            new_tweets = self.api.search(q=word, count=self.tweetsPerQry,since_id=sinceId)\n",
    "                    else:\n",
    "                        if (not sinceId):\n",
    "                            new_tweets = self.api.search(q=word, count=self.tweetsPerQry,\n",
    "                                    max_id=str(max_id - 1))\n",
    "                        else:\n",
    "                            new_tweets = self.api.search(q=word, count=self.tweetsPerQry,\n",
    "                                    max_id=str(max_id - 1),since_id=sinceId)\n",
    "                    if not new_tweets:\n",
    "                        print(\"No more tweets found\")\n",
    "                        break            \n",
    "                        \n",
    "                    for tweet in new_tweets:\n",
    "                        tweet_dict.append(tweet._json)\n",
    " \n",
    "                    tweetCount += len(new_tweets)\n",
    "                    print(\"Downloaded {0} tweets\".format(tweetCount))\n",
    "                    max_id = new_tweets[-1].id\n",
    "                \n",
    "                except tweepy.TweepError as e:\n",
    "                    # Just exit if any error\n",
    "                    print(\"some error : \" + str(e))\n",
    "                    break\n",
    "            \n",
    "            with open(fName, 'w', encoding='utf8', errors='replace') as f:   \n",
    "                json.dump(tweet_dict, f, ensure_ascii=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "consumer_key = 'bppIV0LkQUIARDug4b8Sij8pm'\n",
    "consumer_secret = '5ofpZtxL1CsDmfbF93Qh0EcPOHUVP3ZSF6CGhbkk2ki2fFWNFL'\n",
    "maxTweets = 100000 # Some arbitrary large number\n",
    "tweetsPerQry = 100  # this is the max the API permits\n",
    "\n",
    "keys = keywords()\n",
    "words = keys.getKeyWords()  # this is what we're searching for\n",
    "\n",
    "pulldata = pullData(consumer_key,consumer_secret,maxTweets,tweetsPerQry,words)\n",
    "\n",
    "if ( pulldata.connect() == False ):\n",
    "    print ( \"Connecting to the twitter API Failed\")\n",
    "    sys.exit(1)\n",
    "pulldata.printParams()\n",
    "    \n",
    "pulldata.downloadData()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class definition for JsonParser\n",
    "\n",
    "class JsonParser:\n",
    "    def loadData(self,fname):\n",
    "        with open(fname, encoding='utf8', errors='replace') as json_data:\n",
    "            d = json.load(json_data)\n",
    "        return d\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class definition to Clean the tweets and get the sentiment\n",
    "class TweetCleaner: \n",
    "    def __init__(self,stopwords_fname):\n",
    "        self.stopwords_fname = stopwords_fname\n",
    "        self.negation_cues = self.get_negation_cues('negation_cues.txt')\n",
    "        self.sentiment_fnames = [\"EffectWordNet.tff\", \"subjclueslen1-HLTEMNLP05.tff\"]\n",
    "        self.emoji_pattern = re.compile(r'[\\U00010000-\\U0010ffff]', flags=re.UNICODE)\n",
    "        self.url_pattern = re.compile(r'https?:\\/\\/.*\\b')\n",
    "        self.handle_pattern = re.compile(r'@\\w+')\n",
    "        self.hashtag_pattern = re.compile(r'#\\w+')\n",
    "        \n",
    "        #Populate the Stop Words\n",
    "        self.stopwords = set()\n",
    "        self.populateStopWords(self.stopwords_fname)\n",
    "        \n",
    "        #populate the wordtoEffectMap\n",
    "        self.wordToEffectMap = {}\n",
    "        self.buildWordToEffectMap(self.sentiment_fnames)\n",
    "        \n",
    "        #Initialize nltk classes\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.tokenizer = TweetTokenizer()\n",
    "    \n",
    "    #---------Create the Stop words Set---------#\n",
    "    def populateStopWords(self,fname):\n",
    "        stop_file = open(fname)\n",
    "        \n",
    "        for line in stop_file:\n",
    "            self.stopwords.add(line.strip())\n",
    "    \n",
    "    def get_negation_cues(self, fname):\n",
    "        cues_fname = open(fname, 'r')\n",
    "        neg = []\n",
    "        for line in cues_fname:\n",
    "            if not line:\n",
    "                continue\n",
    "            line = line.strip()\n",
    "            neg.append(line)\n",
    "        return neg\n",
    "    \n",
    "    #---------Build the word to +/- Effect Map----------#\n",
    "    def buildWordToEffectMap(self, sentiment_files):\n",
    "        #read sentiment files into dictionary of words and positive or negative sentiment\n",
    "        for file in sentiment_files:\n",
    "            with open(file, \"r\", encoding='utf8', errors='replace') as f:\n",
    "                if(file == \"EffectWordNet.tff\"):\n",
    "                    for line in f:\n",
    "                        #02279615\t+Effect\tprofiteer\t make an unreasonable profit, as on the sale of difficult to obtain goods \n",
    "                        words = line.split('\\t')\n",
    "                        effect = words[1]\n",
    "                        effect_val = 0\n",
    "                        if '+' in effect:\n",
    "                            effect_val = 1\n",
    "                        elif '-' in effect:\n",
    "                            effect_val = -1\n",
    "                        else:\n",
    "                            effect_val = 0\n",
    "                        list_of_words = []\n",
    "                        if (',' in words[2]):\n",
    "                            list_of_words = words[2].split(',')\n",
    "                        else:\n",
    "                            list_of_words.append(words[2])\n",
    "                        for word in list_of_words:\n",
    "                            self.wordToEffectMap[word] = effect_val\n",
    "                elif(file == \"subjclueslen1-HLTEMNLP05.tff\"):\n",
    "                    for line in f:\n",
    "                        #type=weaksubj len=1 word1=abandoned pos1=adj stemmed1=n priorpolarity=negative\n",
    "                        line = line.strip()\n",
    "                        if not line:\n",
    "                            continue\n",
    "                        words = line.split(' ')\n",
    "                        effect_val = words[5]\n",
    "                        #print(effect_val)\n",
    "                        word_val = words[2]\n",
    "                        word = word_val.split(\"=\")[1]\n",
    "                        effect = effect_val.split(\"=\")[1]\n",
    "                        eff = 0\n",
    "                        if (effect == \"positive\"):\n",
    "                            eff = 1\n",
    "                        elif (effect == \"negative\"):\n",
    "                            eff = -1\n",
    "                        elif (effect == \"neutral\"):\n",
    "                            eff = 0\n",
    "                        self.wordToEffectMap[word] = eff\n",
    "        \n",
    "    #---------Clean the tweets---------#\n",
    "    def cleanTweet(self,tweet):\n",
    "        tweet = tweet.lower()\n",
    "        \n",
    "        #1. remove emojis\n",
    "        tweet = re.sub(self.emoji_pattern, r'', tweet)\n",
    "        \n",
    "        #remove URLS\n",
    "        tweet = re.sub(self.url_pattern, r'', tweet)\n",
    "\n",
    "        #convert @mentions to AT_USER\n",
    "        tweet = re.sub(self.handle_pattern, r'AT_USER', tweet)\n",
    "        \n",
    "        #convert #tags to HASH_TAG\n",
    "        tweet = re.sub(self.hashtag_pattern, r'HASH_TAG', tweet)\n",
    "        \n",
    "        tweet_list = self.tokenizer.tokenize(tweet)\n",
    "        \n",
    "        #2. remove the stop words\n",
    "        words_filtered = []\n",
    "        for word in tweet_list:\n",
    "            if (word not in self.stopwords ):\n",
    "                words_filtered.append( word )\n",
    "              \n",
    "        #3. Stem the words\n",
    "        #words_stemmed = [self.stemmer.stem(word) for word in words_filtered]\n",
    "        \n",
    "        _tweet = \"\"\n",
    "        for word in words_filtered:\n",
    "            _tweet += \" \" + word\n",
    "        \n",
    "        #4. return the fitered tweet\n",
    "        return _tweet\n",
    "             \n",
    "    #--------- negative: 0 , neutral : 1 , positive : 2 ---------#\n",
    "    def getSentiment(self,tweet):\n",
    "        score = 0\n",
    "        tweet = tweet.lower()\n",
    "        tweet_list = self.tokenizer.tokenize(tweet)\n",
    "        for word in tweet_list:\n",
    "            if word in self.wordToEffectMap:\n",
    "                score += self.wordToEffectMap[word]\n",
    "        if (score < 0):\n",
    "            return 0\n",
    "        elif (score > 0):\n",
    "            return 2\n",
    "        else:\n",
    "            return 1\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleans all the tweets , builds the tweet set and adds the sentiment to the tweets\n",
    "class BuildFeatureSet: \n",
    "    \n",
    "    def __init__(self,n,stopwords_fname,max_features_count=5000): # n : ngram for the tweets\n",
    "        self.ngrams = n\n",
    "\n",
    "        #Lexicon related variables\n",
    "        self.lexicon = {}\n",
    "        self.inverse_lexicon = { }\n",
    "        self.ngram_count = 0\n",
    "        self.max_features_count = max_features_count\n",
    "        self.tweetCleaner = TweetCleaner(stopwords_fname)\n",
    "        self.tweet_count = 0\n",
    "        self.tweet_map = {}\n",
    "        \n",
    "        #Training Dataset\n",
    "        self.data = {}\n",
    "\n",
    "    def addToLexicon(self,words):\n",
    "        for word in words:\n",
    "            if ( word not in self.lexicon and ( self.ngram_count <  self.max_features_count ) ):\n",
    "                self.lexicon[word] = self.ngram_count #Assign a unique number for the word seen\n",
    "                self.inverse_lexicon[self.ngram_count] = word\n",
    "                self.ngram_count = self.ngram_count + 1\n",
    "                #print( 'Lexicon: ',self.ngram_count,word)\n",
    "    \n",
    "    def isTweetReply(self,tweet):\n",
    "        if ( tweet[0].lower() == 'r' and  tweet[1].lower() == 't' ):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def get_tweet_map(self):\n",
    "        return self.tweet_map\n",
    "        \n",
    "    def isTweetFromAirline(self, tweet, airline_handle):\n",
    "        if( tweet['user']['screen_name'] == airline_handle):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    #Add the tweet to the lexicon set\n",
    "    def addTweet(self,tweet):\n",
    "        \n",
    "        self.tweet_count += 1\n",
    "        self.tweet_map[self.tweet_count] = tweet\n",
    "        \n",
    "        #1. get the sentiment for the tweet\n",
    "        sentiment = self.tweetCleaner.getSentiment(tweet)\n",
    "        \n",
    "        #2. Clean the Tweet\n",
    "        tweet = self.tweetCleaner.cleanTweet(tweet)\n",
    "        \n",
    "        #3. get the ngrams for the tweet\n",
    "        _ngrams = nltk.ngrams(tweet.split(), self.ngrams)\n",
    "            \n",
    "        #4. Add the ngrams to the lexicon dictionary\n",
    "        words = list(_ngrams)\n",
    "        self.addToLexicon( words )\n",
    "        \n",
    "        #5. Add this tweet row to the training set\n",
    "        self.addToTrainingData(words,sentiment)\n",
    "    \n",
    "    #Build the Feature set for all the tweets\n",
    "    def addToTrainingData(self,ngrams,sentiment):\n",
    "        row = np.zeros(self.max_features_count + 1 ) # last feature is the label \n",
    "        \n",
    "        for word in ngrams:\n",
    "            if ( word in self.lexicon ):\n",
    "                row[ self.lexicon[word] ] = row[ self.lexicon[word] ] + 1 #Increase the count of the word \n",
    "\n",
    "        row [ self.max_features_count ] = sentiment\n",
    "        \n",
    "        self.data[ len(self.data)  ]  = row \n",
    "    \n",
    "    \n",
    "    def getFeatures(self):\n",
    "        features = np.zeros(( len(self.data), self.max_features_count ))\n",
    "        \n",
    "        for index in self.data:\n",
    "            features[index][:] =  self.data[index][:-1]\n",
    "            \n",
    "        return features\n",
    "    \n",
    "    def getLabels(self):\n",
    "        labels = np.zeros( len(self.data) )\n",
    "        \n",
    "        for index in self.data:\n",
    "            labels[index] = ( int(self.data[index][self.max_features_count]) )\n",
    "            \n",
    "        return labels\n",
    "    \n",
    "    def getHeaders(self):\n",
    "        headers = []\n",
    "        for i in range(self.max_features_count):\n",
    "            headers.append( self.inverse_lexicon[i] )\n",
    "        return headers\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Classifiers\n",
    "class Classifiers:\n",
    "    def __init__(self,ngrams,stopwords_fname,f_airlines,split,noOfFeatures):\n",
    "        self.ngrams = ngrams\n",
    "        self.f_airlines = f_airlines\n",
    "        self.split  = split\n",
    "        self.noOfFeatures=noOfFeatures\n",
    "\n",
    "        #1.Define Custom Classes required for the class\n",
    "        self.features = BuildFeatureSet(self.ngrams, stopwords_fname, max_features_count = self.noOfFeatures)\n",
    "        self.jsonParser = JsonParser()\n",
    "\n",
    "        #2. Build the feature set\n",
    "        print ('Building the feature set')\n",
    "        \n",
    "        for file in f_airlines:\n",
    "            print ('Parsing tweets from the file:',file)\n",
    "            tweets =  self.jsonParser.loadData(file)\n",
    "            airline_handle = file.split(\"_\")[1]\n",
    "            for tweet in tweets:\n",
    "                #1. Skip if the tweet the is a reply to an existing tweet\n",
    "                if ( self.features.isTweetReply(tweet['text']) == False and self.features.isTweetFromAirline(tweet, airline_handle) == False ):\n",
    "                    self.features.addTweet ( tweet['text']  ) \n",
    "        \n",
    "        #3. Prepare the train and test datasets\n",
    "        print ( 'Preparing the training and the test set...')\n",
    "        self.X = self.features.getFeatures()\n",
    "        self.y = self.features.getLabels()\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.X, self.y, train_size= self.split,test_size=1- self.split)\n",
    "        print ( 'Done preapring the train and test set...Ready to train classifiers')\n",
    "        \n",
    "        #4. Define Variables for classifiers performance\n",
    "        #4a. Logistic Regression\n",
    "        self.logreg_train_acc = 0\n",
    "        self.logreg_test_acc = 0\n",
    "        \n",
    "        #4b. Gaussian Naive Bayes\n",
    "        self.nb_train_acc = 0\n",
    "        self.nb_test_acc = 0\n",
    "        \n",
    "        #4c. LibLinearSVC\n",
    "        self.svc_train_acc = 0\n",
    "        self.svc_test_acc = 0\n",
    "        \n",
    "        #4d. Decision Tree\n",
    "        self.dt_train_acc = 0\n",
    "        self.dt_test_acc = 0\n",
    "\n",
    "        #4e.  Ada Boost\n",
    "        self.adaboost_train_acc = 0\n",
    "        self.adaboost_test_acc = 0\n",
    "        \n",
    "        #4f.\n",
    "        self.rf_train_acc = 0\n",
    "        self.rf_test_acc = 0\n",
    "    \n",
    "    def applyPCA(self,n):\n",
    "        print ( 'Applying PCA on X' )\n",
    "        pca = PCA(n_components=n)\n",
    "        self.X = pca.fit_transform(self.X)\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.X, self.y, train_size= self.split,test_size=1- self.split)\n",
    "        \n",
    "    def printResults(self):\n",
    "        print ( 'Results for Logistic Regression')\n",
    "        print( '(train,test):(',self.logreg_train_acc,self.logreg_test_acc,')' )\n",
    "        print ( 'Results for Gaussian Naive Bayes')\n",
    "        print( '(train,test):(',self.nb_train_acc,self.nb_test_acc,')' )\n",
    "        print ( 'Results for LibLinear SVM')\n",
    "        print( '(train,test):(',self.svc_train_acc,self.svc_test_acc,')' )\n",
    "        print ( 'Results for Decision Tree')\n",
    "        print( '(train,test):(',self.dt_train_acc,self.dt_test_acc,')' )\n",
    "        print ( 'Results for Adaboost')\n",
    "        print( '(train,test):(',self.adaboost_train_acc,self.adaboost_test_acc,')' )\n",
    "        print ( 'Results for Random Forests')\n",
    "        print( '(train,test):(',self.rf_train_acc,self.rf_test_acc,')' )\n",
    "    \n",
    "    def getResults(self):\n",
    "        return [self.logreg_train_acc,self.logreg_test_acc, \n",
    "                self.nb_train_acc,self.nb_test_acc, \n",
    "                self.svc_train_acc,self.svc_test_acc,  \n",
    "                self.dt_train_acc,self.dt_test_acc, \n",
    "                self.adaboost_train_acc,self.adaboost_test_acc, \n",
    "                self.rf_train_acc,self.rf_test_acc]\n",
    "    \n",
    "    def runClassifiers(self):\n",
    "            #1. Logistic regression classifier\n",
    "            print('Training Logistic Regression Classifiers')\n",
    "\n",
    "            #C = [0.001,  0.01,  0.1,  1.0,  10,  100]\n",
    "            C = [0.1,10]\n",
    "\n",
    "            logreg = GridSearchCV(LogisticRegression(max_iter=100),cv = 10, param_grid= {\"C\" : C},verbose=1,n_jobs=60)\n",
    "            logreg.fit(self.X_train, self.y_train)\n",
    "\n",
    "            #print(clf.best_params_)\n",
    "            self.logreg_train_acc = accuracy_score( self.y_train,logreg.predict(self.X_train) )\n",
    "            self.logreg_test_acc = accuracy_score( self.y_test,logreg.predict(self.X_test) )\n",
    "            \n",
    "            #2. Naive Bayes\n",
    "            print('Training Gaussian Naive Bayes Classifier')\n",
    "            nb = GaussianNB()\n",
    "            nb.fit(self.X_train, self.y_train)\n",
    "\n",
    "            self.nb_train_acc = accuracy_score( self.y_train,nb.predict(self.X_train) )\n",
    "            self.nb_test_acc = accuracy_score( self.y_test,nb.predict(self.X_test) )\n",
    "            \n",
    "            #3.LibLinear SVM\n",
    "            print( 'Training SVM Lib Linear Classifier')\n",
    "            svc = GridSearchCV(LinearSVC(max_iter=1000),cv = 10, param_grid= {\"C\" : C},verbose=1,n_jobs=60)\n",
    "            svc.fit(self.X_train, self.y_train)\n",
    "\n",
    "            self.svc_train_acc =  accuracy_score(self.y_train,svc.predict(self.X_train)) \n",
    "            self.svc_test_acc = accuracy_score(self.y_test,svc.predict(self.X_test)) \n",
    "            \n",
    "            #4.Decision Tree\n",
    "            print( 'Training Decision Tree Classifier' )\n",
    "            max_depth = [1,3,5,7]\n",
    "            dt = GridSearchCV(DecisionTreeClassifier(),cv = 10, param_grid= {\"max_depth\" : max_depth},verbose=1,n_jobs=60)\n",
    "            dt.fit(self.X_train, self.y_train)\n",
    "            \n",
    "            self.dt_train_acc =  accuracy_score(self.y_train,dt.predict(self.X_train)) \n",
    "            self.dt_test_acc = accuracy_score(self.y_test,dt.predict(self.X_test)) \n",
    "            \n",
    "            #5. AdaBoost\n",
    "            print( 'Training AdaBoost' )\n",
    "            grd = GradientBoostingClassifier(n_estimators=500,max_depth=5,random_state=42)\n",
    "            grd.fit(self.X_train, self.y_train)\n",
    "            self.adaboost_train_acc =  accuracy_score(self.y_train,grd.predict(self.X_train)) \n",
    "            self.adaboost_test_acc = accuracy_score(self.y_test,grd.predict(self.X_test)) \n",
    "            \n",
    "            #6. Random Forests\n",
    "            print( 'Training Random Forests' )\n",
    "            rf = RandomForestClassifier(n_estimators=500,max_depth=5,random_state=42)\n",
    "            rf.fit(self.X_train, self.y_train)\n",
    "            self.rf_train_acc =  accuracy_score(self.y_train,rf.predict(self.X_train)) \n",
    "            self.rf_test_acc = accuracy_score(self.y_test,rf.predict(self.X_test)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate results for  0 'th gram Classifiers\n",
      "Building the feature set\n",
      "Parsing tweets from the file: tweets_HawaiianAir.txt\n",
      "Preparing the training and the test set...\n",
      "Done preapring the train and test set...Ready to train classifiers\n",
      "Training Logistic Regression Classifiers\n",
      "Fitting 10 folds for each of 2 candidates, totalling 20 fits\n",
      "Training Gaussian Naive Bayes Classifier\n",
      "Training SVM Lib Linear Classifier\n",
      "Fitting 10 folds for each of 2 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=60)]: Done   6 out of  20 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=60)]: Done  20 out of  20 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=60)]: Done   6 out of  20 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=60)]: Done  20 out of  20 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Decision Tree Classifier\n",
      "Fitting 10 folds for each of 4 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=60)]: Done   3 out of  40 | elapsed:    0.0s remaining:    0.3s\n",
      "[Parallel(n_jobs=60)]: Done  40 out of  40 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AdaBoost\n",
      "Training Random Forests\n",
      "Generate results for  1 'th gram Classifiers\n",
      "Building the feature set\n",
      "Parsing tweets from the file: tweets_HawaiianAir.txt\n",
      "Preparing the training and the test set...\n",
      "Done preapring the train and test set...Ready to train classifiers\n",
      "Training Logistic Regression Classifiers\n",
      "Fitting 10 folds for each of 2 candidates, totalling 20 fits\n",
      "Training Gaussian Naive Bayes Classifier\n",
      "Training SVM Lib Linear Classifier\n",
      "Fitting 10 folds for each of 2 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=60)]: Done   6 out of  20 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=60)]: Done  20 out of  20 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=60)]: Done   6 out of  20 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=60)]: Done  20 out of  20 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Decision Tree Classifier\n",
      "Fitting 10 folds for each of 4 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=60)]: Done   3 out of  40 | elapsed:    0.0s remaining:    0.2s\n",
      "[Parallel(n_jobs=60)]: Done  40 out of  40 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AdaBoost\n",
      "Training Random Forests\n",
      "Generate results for  2 'th gram Classifiers\n",
      "Building the feature set\n",
      "Parsing tweets from the file: tweets_HawaiianAir.txt\n",
      "Preparing the training and the test set...\n",
      "Done preapring the train and test set...Ready to train classifiers\n",
      "Training Logistic Regression Classifiers\n",
      "Fitting 10 folds for each of 2 candidates, totalling 20 fits\n",
      "Training Gaussian Naive Bayes Classifier\n",
      "Training SVM Lib Linear Classifier\n",
      "Fitting 10 folds for each of 2 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=60)]: Done   6 out of  20 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=60)]: Done  20 out of  20 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=60)]: Done   6 out of  20 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=60)]: Done  20 out of  20 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Decision Tree Classifier\n",
      "Fitting 10 folds for each of 4 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=60)]: Done   3 out of  40 | elapsed:    0.0s remaining:    0.3s\n",
      "[Parallel(n_jobs=60)]: Done  40 out of  40 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AdaBoost\n",
      "Training Random Forests\n",
      "Generate results for  3 'th gram Classifiers\n",
      "Building the feature set\n",
      "Parsing tweets from the file: tweets_HawaiianAir.txt\n",
      "Preparing the training and the test set...\n",
      "Done preapring the train and test set...Ready to train classifiers\n",
      "Training Logistic Regression Classifiers\n",
      "Fitting 10 folds for each of 2 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/srinivassuri/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:58: DeprecationWarning: generator 'ngrams' raised StopIteration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Gaussian Naive Bayes Classifier\n",
      "Training SVM Lib Linear Classifier\n",
      "Fitting 10 folds for each of 2 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=60)]: Done   6 out of  20 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=60)]: Done  20 out of  20 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Decision Tree Classifier\n",
      "Fitting 10 folds for each of 4 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=60)]: Done   6 out of  20 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=60)]: Done  20 out of  20 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=60)]: Done   3 out of  40 | elapsed:    0.0s remaining:    0.3s\n",
      "[Parallel(n_jobs=60)]: Done  40 out of  40 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AdaBoost\n",
      "Training Random Forests\n",
      "Generate results for  4 'th gram Classifiers\n",
      "Building the feature set\n",
      "Parsing tweets from the file: tweets_HawaiianAir.txt\n",
      "Preparing the training and the test set...\n",
      "Done preapring the train and test set...Ready to train classifiers\n",
      "Training Logistic Regression Classifiers\n",
      "Fitting 10 folds for each of 2 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/srinivassuri/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:58: DeprecationWarning: generator 'ngrams' raised StopIteration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Gaussian Naive Bayes Classifier\n",
      "Training SVM Lib Linear Classifier\n",
      "Fitting 10 folds for each of 2 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=60)]: Done   6 out of  20 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=60)]: Done  20 out of  20 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Decision Tree Classifier\n",
      "Fitting 10 folds for each of 4 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=60)]: Done   6 out of  20 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=60)]: Done  20 out of  20 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=60)]: Done   3 out of  40 | elapsed:    0.0s remaining:    0.3s\n",
      "[Parallel(n_jobs=60)]: Done  40 out of  40 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AdaBoost\n",
      "Training Random Forests\n",
      "Results (features): ( 25 ):\n",
      "   logreg_train  logreg_test  nb_train   nb_test  svm_train  svm_test  \\\n",
      "0      0.591304     0.534483  0.266667  0.146552   0.591304  0.525862   \n",
      "1      0.602899     0.525862  0.214493  0.206897   0.617391  0.525862   \n",
      "2      0.562319     0.517241  0.200000  0.163793   0.562319  0.517241   \n",
      "3      0.498551     0.439655  0.179710  0.224138   0.504348  0.439655   \n",
      "4      0.486957     0.482759  0.179710  0.206897   0.492754  0.482759   \n",
      "\n",
      "   dt_train   dt_test  adaboost_train  adaboost_test  rf_train   rf_test  \n",
      "0  0.626087  0.612069        0.695652       0.612069  0.611594  0.534483  \n",
      "1  0.576812  0.525862        0.713043       0.568966  0.608696  0.534483  \n",
      "2  0.524638  0.500000        0.573913       0.534483  0.573913  0.551724  \n",
      "3  0.501449  0.439655        0.510145       0.448276  0.510145  0.448276  \n",
      "4  0.486957  0.482759        0.492754       0.482759  0.492754  0.482759  \n",
      "Generate results for  0 'th gram Classifiers\n",
      "Building the feature set\n",
      "Parsing tweets from the file: tweets_HawaiianAir.txt\n",
      "Preparing the training and the test set...\n",
      "Done preapring the train and test set...Ready to train classifiers\n",
      "Training Logistic Regression Classifiers\n",
      "Fitting 10 folds for each of 2 candidates, totalling 20 fits\n",
      "Training Gaussian Naive Bayes Classifier\n",
      "Training SVM Lib Linear Classifier\n",
      "Fitting 10 folds for each of 2 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=60)]: Done   6 out of  20 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=60)]: Done  20 out of  20 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=60)]: Done   6 out of  20 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=60)]: Done  20 out of  20 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Decision Tree Classifier\n",
      "Fitting 10 folds for each of 4 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=60)]: Done   3 out of  40 | elapsed:    0.0s remaining:    0.3s\n",
      "[Parallel(n_jobs=60)]: Done  40 out of  40 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AdaBoost\n",
      "Training Random Forests\n",
      "Generate results for  1 'th gram Classifiers\n",
      "Building the feature set\n",
      "Parsing tweets from the file: tweets_HawaiianAir.txt\n",
      "Preparing the training and the test set...\n",
      "Done preapring the train and test set...Ready to train classifiers\n",
      "Training Logistic Regression Classifiers\n",
      "Fitting 10 folds for each of 2 candidates, totalling 20 fits\n",
      "Training Gaussian Naive Bayes Classifier\n",
      "Training SVM Lib Linear Classifier\n",
      "Fitting 10 folds for each of 2 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=60)]: Done   6 out of  20 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=60)]: Done  20 out of  20 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=60)]: Done   6 out of  20 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=60)]: Done  20 out of  20 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Decision Tree Classifier\n",
      "Fitting 10 folds for each of 4 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=60)]: Done   3 out of  40 | elapsed:    0.0s remaining:    0.3s\n",
      "[Parallel(n_jobs=60)]: Done  40 out of  40 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AdaBoost\n",
      "Training Random Forests\n",
      "Generate results for  2 'th gram Classifiers\n",
      "Building the feature set\n",
      "Parsing tweets from the file: tweets_HawaiianAir.txt\n",
      "Preparing the training and the test set...\n",
      "Done preapring the train and test set...Ready to train classifiers\n",
      "Training Logistic Regression Classifiers\n",
      "Fitting 10 folds for each of 2 candidates, totalling 20 fits\n",
      "Training Gaussian Naive Bayes Classifier\n",
      "Training SVM Lib Linear Classifier\n",
      "Fitting 10 folds for each of 2 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=60)]: Done   6 out of  20 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=60)]: Done  20 out of  20 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=60)]: Done   6 out of  20 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=60)]: Done  20 out of  20 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Decision Tree Classifier\n",
      "Fitting 10 folds for each of 4 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=60)]: Done   3 out of  40 | elapsed:    0.0s remaining:    0.3s\n",
      "[Parallel(n_jobs=60)]: Done  40 out of  40 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AdaBoost\n",
      "Training Random Forests\n",
      "Generate results for  3 'th gram Classifiers\n",
      "Building the feature set\n",
      "Parsing tweets from the file: tweets_HawaiianAir.txt\n",
      "Preparing the training and the test set...\n",
      "Done preapring the train and test set...Ready to train classifiers\n",
      "Training Logistic Regression Classifiers\n",
      "Fitting 10 folds for each of 2 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/srinivassuri/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:58: DeprecationWarning: generator 'ngrams' raised StopIteration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Gaussian Naive Bayes Classifier\n",
      "Training SVM Lib Linear Classifier\n",
      "Fitting 10 folds for each of 2 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=60)]: Done   6 out of  20 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=60)]: Done  20 out of  20 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Decision Tree Classifier\n",
      "Fitting 10 folds for each of 4 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=60)]: Done   6 out of  20 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=60)]: Done  20 out of  20 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=60)]: Done   3 out of  40 | elapsed:    0.0s remaining:    0.4s\n",
      "[Parallel(n_jobs=60)]: Done  40 out of  40 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AdaBoost\n",
      "Training Random Forests\n",
      "Generate results for  4 'th gram Classifiers\n",
      "Building the feature set\n",
      "Parsing tweets from the file: tweets_HawaiianAir.txt\n",
      "Preparing the training and the test set...\n",
      "Done preapring the train and test set...Ready to train classifiers\n",
      "Training Logistic Regression Classifiers\n",
      "Fitting 10 folds for each of 2 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/srinivassuri/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:58: DeprecationWarning: generator 'ngrams' raised StopIteration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Gaussian Naive Bayes Classifier\n",
      "Training SVM Lib Linear Classifier\n",
      "Fitting 10 folds for each of 2 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=60)]: Done   6 out of  20 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=60)]: Done  20 out of  20 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Decision Tree Classifier\n",
      "Fitting 10 folds for each of 4 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=60)]: Done   6 out of  20 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=60)]: Done  20 out of  20 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=60)]: Done   3 out of  40 | elapsed:    0.0s remaining:    0.4s\n",
      "[Parallel(n_jobs=60)]: Done  40 out of  40 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AdaBoost\n",
      "Training Random Forests\n",
      "Results (features): ( 50 ):\n",
      "   logreg_train  logreg_test  nb_train   nb_test  svm_train  svm_test  \\\n",
      "0      0.631884     0.543103  0.278261  0.215517   0.637681  0.534483   \n",
      "1      0.620290     0.594828  0.443478  0.396552   0.605797  0.612069   \n",
      "2      0.533333     0.543103  0.243478  0.189655   0.515942  0.551724   \n",
      "3      0.481159     0.491379  0.356522  0.344828   0.489855  0.491379   \n",
      "4      0.478261     0.508621  0.173913  0.232759   0.486957  0.508621   \n",
      "\n",
      "   dt_train   dt_test  adaboost_train  adaboost_test  rf_train   rf_test  \n",
      "0  0.605797  0.525862        0.776812       0.482759  0.594203  0.551724  \n",
      "1  0.628986  0.594828        0.782609       0.456897  0.602899  0.586207  \n",
      "2  0.542029  0.568966        0.576812       0.603448  0.559420  0.586207  \n",
      "3  0.481159  0.491379        0.495652       0.508621  0.495652  0.508621  \n",
      "4  0.478261  0.508621        0.486957       0.508621  0.486957  0.508621  \n",
      "Results After Performing PCA \n",
      "Generate results for ith gram Classifiers\n",
      "Building the feature set\n",
      "Parsing tweets from the file: tweets_HawaiianAir.txt\n",
      "Preparing the training and the test set...\n",
      "Done preapring the train and test set...Ready to train classifiers\n",
      "Applying PCA on X\n",
      "Training Logistic Regression Classifiers\n",
      "Fitting 10 folds for each of 2 candidates, totalling 20 fits\n",
      "Training Gaussian Naive Bayes Classifier\n",
      "Training SVM Lib Linear Classifier\n",
      "Fitting 10 folds for each of 2 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=60)]: Done   6 out of  20 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=60)]: Done  20 out of  20 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=60)]: Done   6 out of  20 | elapsed:    0.0s remaining:    0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Decision Tree Classifier\n",
      "Fitting 10 folds for each of 4 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=60)]: Done  20 out of  20 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=60)]: Done   3 out of  40 | elapsed:    0.0s remaining:    0.4s\n",
      "[Parallel(n_jobs=60)]: Done  40 out of  40 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AdaBoost\n",
      "Training Random Forests\n",
      "Generate results for ith gram Classifiers\n",
      "Building the feature set\n",
      "Parsing tweets from the file: tweets_HawaiianAir.txt\n",
      "Preparing the training and the test set...\n",
      "Done preapring the train and test set...Ready to train classifiers\n",
      "Applying PCA on X\n",
      "Training Logistic Regression Classifiers\n",
      "Fitting 10 folds for each of 2 candidates, totalling 20 fits\n",
      "Training Gaussian Naive Bayes Classifier\n",
      "Training SVM Lib Linear Classifier\n",
      "Fitting 10 folds for each of 2 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=60)]: Done   6 out of  20 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=60)]: Done  20 out of  20 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=60)]: Done   6 out of  20 | elapsed:    0.0s remaining:    0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Decision Tree Classifier\n",
      "Fitting 10 folds for each of 4 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=60)]: Done  20 out of  20 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=60)]: Done   3 out of  40 | elapsed:    0.0s remaining:    0.4s\n",
      "[Parallel(n_jobs=60)]: Done  40 out of  40 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AdaBoost\n",
      "Training Random Forests\n",
      "Generate results for ith gram Classifiers\n",
      "Building the feature set\n",
      "Parsing tweets from the file: tweets_HawaiianAir.txt\n",
      "Preparing the training and the test set...\n",
      "Done preapring the train and test set...Ready to train classifiers\n",
      "Applying PCA on X\n",
      "Training Logistic Regression Classifiers\n",
      "Fitting 10 folds for each of 2 candidates, totalling 20 fits\n",
      "Training Gaussian Naive Bayes Classifier\n",
      "Training SVM Lib Linear Classifier\n",
      "Fitting 10 folds for each of 2 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=60)]: Done   6 out of  20 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=60)]: Done  20 out of  20 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=60)]: Done   6 out of  20 | elapsed:    0.0s remaining:    0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Decision Tree Classifier\n",
      "Fitting 10 folds for each of 4 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=60)]: Done  20 out of  20 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=60)]: Done   3 out of  40 | elapsed:    0.0s remaining:    0.3s\n",
      "[Parallel(n_jobs=60)]: Done  40 out of  40 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AdaBoost\n",
      "Training Random Forests\n",
      "Generate results for ith gram Classifiers\n",
      "Building the feature set\n",
      "Parsing tweets from the file: tweets_HawaiianAir.txt\n",
      "Preparing the training and the test set...\n",
      "Done preapring the train and test set...Ready to train classifiers\n",
      "Applying PCA on X\n",
      "Training Logistic Regression Classifiers\n",
      "Fitting 10 folds for each of 2 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/srinivassuri/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:58: DeprecationWarning: generator 'ngrams' raised StopIteration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Gaussian Naive Bayes Classifier\n",
      "Training SVM Lib Linear Classifier\n",
      "Fitting 10 folds for each of 2 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=60)]: Done   6 out of  20 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=60)]: Done  20 out of  20 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=60)]: Done   6 out of  20 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=60)]: Done  20 out of  20 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Decision Tree Classifier\n",
      "Fitting 10 folds for each of 4 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=60)]: Done   3 out of  40 | elapsed:    0.0s remaining:    0.3s\n",
      "[Parallel(n_jobs=60)]: Done  40 out of  40 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AdaBoost\n",
      "Training Random Forests\n",
      "Generate results for ith gram Classifiers\n",
      "Building the feature set\n",
      "Parsing tweets from the file: tweets_HawaiianAir.txt\n",
      "Preparing the training and the test set...\n",
      "Done preapring the train and test set...Ready to train classifiers\n",
      "Applying PCA on X\n",
      "Training Logistic Regression Classifiers\n",
      "Fitting 10 folds for each of 2 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/srinivassuri/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:58: DeprecationWarning: generator 'ngrams' raised StopIteration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Gaussian Naive Bayes Classifier\n",
      "Training SVM Lib Linear Classifier\n",
      "Fitting 10 folds for each of 2 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=60)]: Done   6 out of  20 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=60)]: Done  20 out of  20 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Decision Tree Classifier\n",
      "Fitting 10 folds for each of 4 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=60)]: Done   6 out of  20 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=60)]: Done  20 out of  20 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=60)]: Done   3 out of  40 | elapsed:    0.0s remaining:    0.3s\n",
      "[Parallel(n_jobs=60)]: Done  40 out of  40 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AdaBoost\n",
      "Training Random Forests\n",
      "Results (features): ( 25 ):\n",
      "   logreg_train  logreg_test  nb_train   nb_test  svm_train  svm_test  \\\n",
      "0      0.565217     0.612069  0.449275  0.413793   0.556522  0.594828   \n",
      "1      0.588406     0.543103  0.449275  0.353448   0.571014  0.525862   \n",
      "2      0.507246     0.612069  0.226087  0.189655   0.501449  0.612069   \n",
      "3      0.515942     0.387931  0.185507  0.241379   0.530435  0.387931   \n",
      "4      0.478261     0.508621  0.197101  0.155172   0.481159  0.508621   \n",
      "\n",
      "   dt_train   dt_test  adaboost_train  adaboost_test  rf_train   rf_test  \n",
      "0  0.655072  0.586207        0.698551       0.594828  0.681159  0.612069  \n",
      "1  0.634783  0.517241        0.704348       0.586207  0.681159  0.612069  \n",
      "2  0.530435  0.543103        0.542029       0.620690  0.542029  0.620690  \n",
      "3  0.530435  0.396552        0.530435       0.396552  0.530435  0.396552  \n",
      "4  0.475362  0.508621        0.481159       0.508621  0.481159  0.508621  \n",
      "Generate results for ith gram Classifiers\n",
      "Building the feature set\n",
      "Parsing tweets from the file: tweets_HawaiianAir.txt\n",
      "Preparing the training and the test set...\n",
      "Done preapring the train and test set...Ready to train classifiers\n",
      "Applying PCA on X\n",
      "Training Logistic Regression Classifiers\n",
      "Fitting 10 folds for each of 2 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=60)]: Done   6 out of  20 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=60)]: Done  20 out of  20 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Gaussian Naive Bayes Classifier\n",
      "Training SVM Lib Linear Classifier\n",
      "Fitting 10 folds for each of 2 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=60)]: Done   6 out of  20 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=60)]: Done  20 out of  20 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Decision Tree Classifier\n",
      "Fitting 10 folds for each of 4 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=60)]: Done   3 out of  40 | elapsed:    0.0s remaining:    0.2s\n",
      "[Parallel(n_jobs=60)]: Done  40 out of  40 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AdaBoost\n",
      "Training Random Forests\n",
      "Generate results for ith gram Classifiers\n",
      "Building the feature set\n",
      "Parsing tweets from the file: tweets_HawaiianAir.txt\n",
      "Preparing the training and the test set...\n",
      "Done preapring the train and test set...Ready to train classifiers\n",
      "Applying PCA on X\n",
      "Training Logistic Regression Classifiers\n",
      "Fitting 10 folds for each of 2 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=60)]: Done   6 out of  20 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=60)]: Done  20 out of  20 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Gaussian Naive Bayes Classifier\n",
      "Training SVM Lib Linear Classifier\n",
      "Fitting 10 folds for each of 2 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=60)]: Done   6 out of  20 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=60)]: Done  20 out of  20 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Decision Tree Classifier\n",
      "Fitting 10 folds for each of 4 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=60)]: Done   3 out of  40 | elapsed:    0.0s remaining:    0.3s\n",
      "[Parallel(n_jobs=60)]: Done  40 out of  40 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AdaBoost\n",
      "Training Random Forests\n",
      "Generate results for ith gram Classifiers\n",
      "Building the feature set\n",
      "Parsing tweets from the file: tweets_HawaiianAir.txt\n",
      "Preparing the training and the test set...\n",
      "Done preapring the train and test set...Ready to train classifiers\n",
      "Applying PCA on X\n",
      "Training Logistic Regression Classifiers\n",
      "Fitting 10 folds for each of 2 candidates, totalling 20 fits\n",
      "Training Gaussian Naive Bayes Classifier\n",
      "Training SVM Lib Linear Classifier\n",
      "Fitting 10 folds for each of 2 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=60)]: Done   6 out of  20 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=60)]: Done  20 out of  20 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=60)]: Done   6 out of  20 | elapsed:    0.0s remaining:    0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Decision Tree Classifier\n",
      "Fitting 10 folds for each of 4 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=60)]: Done  20 out of  20 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=60)]: Done   3 out of  40 | elapsed:    0.0s remaining:    0.3s\n",
      "[Parallel(n_jobs=60)]: Done  40 out of  40 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AdaBoost\n",
      "Training Random Forests\n",
      "Generate results for ith gram Classifiers\n",
      "Building the feature set\n",
      "Parsing tweets from the file: tweets_HawaiianAir.txt\n",
      "Preparing the training and the test set...\n",
      "Done preapring the train and test set...Ready to train classifiers\n",
      "Applying PCA on X\n",
      "Training Logistic Regression Classifiers\n",
      "Fitting 10 folds for each of 2 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/srinivassuri/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:58: DeprecationWarning: generator 'ngrams' raised StopIteration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Gaussian Naive Bayes Classifier\n",
      "Training SVM Lib Linear Classifier\n",
      "Fitting 10 folds for each of 2 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=60)]: Done   6 out of  20 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=60)]: Done  20 out of  20 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=60)]: Done   6 out of  20 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=60)]: Done  20 out of  20 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Decision Tree Classifier\n",
      "Fitting 10 folds for each of 4 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=60)]: Done   3 out of  40 | elapsed:    0.0s remaining:    0.3s\n",
      "[Parallel(n_jobs=60)]: Done  40 out of  40 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AdaBoost\n",
      "Training Random Forests\n",
      "Generate results for ith gram Classifiers\n",
      "Building the feature set\n",
      "Parsing tweets from the file: tweets_HawaiianAir.txt\n",
      "Preparing the training and the test set...\n",
      "Done preapring the train and test set...Ready to train classifiers\n",
      "Applying PCA on X\n",
      "Training Logistic Regression Classifiers\n",
      "Fitting 10 folds for each of 2 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/srinivassuri/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:58: DeprecationWarning: generator 'ngrams' raised StopIteration\n",
      "[Parallel(n_jobs=60)]: Done   6 out of  20 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=60)]: Done  20 out of  20 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Gaussian Naive Bayes Classifier\n",
      "Training SVM Lib Linear Classifier\n",
      "Fitting 10 folds for each of 2 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=60)]: Done   6 out of  20 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=60)]: Done  20 out of  20 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Decision Tree Classifier\n",
      "Fitting 10 folds for each of 4 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=60)]: Done   3 out of  40 | elapsed:    0.0s remaining:    0.4s\n",
      "[Parallel(n_jobs=60)]: Done  40 out of  40 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AdaBoost\n",
      "Training Random Forests\n",
      "Results (features): ( 50 ):\n",
      "   logreg_train  logreg_test  nb_train   nb_test  svm_train  svm_test  \\\n",
      "0      0.562319     0.543103  0.440580  0.448276   0.585507  0.586207   \n",
      "1      0.579710     0.620690  0.394203  0.396552   0.588406  0.637931   \n",
      "2      0.536232     0.543103  0.269565  0.258621   0.542029  0.543103   \n",
      "3      0.498551     0.439655  0.197101  0.232759   0.510145  0.439655   \n",
      "4      0.481159     0.500000  0.188406  0.206897   0.489855  0.500000   \n",
      "\n",
      "   dt_train   dt_test  adaboost_train  adaboost_test  rf_train   rf_test  \n",
      "0  0.605797  0.508621        0.776812       0.517241  0.710145  0.586207  \n",
      "1  0.640580  0.525862        0.765217       0.543103  0.715942  0.577586  \n",
      "2  0.513043  0.508621        0.585507       0.603448  0.585507  0.594828  \n",
      "3  0.513043  0.448276        0.513043       0.448276  0.513043  0.448276  \n",
      "4  0.484058  0.508621        0.489855       0.508621  0.489855  0.508621  \n"
     ]
    }
   ],
   "source": [
    "#Generate the Classifier Results\n",
    "#files = ['tweets_Allegiant.txt','tweets_AlaskaAir.txt','tweets_Delta.txt','tweets_AmericanAir.txt','tweets_FlyFrontier.txt','tweets_HawaiianAir.txt']  \n",
    "files = ['tweets_HawaiianAir.txt']\n",
    "noOfFeatures = [25,50]\n",
    "\n",
    "headers=['logreg_train','logreg_test','nb_train','nb_test','svm_train','svm_test','dt_train','dt_test','adaboost_train','adaboost_test'\n",
    "        ,'rf_train','rf_test']\n",
    "results=[]\n",
    "\n",
    "for featureLen in noOfFeatures:\n",
    "    results = []\n",
    "    for i in range(5):\n",
    "        print( 'Generate results for ', i, '-gram Classifiers')\n",
    "        clf = Classifiers(i,'stopwords.txt',files,0.75,featureLen)\n",
    "        clf.runClassifiers()\n",
    "        results.append( clf.getResults() )\n",
    "    \n",
    "    print( 'Results (features): (',featureLen,'):' )\n",
    "    df = pd.DataFrame(results, columns=headers)\n",
    "    print( df)\n",
    "    \n",
    "print ( 'Results After Performing PCA ')\n",
    "for featureLen in noOfFeatures:\n",
    "    results = []\n",
    "    for i in range(5):\n",
    "        print( 'Generate results for ith gram Classifiers')\n",
    "        clf = Classifiers(i,'stopwords.txt',files,0.75,featureLen)\n",
    "        clf.applyPCA(20) #Keeping the top 20 important features\n",
    "        clf.runClassifiers()\n",
    "        results.append( clf.getResults() )\n",
    "    \n",
    "    print( 'Results (features): (',featureLen,'):' )\n",
    "    df = pd.DataFrame(results, columns=headers)\n",
    "    print( df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Visualize:\n",
    "    def __init__(self,debug,removeWordsList=[]):\n",
    "        self.debug = debug\n",
    "        \n",
    "        self.jsonParser = JsonParser()\n",
    "        self.tweetCleaner = TweetCleaner('stopwords.txt')\n",
    "        self.removeWordsList = removeWordsList #List of keywords to remove for the wordcount. ex: AT_USER\n",
    "        \n",
    "        #map[word]->count for the word cloud\n",
    "        self.poscount = {} \n",
    "        self.negcount = {}  \n",
    "        self.neutralcount = {} \n",
    "\n",
    "        #Location of tweets\n",
    "        self.posloc = {}\n",
    "        self.negloc = {}\n",
    "        self.neutloc = {}\n",
    "        \n",
    "    \n",
    "    def log(self,msg):\n",
    "        if ( self.debug ):\n",
    "            print (msg)\n",
    "    \n",
    "    def clearCount(self,mode):\n",
    "        if (mode == 0 ):\n",
    "            self.negcount = {}  \n",
    "        elif(mode == 1):\n",
    "            self.neutralcount = {} \n",
    "        else:\n",
    "            self.poscount = {} \n",
    "            \n",
    "    def clearLocations(self,mode):\n",
    "        if (mode == 0 ):\n",
    "            self.negloc = {}  \n",
    "        elif(mode == 1):\n",
    "            self.neutloc = {} \n",
    "        else:\n",
    "            self.posloc = {} \n",
    "    \n",
    "    #Function to return the top n words \n",
    "    def getTopNWords(self,n,mode):# mode: 0 | 1 | 2 \n",
    "        if( mode == 0):\n",
    "            items = sorted(self.negcount.items(), key=lambda x: x[1],reverse = True)\n",
    "        elif( mode == 1 ):\n",
    "            items = sorted(self.neutralcount.items(), key=lambda x: x[1],reverse = True)\n",
    "        elif( mode == 2 ):\n",
    "            items = sorted(self.poscount.items(), key=lambda x: x[1],reverse = True)\n",
    "        items = items[0:n]\n",
    "        result = [i for i in items if i[0] not in self.removeWordsList]\n",
    "        return result\n",
    "    \n",
    "    #Function to return the top n Locations\n",
    "    def getTopNLocations(self,n,mode):# mode: 0 | 1 | 2 \n",
    "        if( mode == 0):\n",
    "            items = sorted(self.negloc.items(), key=lambda x: x[1],reverse = True)\n",
    "        elif( mode == 1 ):\n",
    "            items = sorted(self.neutloc.items(), key=lambda x: x[1],reverse = True)\n",
    "        elif( mode == 2 ):\n",
    "            items = sorted(self.posloc.items(), key=lambda x: x[1],reverse = True)\n",
    "        items = items[0:n]\n",
    "        result = [i for i in items if i[0] not in self.removeWordsList]\n",
    "        return result\n",
    "    \n",
    "    def writeToCSV(self,fname,n,mode):\n",
    "        if(mode == 0):\n",
    "            fname=fname+'_neg' +'.csv'\n",
    "        elif ( mode == 1):\n",
    "            fname=fname+'_neut'+'.csv'\n",
    "        else:\n",
    "            fname=fname+'_pos'+'.csv'\n",
    "            \n",
    "        with open(fname,'w') as out:\n",
    "            csv_out=csv.writer(out)\n",
    "            csv_out.writerow(['word','count'])\n",
    "            rows = self.getTopN(n,mode)\n",
    "            for row in rows:\n",
    "                csv_out.writerow(row)\n",
    "    \n",
    "    def populateCountList(self,text,sentiment):\n",
    "            #Populate word count\n",
    "            if ( sentiment == 2):\n",
    "                for  word in text.split(' '):\n",
    "                    if ( word not in self.poscount ):\n",
    "                        self.poscount[word] = 1\n",
    "                    else:\n",
    "                        self.poscount[word] = self.poscount[word] + 1\n",
    "            elif ( sentiment == 1 ):\n",
    "                for  word in text.split(' '):\n",
    "                    if ( word not in self.neutralcount ):\n",
    "                        self.neutralcount[word] = 1\n",
    "                    else:\n",
    "                        self.neutralcount[word] = self.neutralcount[word] + 1\n",
    "            else:\n",
    "                for  word in text.split(' '):\n",
    "                    if ( word not in self.negcount ):\n",
    "                        self.negcount[word] = 1\n",
    "                    else:\n",
    "                        self.negcount[word] = self.negcount[word] + 1\n",
    "                        \n",
    "    def populateLocationList(self,location,sentiment):\n",
    "            #Populate Location count\n",
    "            if ( sentiment == 2):\n",
    "                    if ( location not in self.posloc ):\n",
    "                        self.posloc[location] = 1\n",
    "                    else:\n",
    "                        self.posloc[location] = self.posloc[location] + 1\n",
    "            elif ( sentiment == 1 ):\n",
    "                    if ( location not in self.neutloc ):\n",
    "                        self.neutloc[location] = 1\n",
    "                    else:\n",
    "                        self.neutloc[location] = self.neutloc[location] + 1\n",
    "            else:\n",
    "                    if ( location not in self.negloc ):\n",
    "                        self.negloc[location] = 1\n",
    "                    else:\n",
    "                        self.negloc[location] = self.negloc[location] + 1\n",
    "\n",
    "             \n",
    "    def setWordCount(self,fname):\n",
    "        self.log('File name is,'+fname)\n",
    "        tweets =  jsonParser.loadData(fname)\n",
    "        \n",
    "        self.log('Reading all the tweets')\n",
    "        \n",
    "        for tweet in tweets:\n",
    "            #1.Skip for retweets\n",
    "            if ( tweet['text'][0].lower() == 'r' and  tweet['text'][1].lower() == 't' ):\n",
    "                continue\n",
    "                \n",
    "            text = self.tweetCleaner.cleanTweet( tweet['text'] )\n",
    "            sentiment = self.tweetCleaner.getSentiment( tweet['text'] )\n",
    "            self.populateCountList(text,sentiment)\n",
    "            self.populateLocationList(tweet['user']['location'],sentiment)\n",
    "\n",
    "                    \n",
    "        self.log('Word count is set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removeWordsList = ['AT_USER','HASH_TAG','...',',','’','rt','','…','flight','?','!','can','t','s','m']\n",
    "visualize = Visualize(False,removeWordsList)\n",
    "visualize.setWordCount('tweets_JetBlue.txt')\n",
    "\n",
    "print ( 'negative', visualize.getTopNWords(30,0) )\n",
    "print ('')\n",
    "print ( 'neutral',visualize.getTopNWords(30,1) )\n",
    "print ('')\n",
    "print ( 'positive',visualize.getTopNWords(30,2) )\n",
    "print ('')\n",
    "print ( 'negative locations', visualize.getTopNLocations(30,0) )\n",
    "print ('')\n",
    "print ( 'neutral locations',visualize.getTopNLocations(30,1) )\n",
    "print ('')\n",
    "print ( 'positive locations',visualize.getTopNLocations(30,2) )\n",
    "\n",
    "\n",
    "#Write Data to a sav file for visualisation in tableau\n",
    "\n",
    "#visualize.writeToCSV('wc_AmericanAir',200,0)\n",
    "#visualize.writeToCSV('wc_AmericanAir',200,1)\n",
    "#visualize.writeToCSV('wc_AmericanAir',200,2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
