{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tweepy in ./miniconda3/lib/python3.6/site-packages\n",
      "Requirement already satisfied: requests>=2.11.1 in ./miniconda3/lib/python3.6/site-packages (from tweepy)\n",
      "Requirement already satisfied: six>=1.10.0 in ./miniconda3/lib/python3.6/site-packages (from tweepy)\n",
      "Requirement already satisfied: PySocks>=1.5.7 in ./miniconda3/lib/python3.6/site-packages (from tweepy)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in ./miniconda3/lib/python3.6/site-packages (from tweepy)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in ./miniconda3/lib/python3.6/site-packages (from requests>=2.11.1->tweepy)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in ./miniconda3/lib/python3.6/site-packages (from requests>=2.11.1->tweepy)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in ./miniconda3/lib/python3.6/site-packages (from requests>=2.11.1->tweepy)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./miniconda3/lib/python3.6/site-packages (from requests>=2.11.1->tweepy)\n",
      "Requirement already satisfied: oauthlib>=0.6.2 in ./miniconda3/lib/python3.6/site-packages (from requests-oauthlib>=0.7.0->tweepy)\n",
      "Requirement already satisfied: textblob in ./miniconda3/lib/python3.6/site-packages\n",
      "Requirement already satisfied: nltk>=3.1 in ./miniconda3/lib/python3.6/site-packages (from textblob)\n",
      "Requirement already satisfied: six in ./miniconda3/lib/python3.6/site-packages (from nltk>=3.1->textblob)\n",
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     /Users/srinivassuri/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/srinivassuri/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/srinivassuri/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/srinivassuri/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package conll2000 to\n",
      "[nltk_data]     /Users/srinivassuri/nltk_data...\n",
      "[nltk_data]   Package conll2000 is already up-to-date!\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /Users/srinivassuri/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "Finished.\n",
      "Requirement already satisfied: nltk in ./miniconda3/lib/python3.6/site-packages\n",
      "Requirement already satisfied: six in ./miniconda3/lib/python3.6/site-packages (from nltk)\n",
      "Requirement already satisfied: numpy in ./miniconda3/lib/python3.6/site-packages\n"
     ]
    }
   ],
   "source": [
    "!pip install tweepy\n",
    "!pip install textblob\n",
    "!python -m textblob.download_corpora\n",
    "!pip install nltk\n",
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import re\n",
    "import tweepy\n",
    "import unicodedata\n",
    "import json\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class Definiton for the keywords that will be used to fetch the tweets\n",
    "class keywords:\n",
    "    def __init__(self):\n",
    "        self.keywords = ['AlaskaAir','Allegiant','AmericanAir','Delta','FlyFrontier','HawaiianAir']#'United Airlines','Virgin America']\n",
    "        \n",
    "    def getKeyWords(self):\n",
    "        return self.keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class Definition to pull the twitter tweets\n",
    "class pullData:      \n",
    "    def __init__(self,key,secret,maxTweets,tweetsPerQry,keywords):\n",
    "        self.key = key   \n",
    "        self.secret = secret\n",
    "        self.maxTweets = maxTweets\n",
    "        self.tweetsPerQry = tweetsPerQry          \n",
    "        self.keywords = keywords\n",
    "        self.api = ''\n",
    "        self.auth = ''\n",
    "        \n",
    "    def printParams(self):\n",
    "        print('Parameters set to...')\n",
    "        print('key...',self.key)\n",
    "        print('secret...',self.secret)\n",
    "        print('maxTweets...',self.maxTweets)\n",
    "        print('tweetsPerQry...',self.tweetsPerQry)\n",
    "        print('keywords...',self.keywords)\n",
    "        \n",
    "    def connect(self):\n",
    "        self.auth = tweepy.AppAuthHandler(self.key,self.secret )  \n",
    "        self.api = tweepy.API(self.auth, wait_on_rate_limit=True,wait_on_rate_limit_notify=True)\n",
    "        return True\n",
    "        \n",
    "        if (not self.api):\n",
    "            print (\"Can't Authenticate\")\n",
    "            return False\n",
    "        \n",
    "    def downloadData(self):\n",
    "        for word in self.keywords:\n",
    "            print( \"Downloading Tweets for the keyword: \", word )\n",
    "            fName = 'tweets_' + word + '.txt'\n",
    "            sinceId = None\n",
    "            max_id = -1\n",
    "            tweetCount = 0\n",
    "            tweet_dict = []\n",
    "    \n",
    "            print(\"Downloading max {0} tweets\".format(maxTweets))\n",
    "\n",
    "            while tweetCount < self.maxTweets:\n",
    "                try:\n",
    "                    if (max_id <= 0):\n",
    "                        if (not sinceId):   \n",
    "                            new_tweets = self.api.search(q=word, count=self.tweetsPerQry)\n",
    "                        else:\n",
    "                            new_tweets = self.api.search(q=word, count=self.tweetsPerQry,since_id=sinceId)\n",
    "                    else:\n",
    "                        if (not sinceId):\n",
    "                            new_tweets = self.api.search(q=word, count=self.tweetsPerQry,\n",
    "                                    max_id=str(max_id - 1))\n",
    "                        else:\n",
    "                            new_tweets = self.api.search(q=word, count=self.tweetsPerQry,\n",
    "                                    max_id=str(max_id - 1),since_id=sinceId)\n",
    "                    if not new_tweets:\n",
    "                        print(\"No more tweets found\")\n",
    "                        break            \n",
    "                        \n",
    "                    for tweet in new_tweets:\n",
    "                        tweet_dict.append(tweet._json)\n",
    " \n",
    "                    tweetCount += len(new_tweets)\n",
    "                    print(\"Downloaded {0} tweets\".format(tweetCount))\n",
    "                    max_id = new_tweets[-1].id\n",
    "                \n",
    "                except tweepy.TweepError as e:\n",
    "                    # Just exit if any error\n",
    "                    print(\"some error : \" + str(e))\n",
    "                    break\n",
    "            \n",
    "            with open(fName, 'w') as f:   \n",
    "                json.dump(tweet_dict, f, ensure_ascii=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters set to...\n",
      "key... bppIV0LkQUIARDug4b8Sij8pm\n",
      "secret... 5ofpZtxL1CsDmfbF93Qh0EcPOHUVP3ZSF6CGhbkk2ki2fFWNFL\n",
      "maxTweets... 30000\n",
      "tweetsPerQry... 100\n",
      "keywords... ['AlaskaAir', 'Allegiant', 'AmericanAir', 'Delta', 'FlyFrontier', 'HawaiianAir']\n",
      "Downloading Tweets for the keyword:  AlaskaAir\n",
      "Downloading max 30000 tweets\n",
      "Downloaded 100 tweets\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "consumer_key = 'bppIV0LkQUIARDug4b8Sij8pm'\n",
    "consumer_secret = '5ofpZtxL1CsDmfbF93Qh0EcPOHUVP3ZSF6CGhbkk2ki2fFWNFL'\n",
    "maxTweets = 30000 # Some arbitrary large number\n",
    "tweetsPerQry = 100  # this is the max the API permits\n",
    "\n",
    "keys = keywords()\n",
    "words = keys.getKeyWords()  # this is what we're searching for\n",
    "\n",
    "pulldata = pullData(consumer_key,consumer_secret,maxTweets,tweetsPerQry,words)\n",
    "\n",
    "if ( pulldata.connect() == False ):\n",
    "    print ( \"Connecting to the twitter API Failed\")\n",
    "    sys.exit(1)\n",
    "pulldata.printParams()\n",
    "    \n",
    "pulldata.downloadData()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class definition for JsonParser\n",
    "\n",
    "class JsonParser:\n",
    "    def loadData(self,fname):\n",
    "        with open(fname) as json_data:\n",
    "            d = json.load(json_data)\n",
    "        return d\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class definition to Clean the tweets and get the sentiment\n",
    "class TweetCleaner: \n",
    "    def __init__(self,stopwords_fname):\n",
    "        self.stopwords_fname = stopwords_fname\n",
    "        self.emoji_pattern = re.compile(\n",
    "            u\"(\\ud83d[\\ude00-\\ude4f])|\"  # emoticons\n",
    "            u\"(\\ud83c[\\udf00-\\uffff])|\"  # symbols & pictographs (1 of 2)\n",
    "            u\"(\\ud83d[\\u0000-\\uddff])|\"  # symbols & pictographs (2 of 2)\n",
    "            u\"(\\ud83d[\\ude80-\\udeff])|\"  # transport & map symbols\n",
    "            u\"(\\ud83c[\\udde0-\\uddff])\"  # flags (iOS)\n",
    "            u\"+\", flags=re.UNICODE)\n",
    "        \n",
    "        #Populate the Stop Words\n",
    "        self.stopwords = set()\n",
    "        self.populateStopWords(self.stopwords_fname)\n",
    "        \n",
    "        #Initialize nltk classes\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.tokenizer = TweetTokenizer()\n",
    "    \n",
    "    #---------Create the Stop words Set---------#\n",
    "    def populateStopWords(self,fname):\n",
    "        stop_file = open(fname)\n",
    "        \n",
    "        for line in stop_file:\n",
    "            self.stopwords.add(line.strip())\n",
    "        \n",
    "    #---------Clean the tweets---------#\n",
    "    def cleanTweet(self,tweet):\n",
    "        tweet = tweet.lower()\n",
    "        \n",
    "        #1. remove emojis\n",
    "        tweet = self.emoji_pattern.sub(r'', tweet)\n",
    "        tweet_list = self.tokenizer.tokenize(tweet)\n",
    "        \n",
    "        #2. remove the stop words\n",
    "        words_filtered = []\n",
    "        for word in tweet_list:\n",
    "            if (word not in self.stopwords ):\n",
    "                words_filtered.append( word )\n",
    "              \n",
    "        #3. Stem the words\n",
    "        words_stemmed = [self.stemmer.stem(word) for word in words_filtered]\n",
    "        \n",
    "        _tweet = \"\"\n",
    "        for word in words_stemmed:\n",
    "            _tweet += \" \" + word\n",
    "        \n",
    "        #4. return the fitered tweet\n",
    "        return _tweet\n",
    "             \n",
    "    #--------- negative: 0 , neutral : 1 , positive : 2 ---------#\n",
    "    def getSentiment(self,tweet):\n",
    "        return 2 \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleans all the tweets , builds the tweet set and adds the sentiment to the tweets\n",
    "\n",
    "class BuildFeatureSet: \n",
    "    \n",
    "    def __init__(self,n,stopwords_fname,max_features_count=10000): # n : ngram for the tweets\n",
    "        self.ngrams = n\n",
    "\n",
    "        #Lexicon related variables\n",
    "        self.lexicon = {}\n",
    "        self.inverse_lexicon = { }\n",
    "        self.ngram_count = 0\n",
    "        self.max_features_count = max_features_count\n",
    "        self.tweetCleaner = TweetCleaner(stopwords_fname)\n",
    "        \n",
    "        #Training Dataset\n",
    "        self.data = {}\n",
    "\n",
    "    def addToLexicon(self,words):\n",
    "        for word in words:\n",
    "            if ( word not in self.lexicon and ( self.ngram_count <  self.max_features_count ) ):\n",
    "                self.lexicon[word] = self.ngram_count #Assign a unique number for the word seen\n",
    "                self.inverse_lexicon[self.ngram_count] = word\n",
    "                self.ngram_count = self.ngram_count + 1\n",
    "                #print( 'Lexicon: ',self.ngram_count,word)\n",
    "    \n",
    "    def isTweetReply(self,tweet):\n",
    "        if ( tweet[0].lower() == 'r' and  tweet[1].lower() == 't' ):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    #Add the tweet to the lexicon set\n",
    "    def addTweet(self,tweet):\n",
    "        #1. get the sentiment for the tweet\n",
    "        sentiment = self.tweetCleaner.getSentiment(tweet)\n",
    "        \n",
    "        #2. Clean the Tweet\n",
    "        tweet = self.tweetCleaner.cleanTweet(tweet)\n",
    "        \n",
    "        #3. get the ngrams for the tweet\n",
    "        _ngrams = nltk.ngrams(tweet.split(), self.ngrams)\n",
    "            \n",
    "        #4. Add the ngrams to the lexicon dictionary\n",
    "        words = list(_ngrams)\n",
    "        self.addToLexicon( words )\n",
    "        \n",
    "        #5. Add this tweet row to the training set\n",
    "        self.addToTrainingData(words,sentiment)\n",
    "    \n",
    "    #Build the Feature set for all the tweets\n",
    "    def addToTrainingData(self,ngrams,sentiment):\n",
    "        row = np.zeros(self.max_features_count + 1 ) # last feature is the label \n",
    "        \n",
    "        for word in ngrams:\n",
    "            if ( word in self.lexicon ):\n",
    "                row[ self.lexicon[word] ] = row[ self.lexicon[word] ] + 1 #Increase the count of the word \n",
    "\n",
    "        row [ self.max_features_count ] = sentiment\n",
    "        \n",
    "        self.data[ len(self.data)  ]  = row \n",
    "    \n",
    "    \n",
    "    def getFeatures(self):\n",
    "        features = np.zeros(( len(self.data), self.max_features_count ))\n",
    "        \n",
    "        for index in self.data:\n",
    "            features[index][:] =  self.data[index][:-1]\n",
    "            \n",
    "        return features\n",
    "    \n",
    "    def getLabels(self):\n",
    "        labels = np.zeros( len(self.data) )\n",
    "        \n",
    "        for index in self.data:\n",
    "            labels[index] = ( int(self.data[index][self.max_features_count]) )\n",
    "            \n",
    "        return labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parse the Tweets\n",
    "jsonParser = JsonParser()\n",
    "features = BuildFeatureSet(1,'stopwords.txt') \n",
    "\n",
    "files = ['tweets_Allegiant.txt','tweets_AlaskaAir.txt','tweets_Delta.txt','tweets_AmericanAir.txt','tweets_FlyFrontier.txt','tweets_HawaiianAir.txt'] \n",
    "\n",
    "\n",
    "for file in files:\n",
    "    print ('Building Feature set for the file:',file)\n",
    "    tweets =  jsonParser.loadData(file)\n",
    "\n",
    "    for tweet in tweets:\n",
    "        #1. Skip if the tweet the is a reply to an existing tweet\n",
    "        if ( buildFeatures.isTweetReply(tweet['text']) == False ):\n",
    "            features.addTweet ( tweet['text']  ) \n",
    "\n",
    "print ( features.getFeatures().shape )\n",
    "\n",
    "print ( features.getLabels().shape )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
