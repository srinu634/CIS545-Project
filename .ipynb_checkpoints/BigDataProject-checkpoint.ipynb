{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tweepy in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages\n",
      "Requirement already satisfied: six>=1.10.0 in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages (from tweepy)\n",
      "Requirement already satisfied: requests>=2.11.1 in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages (from tweepy)\n",
      "Requirement already satisfied: PySocks>=1.5.7 in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages (from tweepy)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages (from tweepy)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages (from requests>=2.11.1->tweepy)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages (from requests>=2.11.1->tweepy)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages (from requests>=2.11.1->tweepy)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages (from requests>=2.11.1->tweepy)\n",
      "Requirement already satisfied: oauthlib>=0.6.2 in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages (from requests-oauthlib>=0.7.0->tweepy)\n",
      "\u001b[33mYou are using pip version 9.0.3, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: textblob in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages\n",
      "Requirement already satisfied: nltk>=3.1 in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages (from textblob)\n",
      "Requirement already satisfied: six in /Users/srinivassuri/miniconda3/lib/python3.6/site-packages (from nltk>=3.1->textblob)\n",
      "\u001b[33mYou are using pip version 9.0.3, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tweepy\n",
    "!pip install textblob\n",
    "!python -m textblob.download_corpora\n",
    "!pip install nltk\n",
    "!pip install numpy\n",
    "!pip install sklearn\n",
    "!pip install scipy\n",
    "!pip install wordcloud\n",
    "!pip install keras\n",
    "!pip install tensorflow\n",
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import re\n",
    "import csv\n",
    "import tweepy\n",
    "import unicodedata\n",
    "import json\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.misc import imread\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM, Convolution1D, Flatten, Dropout, LSTM, MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "from textblob import TextBlob\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "#download this file from - https://nlp.stanford.edu/projects/glove/\n",
    "glove_file = \"glove.6B.300d.txt\"\n",
    "tmp_file = \"glove_w2vec.txt\"\n",
    "#remove these comments when you want word embeddings\n",
    "glove2word2vec(glove_file, tmp_file)\n",
    "vec = KeyedVectors.load_word2vec_format(tmp_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class Definiton for the keywords that will be used to fetch the tweets\n",
    "class keywords:\n",
    "    def __init__(self):\n",
    "        self.keywords = ['AlaskaAir','Allegiant','AmericanAir','Delta','FlyFrontier','HawaiianAir','@united','JetBlue','SouthwestAir','SpiritAirlines','VirginAmerica','SunCountryAir']\n",
    "        \n",
    "    def getKeyWords(self):\n",
    "        return self.keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class Definition to pull the twitter tweets\n",
    "class pullData:      \n",
    "    def __init__(self,key,secret,maxTweets,tweetsPerQry,keywords):\n",
    "        self.key = key   \n",
    "        self.secret = secret\n",
    "        self.maxTweets = maxTweets\n",
    "        self.tweetsPerQry = tweetsPerQry          \n",
    "        self.keywords = keywords\n",
    "        self.api = ''\n",
    "        self.auth = ''\n",
    "        \n",
    "    def printParams(self):\n",
    "        print('Parameters set to...')\n",
    "        print('key...',self.key)\n",
    "        print('secret...',self.secret)\n",
    "        print('maxTweets...',self.maxTweets)\n",
    "        print('tweetsPerQry...',self.tweetsPerQry)\n",
    "        print('keywords...',self.keywords)\n",
    "        \n",
    "    def connect(self):\n",
    "        self.auth = tweepy.AppAuthHandler(self.key,self.secret )  \n",
    "        self.api = tweepy.API(self.auth, wait_on_rate_limit=True,wait_on_rate_limit_notify=True)\n",
    "        return True\n",
    "        \n",
    "        if (not self.api):\n",
    "            print (\"Can't Authenticate\")\n",
    "            return False\n",
    "        \n",
    "    def downloadData(self):\n",
    "        for word in self.keywords:\n",
    "            print( \"Downloading Tweets for the keyword: \", word )\n",
    "            fName = 'tweets_' + word + '.txt'\n",
    "            sinceId = None\n",
    "            max_id = -1\n",
    "            tweetCount = 0\n",
    "            tweet_dict = []\n",
    "    \n",
    "            print(\"Downloading max {0} tweets\".format(maxTweets))\n",
    "\n",
    "            while tweetCount < self.maxTweets:\n",
    "                try:\n",
    "                    if (max_id <= 0):\n",
    "                        if (not sinceId):   \n",
    "                            new_tweets = self.api.search(q=word, count=self.tweetsPerQry)\n",
    "                        else:\n",
    "                            new_tweets = self.api.search(q=word, count=self.tweetsPerQry,since_id=sinceId)\n",
    "                    else:\n",
    "                        if (not sinceId):\n",
    "                            new_tweets = self.api.search(q=word, count=self.tweetsPerQry,\n",
    "                                    max_id=str(max_id - 1))\n",
    "                        else:\n",
    "                            new_tweets = self.api.search(q=word, count=self.tweetsPerQry,\n",
    "                                    max_id=str(max_id - 1),since_id=sinceId)\n",
    "                    if not new_tweets:\n",
    "                        print(\"No more tweets found\")\n",
    "                        break            \n",
    "                        \n",
    "                    for tweet in new_tweets:\n",
    "                        tweet_dict.append(tweet._json)\n",
    " \n",
    "                    tweetCount += len(new_tweets)\n",
    "                    print(\"Downloaded {0} tweets\".format(tweetCount))\n",
    "                    max_id = new_tweets[-1].id\n",
    "                \n",
    "                except tweepy.TweepError as e:\n",
    "                    # Just exit if any error\n",
    "                    print(\"some error : \" + str(e))\n",
    "                    break\n",
    "            \n",
    "            with open(fName, 'w', encoding='utf8', errors='replace') as f:   \n",
    "                json.dump(tweet_dict, f, ensure_ascii=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class definition for JsonParser\n",
    "\n",
    "class JsonParser:\n",
    "    def loadData(self,fname):\n",
    "        with open(fname, encoding='utf8', errors='replace') as json_data:\n",
    "            d = json.load(json_data)\n",
    "        return d\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class definition to Clean the tweets and get the sentiment\n",
    "class TweetCleaner: \n",
    "    def __init__(self,stopwords_fname):\n",
    "        self.stopwords_fname = stopwords_fname\n",
    "        self.negation_cues = self.get_negation_cues('negation_cues.txt')\n",
    "        self.sentiment_fnames = [\"EffectWordNet.tff\", \"subjclueslen1-HLTEMNLP05.tff\"]\n",
    "        self.emoji_pattern = re.compile(r'[\\U00010000-\\U0010ffff]', flags=re.UNICODE)\n",
    "        self.url_pattern = re.compile(r'https?:\\/\\/.*\\b')\n",
    "        self.handle_pattern = re.compile(r'@\\w+')\n",
    "        self.hashtag_pattern = re.compile(r'#\\w+')\n",
    "        \n",
    "        #Populate the Stop Words\n",
    "        self.stopwords = set()\n",
    "        self.populateStopWords(self.stopwords_fname)\n",
    "        \n",
    "        #populate the wordtoEffectMap\n",
    "        self.wordToEffectMap = {}\n",
    "        self.buildWordToEffectMap(self.sentiment_fnames)\n",
    "        \n",
    "        #Initialize nltk classes\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.tokenizer = TweetTokenizer()\n",
    "    \n",
    "    #---------Create the Stop words Set---------#\n",
    "    def populateStopWords(self,fname):\n",
    "        stop_file = open(fname)\n",
    "        \n",
    "        for line in stop_file:\n",
    "            self.stopwords.add(line.strip())\n",
    "    \n",
    "    def get_negation_cues(self, fname):\n",
    "        cues_fname = open(fname, 'r')\n",
    "        neg = []\n",
    "        for line in cues_fname:\n",
    "            if not line:\n",
    "                continue\n",
    "            line = line.strip()\n",
    "            neg.append(line)\n",
    "        return neg\n",
    "    \n",
    "    #---------Build the word to +/- Effect Map----------#\n",
    "    def buildWordToEffectMap(self, sentiment_files):\n",
    "        #read sentiment files into dictionary of words and positive or negative sentiment\n",
    "        for file in sentiment_files:\n",
    "            with open(file, \"r\", encoding='utf8', errors='replace') as f:\n",
    "                if(file == \"EffectWordNet.tff\"):\n",
    "                    for line in f:\n",
    "                        #02279615\t+Effect\tprofiteer\t make an unreasonable profit, as on the sale of difficult to obtain goods \n",
    "                        words = line.split('\\t')\n",
    "                        effect = words[1]\n",
    "                        effect_val = 0\n",
    "                        if '+' in effect:\n",
    "                            effect_val = 1\n",
    "                        elif '-' in effect:\n",
    "                            effect_val = -1\n",
    "                        else:\n",
    "                            effect_val = 0\n",
    "                        list_of_words = []\n",
    "                        if (',' in words[2]):\n",
    "                            list_of_words = words[2].split(',')\n",
    "                        else:\n",
    "                            list_of_words.append(words[2])\n",
    "                        for word in list_of_words:\n",
    "                            self.wordToEffectMap[word] = effect_val\n",
    "                elif(file == \"subjclueslen1-HLTEMNLP05.tff\"):\n",
    "                    for line in f:\n",
    "                        #type=weaksubj len=1 word1=abandoned pos1=adj stemmed1=n priorpolarity=negative\n",
    "                        line = line.strip()\n",
    "                        if not line:\n",
    "                            continue\n",
    "                        words = line.split(' ')\n",
    "                        effect_val = words[5]\n",
    "                        #print(effect_val)\n",
    "                        word_val = words[2]\n",
    "                        word = word_val.split(\"=\")[1]\n",
    "                        effect = effect_val.split(\"=\")[1]\n",
    "                        eff = 0\n",
    "                        if (effect == \"positive\"):\n",
    "                            eff = 1\n",
    "                        elif (effect == \"negative\"):\n",
    "                            eff = -1\n",
    "                        elif (effect == \"neutral\"):\n",
    "                            eff = 0\n",
    "                        self.wordToEffectMap[word] = eff\n",
    "        \n",
    "    #---------Clean the tweets---------#\n",
    "    def cleanTweet(self,tweet):\n",
    "        tweet = tweet.lower()\n",
    "        \n",
    "        #1. remove emojis\n",
    "        tweet = re.sub(self.emoji_pattern, r'', tweet)\n",
    "        \n",
    "        #remove URLS\n",
    "        tweet = re.sub(self.url_pattern, r'', tweet)\n",
    "\n",
    "        #convert @mentions to AT_USER\n",
    "        tweet = re.sub(self.handle_pattern, r'AT_USER', tweet)\n",
    "        \n",
    "        #convert #tags to HASH_TAG\n",
    "        tweet = re.sub(self.hashtag_pattern, r'HASH_TAG', tweet)\n",
    "        \n",
    "        tweet_list = self.tokenizer.tokenize(tweet)\n",
    "        \n",
    "        #2. remove the stop words\n",
    "        words_filtered = []\n",
    "        for word in tweet_list:\n",
    "            if (word not in self.stopwords ):\n",
    "                words_filtered.append( word )\n",
    "              \n",
    "        #3. Stem the words\n",
    "        #words_stemmed = [self.stemmer.stem(word) for word in words_filtered]\n",
    "        \n",
    "        _tweet = \"\"\n",
    "        for word in words_filtered:\n",
    "            _tweet += \" \" + word\n",
    "        \n",
    "        #4. return the fitered tweet\n",
    "        return _tweet\n",
    "             \n",
    "    #--------- negative: 0 , neutral : 1 , positive : 2 ---------#\n",
    "    def getSentiment(self,tweet):\n",
    "        score = 0\n",
    "        tweet = tweet.lower()\n",
    "        tweet_list = self.tokenizer.tokenize(tweet)\n",
    "        for word in tweet_list:\n",
    "            if word in self.wordToEffectMap:\n",
    "                score += self.wordToEffectMap[word]\n",
    "        if (score < 0):\n",
    "            return 0\n",
    "        elif (score > 0):\n",
    "            return 2\n",
    "        else:\n",
    "            return 1\n",
    "        \n",
    "    def getSentimentWithTextBlob(self, tweet):\n",
    "        tweet = tweet.lower()\n",
    "        tweetBlob = TextBlob(tweet)\n",
    "        if (tweetBlob.sentiment.polarity > 0 ):\n",
    "            return 2\n",
    "        elif ( tweetBlob.sentiment.polarity < 0):\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleans all the tweets , builds the tweet set and adds the sentiment to the tweets\n",
    "class BuildFeatureSet: \n",
    "    \n",
    "    def __init__(self,n,stopwords_fname,max_features_count=5000): # n : ngram for the tweets\n",
    "        self.ngrams = n\n",
    "\n",
    "        #Lexicon related variables\n",
    "        self.lexicon = {}\n",
    "        self.inverse_lexicon = { }\n",
    "        self.ngram_count = 0\n",
    "        self.max_features_count = max_features_count\n",
    "        self.tweetCleaner = TweetCleaner(stopwords_fname)\n",
    "        self.tweet_count = 0\n",
    "        self.tweet_map = {}\n",
    "        \n",
    "        #Training Dataset\n",
    "        self.data = {}\n",
    "\n",
    "    def addToLexicon(self,words):\n",
    "        for word in words:\n",
    "            if ( word not in self.lexicon and ( self.ngram_count <  self.max_features_count ) ):\n",
    "                self.lexicon[word] = self.ngram_count #Assign a unique number for the word seen\n",
    "                self.inverse_lexicon[self.ngram_count] = word\n",
    "                self.ngram_count = self.ngram_count + 1\n",
    "                #print( 'Lexicon: ',self.ngram_count,word)\n",
    "    \n",
    "    def isTweetReply(self,tweet):\n",
    "        if ( tweet[0].lower() == 'r' and  tweet[1].lower() == 't' ):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def get_tweet_map(self):\n",
    "        return self.tweet_map\n",
    "        \n",
    "    def isTweetFromAirline(self, tweet, airline_handle):\n",
    "        if( tweet['user']['screen_name'] == airline_handle):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    #Add the tweet to the lexicon set\n",
    "    def addTweet(self,tweet, wordvec=True):\n",
    "        \n",
    "        self.tweet_count += 1\n",
    "        self.tweet_map[self.tweet_count] = tweet\n",
    "        \n",
    "        #1. get the sentiment for the tweet\n",
    "        sentiment = self.tweetCleaner.getSentiment(tweet)\n",
    "        \n",
    "        #2. Clean the Tweet\n",
    "        tweet = self.tweetCleaner.cleanTweet(tweet)\n",
    "        \n",
    "        #3. get the ngrams for the tweet\n",
    "        _ngrams = nltk.ngrams(tweet.split(), self.ngrams)\n",
    "            \n",
    "        #4. Add the ngrams to the lexicon dictionary\n",
    "        words = list(_ngrams)\n",
    "        if not wordvec:\n",
    "            self.addToLexicon( words )\n",
    "        \n",
    "        #5. Add this tweet row to the training set\n",
    "        if not wordvec:\n",
    "            self.addToTrainingData(words,sentiment)\n",
    "        else:\n",
    "            self.addVecToTrainingData(words, sentiment)\n",
    "    \n",
    "    #Build the Feature set for all the tweets\n",
    "    def addToTrainingData(self,ngrams,sentiment):\n",
    "        row = np.zeros(self.max_features_count + 1 ) # last feature is the label \n",
    "        \n",
    "        for word in ngrams:\n",
    "            if ( word in self.lexicon ):\n",
    "                row[ self.lexicon[word] ] = row[ self.lexicon[word] ] + 1 #Increase the count of the word \n",
    "\n",
    "        row [ self.max_features_count ] = sentiment\n",
    "        \n",
    "        self.data[ len(self.data)  ]  = row \n",
    "        \n",
    "    def addVecToTrainingData(self, ngrams, sentiment):\n",
    "        sentence = []\n",
    "        #print(\"Inside addVec\")\n",
    "        for word in ngrams:\n",
    "            if( word[0] in vec):\n",
    "                #print (\"found word vec\")\n",
    "                row  = vec[word[0]]\n",
    "            else: \n",
    "                row = [0.0 for i in range(300)]\n",
    "            sentence.append(row)\n",
    "        if(np.sum(sentence) == 0):\n",
    "            average_row = [0.0 for i in range(300)]\n",
    "        else:\n",
    "            average_row = np.mean(sentence, axis=0)\n",
    "        average_row = np.append(average_row, [sentiment])\n",
    "        self.data[ len(self.data) ] = average_row\n",
    "    \n",
    "    \n",
    "    def getFeatures(self):\n",
    "        features = np.zeros(( len(self.data), self.max_features_count ))\n",
    "        \n",
    "        for index in self.data:\n",
    "            features[index][:] =  self.data[index][:-1]\n",
    "            \n",
    "        return features\n",
    "    \n",
    "    def getLabels(self):\n",
    "        labels = np.zeros( len(self.data) )\n",
    "        \n",
    "        for index in self.data:\n",
    "            labels[index] = ( int(self.data[index][self.max_features_count]) )\n",
    "            \n",
    "        return labels\n",
    "    \n",
    "    def getHeaders(self):\n",
    "        headers = []\n",
    "        for i in range(self.max_features_count):\n",
    "            headers.append( self.inverse_lexicon[i] )\n",
    "        return headers\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Classifiers\n",
    "class Classifiers:\n",
    "    def __init__(self,ngrams,stopwords_fname,f_airlines,split,noOfFeatures):\n",
    "        self.ngrams = ngrams\n",
    "        self.f_airlines = f_airlines\n",
    "        self.split  = split\n",
    "        self.noOfFeatures=noOfFeatures\n",
    "\n",
    "        #1.Define Custom Classes required for the class\n",
    "        self.features = BuildFeatureSet(self.ngrams, stopwords_fname, max_features_count = self.noOfFeatures)\n",
    "        self.jsonParser = JsonParser()\n",
    "\n",
    "        #2. Build the feature set\n",
    "        print ('Building the feature set')\n",
    "        \n",
    "        for file in f_airlines:\n",
    "            print ('Parsing tweets from the file:',file)\n",
    "            tweets =  self.jsonParser.loadData(file)\n",
    "            airline_handle = file.split(\"_\")[1]\n",
    "            for tweet in tweets:\n",
    "                #1. Skip if the tweet the is a reply to an existing tweet\n",
    "                if ( self.features.isTweetReply(tweet['text']) == False and self.features.isTweetFromAirline(tweet, airline_handle) == False ):\n",
    "                    self.features.addTweet ( tweet['text']  )  #use wordvec=True if word embeddings needed\n",
    "        \n",
    "        #3. Prepare the train and test datasets\n",
    "        print ( 'Preparing the training and the test set...')\n",
    "        self.X = self.features.getFeatures()\n",
    "        self.y = self.features.getLabels()\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.X, self.y, train_size= self.split,test_size=1- self.split)\n",
    "        print ( 'Done preapring the train and test set...Ready to train classifiers')\n",
    "        \n",
    "        #4. Define Variables for classifiers performance\n",
    "        #4a. Logistic Regression\n",
    "        self.logreg_train_acc = 0\n",
    "        self.logreg_test_acc = 0\n",
    "        \n",
    "        #4b. Gaussian Naive Bayes\n",
    "        self.nb_train_acc = 0\n",
    "        self.nb_test_acc = 0\n",
    "        \n",
    "        #4c. LibLinearSVC\n",
    "        self.svc_train_acc = 0\n",
    "        self.svc_test_acc = 0\n",
    "        \n",
    "        #4d. Decision Tree\n",
    "        self.dt_train_acc = 0\n",
    "        self.dt_test_acc = 0\n",
    "\n",
    "        #4e.  Ada Boost\n",
    "        self.adaboost_train_acc = 0\n",
    "        self.adaboost_test_acc = 0\n",
    "        \n",
    "        #4f.\n",
    "        self.rf_train_acc = 0\n",
    "        self.rf_test_acc = 0\n",
    "        \n",
    "        #4g.\n",
    "        self.ltsm_train_acc = 0\n",
    "        self.ltsm_test_acc = 0\n",
    "    \n",
    "    def applyPCA(self,n):\n",
    "        print ( 'Applying PCA on X' )\n",
    "        pca = PCA(n_components=n)\n",
    "        self.X = pca.fit_transform(self.X)\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.X, self.y, train_size= self.split,test_size=1- self.split)\n",
    "        \n",
    "    def printResults(self):\n",
    "        print ( 'Results for Logistic Regression')\n",
    "        print( '(train,test):(',self.logreg_train_acc,self.logreg_test_acc,')' )\n",
    "        print ( 'Results for Gaussian Naive Bayes')\n",
    "        print( '(train,test):(',self.nb_train_acc,self.nb_test_acc,')' )\n",
    "        print ( 'Results for LibLinear SVM')\n",
    "        print( '(train,test):(',self.svc_train_acc,self.svc_test_acc,')' )\n",
    "        print ( 'Results for Decision Tree')\n",
    "        print( '(train,test):(',self.dt_train_acc,self.dt_test_acc,')' )\n",
    "        print ( 'Results for Adaboost')\n",
    "        print( '(train,test):(',self.adaboost_train_acc,self.adaboost_test_acc,')' )\n",
    "        print ( 'Results for Random Forests')\n",
    "        print( '(train,test):(',self.rf_train_acc,self.rf_test_acc,')' )\n",
    "        print ( 'Results for LTSM')\n",
    "        print( '(train,test):(',self.ltsm_train_acc,self.ltsm_test_acc,')' )\n",
    "    \n",
    "    def getResults(self):\n",
    "        return [self.logreg_train_acc,self.logreg_test_acc, \n",
    "                self.nb_train_acc,self.nb_test_acc, \n",
    "                self.svc_train_acc,self.svc_test_acc,  \n",
    "                self.dt_train_acc,self.dt_test_acc, \n",
    "                self.adaboost_train_acc,self.adaboost_test_acc, \n",
    "                self.rf_train_acc,self.rf_test_acc,\n",
    "                self.ltsm_train_acc,self.ltsm_test_acc]\n",
    "    \n",
    "    def runClassifiers(self):\n",
    "            #1. Logistic regression classifier\n",
    "            print('Training Logistic Regression Classifiers')\n",
    "\n",
    "            #C = [0.001,  0.01,  0.1,  1.0,  10,  100]\n",
    "            C = [0.1,10]\n",
    "\n",
    "            logreg = GridSearchCV(LogisticRegression(max_iter=100),cv = 10, param_grid= {\"C\" : C},verbose=1,n_jobs=60)\n",
    "            logreg.fit(self.X_train, self.y_train)\n",
    "\n",
    "            #print(clf.best_params_)\n",
    "            self.logreg_train_acc = accuracy_score( self.y_train,logreg.predict(self.X_train) )\n",
    "            self.logreg_test_acc = accuracy_score( self.y_test,logreg.predict(self.X_test) )\n",
    "            \n",
    "            #2. Naive Bayes\n",
    "            print('Training Gaussian Naive Bayes Classifier')\n",
    "            nb = GaussianNB()\n",
    "            nb.fit(self.X_train, self.y_train)\n",
    "\n",
    "            self.nb_train_acc = accuracy_score( self.y_train,nb.predict(self.X_train) )\n",
    "            self.nb_test_acc = accuracy_score( self.y_test,nb.predict(self.X_test) )\n",
    "            \n",
    "            #3.LibLinear SVM\n",
    "            print( 'Training SVM Lib Linear Classifier')\n",
    "            svc = GridSearchCV(LinearSVC(max_iter=1000),cv = 10, param_grid= {\"C\" : C},verbose=1,n_jobs=60)\n",
    "            svc.fit(self.X_train, self.y_train)\n",
    "\n",
    "            self.svc_train_acc =  accuracy_score(self.y_train,svc.predict(self.X_train)) \n",
    "            self.svc_test_acc = accuracy_score(self.y_test,svc.predict(self.X_test)) \n",
    "            \n",
    "            #4.Decision Tree\n",
    "            print( 'Training Decision Tree Classifier' )\n",
    "            max_depth = [1,3,5,7]\n",
    "            dt = GridSearchCV(DecisionTreeClassifier(),cv = 10, param_grid= {\"max_depth\" : max_depth},verbose=1,n_jobs=60)\n",
    "            dt.fit(self.X_train, self.y_train)\n",
    "            \n",
    "            self.dt_train_acc =  accuracy_score(self.y_train,dt.predict(self.X_train)) \n",
    "            self.dt_test_acc = accuracy_score(self.y_test,dt.predict(self.X_test)) \n",
    "            \n",
    "            #5. AdaBoost\n",
    "            print( 'Training AdaBoost' )\n",
    "            grd = GradientBoostingClassifier(n_estimators=500,max_depth=5,random_state=42)\n",
    "            grd.fit(self.X_train, self.y_train)\n",
    "            self.adaboost_train_acc =  accuracy_score(self.y_train,grd.predict(self.X_train)) \n",
    "            self.adaboost_test_acc = accuracy_score(self.y_test,grd.predict(self.X_test)) \n",
    "            \n",
    "            #6. Random Forests\n",
    "            print( 'Training Random Forests' )\n",
    "            rf = RandomForestClassifier(n_estimators=500,max_depth=5,random_state=42)\n",
    "            rf.fit(self.X_train, self.y_train)\n",
    "            self.rf_train_acc =  accuracy_score(self.y_train,rf.predict(self.X_train)) \n",
    "            self.rf_test_acc = accuracy_score(self.y_test,rf.predict(self.X_test)) \n",
    "            \n",
    "            #7.Deep net LTSM\n",
    "            print( 'Training LTSM Deep Net' )\n",
    "            embedding_vecor_length = 300\n",
    "            model = Sequential()\n",
    "            top_words = self.X_train.shape[0]\n",
    "            max_review_length = self.X_train.shape[1]\n",
    "            model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "\n",
    "            # Convolutional model (3x conv, flatten, 2x dense)\n",
    "            model.add(Dropout(0.2))\n",
    "            model.add(Convolution1D(64, 5, activation='relu'))\n",
    "            model.add(MaxPooling1D(pool_size=4))\n",
    "            model.add(LSTM(128))\n",
    "            model.add(Dense(3,activation='softmax'))\n",
    "\n",
    "            # Log to tensorboard\n",
    "            tensorBoardCallback = TensorBoard(log_dir='./logs', write_graph=True)\n",
    "            model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "            model.fit(self.X_train, self.y_train, epochs=3, batch_size=64)\n",
    "\n",
    "            # Evaluation on the test set\n",
    "            self.ltsm_train_acc =  model.evaluate(self.X_train, self.y_train, verbose=0)[1]\n",
    "            self.ltsm_test_acc =  model.evaluate(self.X_test, self.y_test, verbose=0)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate the Classifier Results\n",
    "files = ['tweets_Allegiant.txt','tweets_AlaskaAir.txt','tweets_Delta.txt','tweets_AmericanAir.txt','tweets_FlyFrontier.txt','tweets_HawaiianAir.txt']  \n",
    "noOfFeatures = [5000]\n",
    "\n",
    "headers=['logreg_train','logreg_test','nb_train','nb_test','svm_train','svm_test','dt_train','dt_test','adaboost_train','adaboost_test'\n",
    "        ,'rf_train','rf_test','ltsm_train','ltsm_test']\n",
    "results=[]\n",
    "\n",
    "for featureLen in noOfFeatures:\n",
    "    results = []\n",
    "    for i in range(5):\n",
    "        print( 'Generate results for ', i, '-gram Classifiers')\n",
    "        clf = Classifiers(i,'stopwords.txt',files,0.75,featureLen)\n",
    "        clf.runClassifiers()\n",
    "        results.append( clf.getResults() )\n",
    "    \n",
    "    print( 'Results (features): (',featureLen,'):' )\n",
    "    df = pd.DataFrame(results, columns=headers)\n",
    "    print( df)\n",
    "    \n",
    "print ( 'Results After Performing PCA ')\n",
    "for featureLen in noOfFeatures:\n",
    "    results = []\n",
    "    for i in range(5):\n",
    "        print( 'Generate results for ith gram Classifiers')\n",
    "        clf = Classifiers(i,'stopwords.txt',files,0.75,featureLen)\n",
    "        #clf.applyPCA(20) #Keeping the top 20 important features\n",
    "        clf.runClassifiers()\n",
    "        results.append( clf.getResults() )\n",
    "    \n",
    "    print( 'Results (features): (',featureLen,'):' )\n",
    "    df = pd.DataFrame(results, columns=headers)\n",
    "    print( df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Visualize:\n",
    "    def __init__(self,debug,removeWordsList=[]):\n",
    "        self.debug = debug\n",
    "        \n",
    "        self.jsonParser = JsonParser()\n",
    "        self.tweetCleaner = TweetCleaner('stopwords.txt')\n",
    "        self.removeWordsList = removeWordsList #List of keywords to remove for the wordcount. ex: AT_USER\n",
    "        \n",
    "        #map[word]->count for the word cloud\n",
    "        self.poscount = {} \n",
    "        self.negcount = {}  \n",
    "        self.neutralcount = {} \n",
    "\n",
    "        #Location of tweets\n",
    "        self.posloc = {}\n",
    "        self.negloc = {}\n",
    "        self.neutloc = {}\n",
    "        \n",
    "        #month_day of tweets\n",
    "        self.posdate = {}\n",
    "        self.negdate = {}\n",
    "        self.neutraldate = {}\n",
    "        \n",
    "    \n",
    "    def log(self,msg):\n",
    "        if ( self.debug ):\n",
    "            print (msg)\n",
    "    \n",
    "    def clearCount(self,mode):\n",
    "        if (mode == 0 ):\n",
    "            self.negcount = {}  \n",
    "        elif(mode == 1):\n",
    "            self.neutralcount = {} \n",
    "        else:\n",
    "            self.poscount = {} \n",
    "            \n",
    "    def clearLocations(self,mode):\n",
    "        if (mode == 0 ):\n",
    "            self.negloc = {}  \n",
    "        elif(mode == 1):\n",
    "            self.neutloc = {} \n",
    "        else:\n",
    "            self.posloc = {} \n",
    "    \n",
    "    #Function to return the top n words \n",
    "    def getTopNWords(self,n,mode):# mode: 0 | 1 | 2 \n",
    "        if( mode == 0):\n",
    "            items = sorted(self.negcount.items(), key=lambda x: x[1],reverse = True)\n",
    "        elif( mode == 1 ):\n",
    "            items = sorted(self.neutralcount.items(), key=lambda x: x[1],reverse = True)\n",
    "        elif( mode == 2 ):\n",
    "            items = sorted(self.poscount.items(), key=lambda x: x[1],reverse = True)\n",
    "        items = items[0:n]\n",
    "        result = [i for i in items if i[0] not in self.removeWordsList]\n",
    "        return result\n",
    "    \n",
    "    #Function to return the top n Locations\n",
    "    def getTopNLocations(self,n,mode):# mode: 0 | 1 | 2 \n",
    "        if( mode == 0):\n",
    "            items = sorted(self.negloc.items(), key=lambda x: x[1],reverse = True)\n",
    "        elif( mode == 1 ):\n",
    "            items = sorted(self.neutloc.items(), key=lambda x: x[1],reverse = True)\n",
    "        elif( mode == 2 ):\n",
    "            items = sorted(self.posloc.items(), key=lambda x: x[1],reverse = True)\n",
    "        items = items[0:n]\n",
    "        result = [i for i in items if i[0] not in self.removeWordsList]\n",
    "        return result\n",
    "    \n",
    "    #Function to return the top n dates\n",
    "    def getTopNDates(self,n,mode):# mode: 0 | 1 | 2 \n",
    "        if( mode == 0):\n",
    "            items = sorted(self.negdate.items(), key=lambda x: x[1],reverse = True)\n",
    "        elif( mode == 1 ):\n",
    "            items = sorted(self.neutraldate.items(), key=lambda x: x[1],reverse = True)\n",
    "        elif( mode == 2 ):\n",
    "            items = sorted(self.posdate.items(), key=lambda x: x[1],reverse = True)\n",
    "        items = items[0:n]\n",
    "        result = [i for i in items if i[0] not in self.removeWordsList]\n",
    "        return result\n",
    "    \n",
    "    def writeToCSV(self,fname,n,mode):\n",
    "        if(mode == 0):\n",
    "            fname=fname+'_neg' +'.csv'\n",
    "        elif ( mode == 1):\n",
    "            fname=fname+'_neut'+'.csv'\n",
    "        else:\n",
    "            fname=fname+'_pos'+'.csv'\n",
    "            \n",
    "        with open(fname,'w') as out:\n",
    "            csv_out=csv.writer(out)\n",
    "            csv_out.writerow(['word','count'])\n",
    "            rows = self.getTopN(n,mode)\n",
    "            for row in rows:\n",
    "                csv_out.writerow(row)\n",
    "    \n",
    "    def populateCountList(self,text,sentiment):\n",
    "            #Populate word count\n",
    "            if ( sentiment == 2):\n",
    "                for  word in text.split(' '):\n",
    "                    if ( word not in self.poscount ):\n",
    "                        self.poscount[word] = 1\n",
    "                    else:\n",
    "                        self.poscount[word] = self.poscount[word] + 1\n",
    "            elif ( sentiment == 1 ):\n",
    "                for  word in text.split(' '):\n",
    "                    if ( word not in self.neutralcount ):\n",
    "                        self.neutralcount[word] = 1\n",
    "                    else:\n",
    "                        self.neutralcount[word] = self.neutralcount[word] + 1\n",
    "            else:\n",
    "                for  word in text.split(' '):\n",
    "                    if ( word not in self.negcount ):\n",
    "                        self.negcount[word] = 1\n",
    "                    else:\n",
    "                        self.negcount[word] = self.negcount[word] + 1\n",
    "                        \n",
    "    def populateLocationList(self,location,sentiment):\n",
    "            #Populate Location count\n",
    "            if ( sentiment == 2):\n",
    "                    if ( location not in self.posloc ):\n",
    "                        self.posloc[location] = 1\n",
    "                    else:\n",
    "                        self.posloc[location] = self.posloc[location] + 1\n",
    "            elif ( sentiment == 1 ):\n",
    "                    if ( location not in self.neutloc ):\n",
    "                        self.neutloc[location] = 1\n",
    "                    else:\n",
    "                        self.neutloc[location] = self.neutloc[location] + 1\n",
    "            else:\n",
    "                    if ( location not in self.negloc ):\n",
    "                        self.negloc[location] = 1\n",
    "                    else:\n",
    "                        self.negloc[location] = self.negloc[location] + 1\n",
    "    \n",
    "    def populateTimeTweetList(self, date, sentiment):\n",
    "        #first extract the date\n",
    "        #Sat Apr 14 04:46:23 +0000 2018\n",
    "        date_words = date.split(\" \")\n",
    "        _date = date_words[1] + \" \" + date_words[2]\n",
    "        \n",
    "        #populate the date count\n",
    "        if ( sentiment == 2):\n",
    "                    if ( _date not in self.posdate ):\n",
    "                        self.posdate[_date] = 1\n",
    "                    else:\n",
    "                        self.posdate[_date] += 1\n",
    "        elif ( sentiment == 1 ):\n",
    "                    if ( _date not in self.neutraldate ):\n",
    "                        self.neutraldate[_date] = 1\n",
    "                    else:\n",
    "                        self.neutraldate[_date] += 1\n",
    "        else:\n",
    "                    if ( _date not in self.negdate ):\n",
    "                        self.negdate[_date] = 1\n",
    "                    else:\n",
    "                        self.negdate[_date] += 1\n",
    "\n",
    "             \n",
    "    def setWordCount(self,fname):\n",
    "        self.log('File name is,'+fname)\n",
    "        tweets =  jsonParser.loadData(fname)\n",
    "        \n",
    "        self.log('Reading all the tweets')\n",
    "        \n",
    "        for tweet in tweets:\n",
    "            #1.Skip for retweets\n",
    "            if ( tweet['text'][0].lower() == 'r' and  tweet['text'][1].lower() == 't' ):\n",
    "                continue\n",
    "                \n",
    "            text = self.tweetCleaner.cleanTweet( tweet['text'] )\n",
    "            sentiment = self.tweetCleaner.getSentiment( tweet['text'] )\n",
    "            self.populateCountList(text,sentiment)\n",
    "            self.populateLocationList(tweet['user']['location'],sentiment)\n",
    "            self.populateTimeTweetList(tweet['created_at'], sentiment)\n",
    "\n",
    "                    \n",
    "        self.log('Word count is set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removeWordsList = ['AT_USER','HASH_TAG','...',',','’','rt','','…','flight','?','!','can','t','s','m']\n",
    "visualize = Visualize(False,removeWordsList)\n",
    "visualize.setWordCount('tweets_JetBlue.txt')\n",
    "\n",
    "print ( 'negative', visualize.getTopNWords(30,0) )\n",
    "print ('')\n",
    "print ( 'neutral',visualize.getTopNWords(30,1) )\n",
    "print ('')\n",
    "print ( 'positive',visualize.getTopNWords(30,2) )\n",
    "print ('')\n",
    "print ( 'negative locations', visualize.getTopNLocations(30,0) )\n",
    "print ('')\n",
    "print ( 'neutral locations',visualize.getTopNLocations(30,1) )\n",
    "print ('')\n",
    "print ( 'positive locations',visualize.getTopNLocations(30,2) )\n",
    "print ('')\n",
    "print ( 'negative dates', visualize.getTopNDates(5,0) )\n",
    "print ('')\n",
    "print ( 'neutral dates',visualize.getTopNDates(5,1) )\n",
    "print ('')\n",
    "print ( 'positive dates',visualize.getTopNDates(5,2) )\n",
    "\n",
    "\n",
    "#Write Data to a sav file for visualisation in tableau\n",
    "\n",
    "#visualize.writeToCSV('wc_AmericanAir',200,0)\n",
    "#visualize.writeToCSV('wc_AmericanAir',200,1)\n",
    "#visualize.writeToCSV('wc_AmericanAir',200,2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['tweets_JetBlue.txt','tweets_SouthwestAir.txt','tweets_SpiritAirlines.txt','tweets_SunCountryAir.txt','tweets_VirginAmerica.txt']\n",
    "\n",
    "jsonParser = JsonParser()\n",
    "features = BuildFeatureSet(1, 'stopwords.txt', max_features_count = 10000000)\n",
    "\n",
    "for file in files:\n",
    "            tweet_count = 0\n",
    "            print ('Parsing tweets from the file:',file)\n",
    "            tweets =  jsonParser.loadData(file)\n",
    "            airline_handle = file.split(\"_\")[1]\n",
    "            for tweet in tweets:\n",
    "                tweet_count = tweet_count + 1 \n",
    "                #1. Skip if the tweet the is a reply to an existing tweet\n",
    "                if ( features.isTweetReply(tweet['text']) == False and features.isTweetFromAirline(tweet, airline_handle) == False ):\n",
    "                    features.addTweet ( tweet['text']  ) \n",
    "            print (tweet_count)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
